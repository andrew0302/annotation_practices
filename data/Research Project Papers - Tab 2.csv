dataset,venue,paper DOI,dataset DOI,link to paper,paper name,notes,period (used for statistics),original paper citations (used for statistics),dataset lowercase (used for statistics)
Arcade Learning Environment,AAAI,10.1609/aaai.v30i1.10295,10.48550/arXiv.1207.4708,https://arxiv.org/pdf/1207.4708,Deep reinforcement learning with double Q-Learning,Dataset was gathered using the ALE,15,5484,arcade learning environment
Office,AAAI,10.1609/aaai.v30i1.10306,10.1007/978-3-642-15561-1_16,https://link.springer.com/chapter/10.1007/978-3-642-15561-1_16#preview,Return of frustratingly easy domain adaptation,,15,1392,office
Office-Caltech10,AAAI,10.1609/aaai.v30i1.10306,10.1109/CVPR.2012.6247911,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=6247911,Return of frustratingly easy domain adaptation,,15,1392,office-caltech10
MovieTriples,AAAI,10.1609/aaai.v30i1.9883,10.48550/arXiv.1507.04808,https://arxiv.org/pdf/1507.04808,Building end-To-end dialogue systems using generative hierarchical neural network models,Dataset is in the main paper itself,15,1258,movietriples
Movie-DiC,AAAI,10.1609/aaai.v30i1.9883,https://aclanthology.org/P12-2040/,https://aclanthology.org/P12-2040/,Building end-To-end dialogue systems using generative hierarchical neural network models,,15,1258,movie-dic
SubTle,AAAI,10.1609/aaai.v30i1.9883,N/A,https://www.academia.edu/72061745/From_subtitles_to_human_interactions_introducing_the_SubTle_Corpus,Building end-To-end dialogue systems using generative hierarchical neural network models,"If you need the dataset paper, ask Damjan",15,1258,subtle
TaxiBJ,AAAI,10.1609/aaai.v31i1.10735,10.1609/aaai.v31i1.10735,https://ojs.aaai.org/index.php/AAAI/article/view/10735,Deep spatio-temporal residual networks for citywide crowd flows prediction,No dataset paper could be found,15,1787,taxibj
BikeNYC-1,AAAI,10.1609/aaai.v31i1.10735,10.1609/aaai.v31i1.10735,https://ojs.aaai.org/index.php/AAAI/article/view/10735,Deep spatio-temporal residual networks for citywide crowd flows prediction,Introduced and used in the paper,15,1787,bikenyc-1
Chinese poems,AAAI,10.1609/aaai.v31i1.10804,N/A,N/A,SeqGAN: Sequence generative adversarial nets with policy gradient,Broken URL for the dataset,15,1695,chinese poems
Obama speeches,AAAI,10.1609/aaai.v31i1.10804,https://github.com/samim23/obama-rnn?tab=readme-ov-file,https://github.com/samim23/obama-rnn?tab=readme-ov-file,SeqGAN: Sequence generative adversarial nets with policy gradient,,15,1695,obama speeches
Nottingham music dataset,AAAI,10.1609/aaai.v31i1.10804,https://www-labs.iro.umontreal.ca/~lisa/deep/data/,https://www-labs.iro.umontreal.ca/~lisa/deep/data/,SeqGAN: Sequence generative adversarial nets with policy gradient,,15,1695,nottingham music dataset
ImageNet,AAAI,10.1609/aaai.v31i1.11231,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,"Inception-v4, inception-ResNet and the impact of residual connections on learning",,15,9648,imagenet
Anchors,AAAI,10.1609/aaai.v32i1.11491,10.1609/aaai.v32i1.11491,https://ojs.aaai.org/index.php/AAAI/article/view/11491,Anchors: High-precision model-agnostic explanations,"Dataset is in the main paper itself. Dataset did not have a name so I just named it ""Anchors"" by the paper title",15,1527,anchors
WN18,AAAI,10.1609/aaai.v32i1.11573,N/A,https://papers.nips.cc/paper_files/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html,Convolutional 2D knowledge graph embeddings,,15,2228,wn18
FB15k,AAAI,10.1609/aaai.v32i1.11573,N/A,https://papers.nips.cc/paper_files/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html,Convolutional 2D knowledge graph embeddings,,15,2228,fb15k
YAGO3-10,AAAI,10.1609/aaai.v32i1.11573,N/A,https://scispace.com/pdf/yago3-a-knowledge-base-from-multilingual-wikipedias-8ieljppqqg.pdf,Convolutional 2D knowledge graph embeddings,,15,2228,yago3-10
Countries,AAAI,10.1609/aaai.v32i1.11573,N/A,https://aaai.org/papers/10257-10257-on-approximate-reasoning-capabilities-of-low-rank-vector-spaces/,Convolutional 2D knowledge graph embeddings,,15,2228,countries
WN18RR,AAAI,10.1609/aaai.v32i1.11573,10.1609/aaai.v32i1.11573,https://ojs.aaai.org/index.php/AAAI/article/view/11573,Convolutional 2D knowledge graph embeddings,Dataset is in the main paper itself,15,2228,wn18rr
FB15k-237,AAAI,10.1609/aaai.v32i1.11573,10.18653/v1/D15-1174,https://aclanthology.org/D15-1174/,Convolutional 2D knowledge graph embeddings,,15,2228,fb15k-237
CiteSeer,AAAI,10.1609/aaai.v32i1.11604,10.1609/aimag.v29i3.2157,https://onlinelibrary.wiley.com/doi/epdf/10.1609/aimag.v29i3.2157,Deeper insights into graph convolutional networks for semi-supervised learning,,15,2136,citeseer
Cora,AAAI,10.1609/aaai.v32i1.11604,10.1609/aimag.v29i3.2157,https://onlinelibrary.wiley.com/doi/epdf/10.1609/aimag.v29i3.2157,Deeper insights into graph convolutional networks for semi-supervised learning,,15,2136,cora
PubMed,AAAI,10.1609/aaai.v32i1.11604,10.1609/aimag.v29i3.2157,https://onlinelibrary.wiley.com/doi/epdf/10.1609/aimag.v29i3.2157,Deeper insights into graph convolutional networks for semi-supervised learning,PubMed was cited as if it was in this paper but I could not find it there,15,2136,pubmed
CLEVR,AAAI,10.1609/aaai.v32i1.11671,10.48550/arXiv.1612.06890,https://arxiv.org/pdf/1612.06890,FiLM: Visual reasoning with a general conditioning layer,,15,1238,clevr
MUTAG,AAAI,10.1609/aaai.v32i1.11782,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,An end-to-end deep learning architecture for graph classification,Dataset is in the main paper itself,15,1264,mutag
PTC,AAAI,10.1609/aaai.v32i1.11782,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,An end-to-end deep learning architecture for graph classification,Dataset is in the main paper itself,15,1264,ptc
NCI1,AAAI,10.1609/aaai.v32i1.11782,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,An end-to-end deep learning architecture for graph classification,Dataset is in the main paper itself,15,1264,nci1
PROTEINS,AAAI,10.1609/aaai.v32i1.11782,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,An end-to-end deep learning architecture for graph classification,Dataset is in the main paper itself,15,1264,proteins
DD,AAAI,10.1609/aaai.v32i1.11782,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,An end-to-end deep learning architecture for graph classification,Dataset is in the main paper itself,15,1264,dd
Decentralised StarCraft Micromanagement,AAAI,10.1609/aaai.v32i1.11794,10.1609/aaai.v32i1.11794,https://ojs.aaai.org/index.php/AAAI/article/view/11794,Counterfactual multi-agent policy gradients,The videogame StarCraft was used as the dataset for the RL model,15,1428,decentralised starcraft micromanagement
Arcade Learning Environment,AAAI,10.1609/aaai.v32i1.11796,10.48550/arXiv.1207.4708,https://arxiv.org/pdf/1207.4708,Rainbow: Combining improvements in deep reinforcement learning,Dataset was gathered using the ALE,15,1298,arcade learning environment
Kinetics,AAAI,10.1609/aaai.v32i1.12328,10.48550/arXiv.1705.06950,https://arxiv.org/abs/1705.06950,Spatial temporal graph convolutional networks for skeleton-based action recognition,,15,3458,kinetics
NTU RGB+D,AAAI,10.1609/aaai.v32i1.12328,10.48550/arXiv.1604.02808,https://arxiv.org/abs/1604.02808,Spatial temporal graph convolutional networks for skeleton-based action recognition,,15,3458,ntu rgb+d
YOOCHOOSE,AAAI,10.1609/aaai.v33i01.3301346,10.1145/2792838.2798723,https://dl-acm-org.tudelft.idm.oclc.org/doi/10.1145/2792838.2798723,Session-based recommendation with graph neural networks,,15,1487,yoochoose
Diginetica,AAAI,10.1609/aaai.v33i01.3301346,N/A,N/A,Session-based recommendation with graph neural networks,,15,1487,diginetica
Cora,AAAI,10.1609/aaai.v33i01.33013558,10.1609/aimag.v29i3.2157,https://onlinelibrary.wiley.com/doi/epdf/10.1609/aimag.v29i3.2157,Hypergraph neural networks,,15,1180,cora
PubMed,AAAI,10.1609/aaai.v33i01.33013558,10.1609/aimag.v29i3.2157,https://onlinelibrary.wiley.com/doi/epdf/10.1609/aimag.v29i3.2157,Hypergraph neural networks,PubMed was cited as if it was in this paper but I could not find it there,15,1180,pubmed
CIFAR-10,AAAI,10.1609/aaai.v33i01.33014780,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Regularized evolution for image classifier architecture search,,15,2043,cifar-10
MNIST,AAAI,10.1609/aaai.v33i01.33014780,10.1109/5.726791,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=726791,Regularized evolution for image classifier architecture search,,15,2043,mnist
ImageNet,AAAI,10.1609/aaai.v33i01.33014780,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Regularized evolution for image classifier architecture search,,15,2043,imagenet
CheXpert,AAAI,10.1609/aaai.v33i01.3301590,10.1609/aaai.v33i01.3301590,https://ojs.aaai.org/index.php/AAAI/article/view/3834,CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison,Dataset is in the main paper itself,15,1651,chexpert
20NG,AAAI,10.1609/aaai.v33i01.33017370,N/A,https://disi.unitn.it/moschitti/corpora.htm,Graph convolutional networks for text classification,,15,1737,20ng
Ohsumed,AAAI,10.1609/aaai.v33i01.33017370,https://disi.unitn.it/moschitti/corpora.htm,https://disi.unitn.it/moschitti/corpora.htm,Graph convolutional networks for text classification,,15,1737,ohsumed
Reuters 21578,AAAI,10.1609/aaai.v33i01.33017370,N/A,https://disi.unitn.it/moschitti/corpora.htm,Graph convolutional networks for text classification,,15,1737,reuters 21578
MR,AAAI,10.1609/aaai.v33i01.33017370,10.3115/1219840.1219855,https://dl.acm.org/doi/pdf/10.3115/1219840.1219855,Graph convolutional networks for text classification,,15,1737,mr
PeMSD4,AAAI,10.1609/aaai.v33i01.3301922,10.1609/aaai.v33i01.3301922,https://ojs.aaai.org/index.php/AAAI/article/view/3881,Attention based spatial-temporal graph convolutional networks for traffic flow forecasting,,15,2246,pemsd4
PeMSD8,AAAI,10.1609/aaai.v33i01.3301922,10.1609/aaai.v33i01.3301922,https://ojs.aaai.org/index.php/AAAI/article/view/3881,Attention based spatial-temporal graph convolutional networks for traffic flow forecasting,,15,2246,pemsd8
Weibo 2,AAAI,10.1609/aaai.v34i01.5393,N/A,https://www.ijcai.org/Proceedings/16/Papers/537.pdf,Rumor detection on social media with bi-directional graph convolutional networks,,5,579,weibo 2
Twitter15,AAAI,10.1609/aaai.v34i01.5393,10.1145/2806416.2806651,https://dl.acm.org/doi/pdf/10.1145/2806416.2806651,Rumor detection on social media with bi-directional graph convolutional networks,"In the original paper, the dataset is linked to a paper from which the current dataset paper was taken",5,579,twitter15
Twitter16,AAAI,10.1609/aaai.v34i01.5393,N/A,https://www.ijcai.org/Proceedings/16/Papers/537.pdf,Rumor detection on social media with bi-directional graph convolutional networks,"In the original paper, the dataset is linked to a paper from which the current dataset paper was taken",5,579,twitter16
PEMS03,AAAI,10.1609/aaai.v34i01.5438,10.1609/aaai.v34i01.5438,https://ojs.aaai.org/index.php/AAAI/article/view/5438,Spatial-temporal synchronous graph convolutional networks: A new framework for spatial-temporal network data forecasting,,5,1130,pems03
PEMS04,AAAI,10.1609/aaai.v34i01.5438,10.1609/aaai.v34i01.5438,https://ojs.aaai.org/index.php/AAAI/article/view/5438,Spatial-temporal synchronous graph convolutional networks: A new framework for spatial-temporal network data forecasting,,5,1130,pems04
PEMS07,AAAI,10.1609/aaai.v34i01.5438,10.1609/aaai.v34i01.5438,https://ojs.aaai.org/index.php/AAAI/article/view/5438,Spatial-temporal synchronous graph convolutional networks: A new framework for spatial-temporal network data forecasting,,5,1130,pems07
PEMS08,AAAI,10.1609/aaai.v34i01.5438,10.1609/aaai.v34i01.5438,https://ojs.aaai.org/index.php/AAAI/article/view/5438,Spatial-temporal synchronous graph convolutional networks: A new framework for spatial-temporal network data forecasting,,5,1130,pems08
Xiamen,AAAI,10.1609/aaai.v34i01.5477,10.1109/ICWS.2017.106,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=8029849,GMAN: A graph multi-attention network for traffic prediction,,15,1235,xiamen
PEMS-BAY,AAAI,10.1609/aaai.v34i01.5477,https://openreview.net/forum?id=SJiHXGWAZ,https://openreview.net/pdf?id=SJiHXGWAZ,GMAN: A graph multi-attention network for traffic prediction,,15,1235,pems-bay
WikiZh,AAAI,10.1609/aaai.v34i03.5681,N/A,https://dumps.wikimedia.org/zhwiki/latest/,K-BERT: Enabling language representation with knowledge graph,Used in paper for pre-training,5,614,wikizh
WebtextZh,AAAI,10.1609/aaai.v34i03.5681,N/A,https://github.com/brightmart/nlp_chinese_corpus,K-BERT: Enabling language representation with knowledge graph,Used in paper for pre-training,5,614,webtextzh
CN-DBpedia,AAAI,10.1609/aaai.v34i03.5681,10.1007/978-3-319-60045-1_44,https://link.springer.com/chapter/10.1007/978-3-319-60045-1_44,K-BERT: Enabling language representation with knowledge graph,,5,614,cn-dbpedia
HowNet,AAAI,10.1609/aaai.v34i03.5681,10.1142/5935,https://aclanthology.org/C10-3014.pdf,K-BERT: Enabling language representation with knowledge graph,,5,614,hownet
MedicalKG,AAAI,10.1609/aaai.v34i03.5681,10.48550/arXiv.1909.07606,https://arxiv.org/pdf/1909.07606,K-BERT: Enabling language representation with knowledge graph,,5,614,medicalkg
Book_review,AAAI,10.1609/aaai.v34i03.5681,N/A,https://embedding.github.io/evaluation/,K-BERT: Enabling language representation with knowledge graph,,5,614,book_review
Chnsenticorp,AAAI,10.1609/aaai.v34i03.5681,N/A,https://github.com/pengming617/bert_classification,K-BERT: Enabling language representation with knowledge graph,,5,614,chnsenticorp
Shopping,AAAI,10.1609/aaai.v34i03.5681,N/A,https://share.weiyun.com/5xxYiig,K-BERT: Enabling language representation with knowledge graph,,5,614,shopping
Weibo 1,AAAI,10.1609/aaai.v34i03.5681,N/A,https://share.weiyun.com/5lEsv0w,K-BERT: Enabling language representation with knowledge graph,,5,614,weibo 1
XNLI,AAAI,10.1609/aaai.v34i03.5681,10.18653/v1/D18-1269,https://aclanthology.org/D18-1269.pdf,K-BERT: Enabling language representation with knowledge graph,,5,614,xnli
LCQMC,AAAI,10.1609/aaai.v34i03.5681,https://aclanthology.org/C18-1166/,https://aclanthology.org/C18-1166.pdf,K-BERT: Enabling language representation with knowledge graph,,5,614,lcqmc
NLPCC-DBQA,AAAI,10.1609/aaai.v34i03.5681,http://tcci.ccf.org.cn/conference/2016/dldoc/evagline2.pdf,http://tcci.ccf.org.cn/conference/2016/dldoc/evagline2.pdf,K-BERT: Enabling language representation with knowledge graph,,5,614,nlpcc-dbqa
MSRA-NER,AAAI,10.1609/aaai.v34i03.5681,https://aclanthology.org/W06-0115/,https://aclanthology.org/W06-0115.pdf,K-BERT: Enabling language representation with knowledge graph,,5,614,msra-ner
CORA,AAAI,10.1609/aaai.v34i04.5747,10.1609/aimag.v29i3.2157,https://onlinelibrary.wiley.com/doi/epdf/10.1609/aimag.v29i3.2157,Measuring and relieving the over-smoothing problem for graph neural networks from the topological view,,5,820,cora
CiteSeer,AAAI,10.1609/aaai.v34i04.5747,10.1609/aimag.v29i3.2157,https://onlinelibrary.wiley.com/doi/epdf/10.1609/aimag.v29i3.2157,Measuring and relieving the over-smoothing problem for graph neural networks from the topological view,,5,820,citeseer
PubMed,AAAI,10.1609/aaai.v34i04.5747,10.1609/aimag.v29i3.2157,https://onlinelibrary.wiley.com/doi/epdf/10.1609/aimag.v29i3.2157,Measuring and relieving the over-smoothing problem for graph neural networks from the topological view,,5,820,pubmed
CS,AAAI,10.1609/aaai.v34i04.5747,N/A,N/A,Measuring and relieving the over-smoothing problem for graph neural networks from the topological view,Broken URL; they describe the dataset in the appendix (check the arXiv version of the original paper),5,820,cs
Physics,AAAI,10.1609/aaai.v34i04.5747,?,?,Measuring and relieving the over-smoothing problem for graph neural networks from the topological view,Broken URL; they describe the dataset in the appendix (check the arXiv version of the original paper),5,820,physics
Amazon Computers,AAAI,10.1609/aaai.v34i04.5747,10.1145/2766462.2767755,https://dl.acm.org/doi/pdf/10.1145/2766462.2767755,Measuring and relieving the over-smoothing problem for graph neural networks from the topological view,,5,820,amazon computers
Amazon Photo,AAAI,10.1609/aaai.v34i04.5747,10.1145/2766462.2767755,https://dl.acm.org/doi/pdf/10.1145/2766462.2767755,Measuring and relieving the over-smoothing problem for graph neural networks from the topological view,,5,820,amazon photo
MLT-2017,AAAI,10.1609/aaai.v34i07.6812,10.1109/ICDAR.2017.237,https://ieeexplore.ieee.org/document/8270168,Real-time scene text detection with differentiable binarization,,5,659,mlt-2017
ICDAR2015-Challenge-4,AAAI,10.1609/aaai.v34i07.6812,10.1109/ICDAR.2015.7333942,https://ieeexplore.ieee.org/document/7333942,Real-time scene text detection with differentiable binarization,,5,659,icdar2015-challenge-4
MSRA-TD500,AAAI,10.1609/aaai.v34i07.6812,10.1109/CVPR.2012.6247787,https://ieeexplore.ieee.org/document/6247787,Real-time scene text detection with differentiable binarization,,5,659,msra-td500
CTW1500,AAAI,10.1609/aaai.v34i07.6812,10.1016/j.patcog.2019.02.002,https://www.sciencedirect.com/science/article/pii/S0031320319300664,Real-time scene text detection with differentiable binarization,,5,659,ctw1500
Total-Text,AAAI,10.1609/aaai.v34i07.6812,10.1109/ICDAR.2017.157,https://ieeexplore.ieee.org/document/8270088,Real-time scene text detection with differentiable binarization,,5,659,total-text
CIFAR-10,AAAI,10.1609/aaai.v34i04.5963,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Improved knowledge distillation via teacher assistant,,5,815,cifar-10
CIFAR-100,AAAI,10.1609/aaai.v34i04.5963,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Improved knowledge distillation via teacher assistant,,5,815,cifar-100
ImageNet,AAAI,10.1609/aaai.v34i04.5963,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Improved knowledge distillation via teacher assistant,,5,815,imagenet
SBM,AAAI,10.1609/aaai.v34i04.5984,10.1080/01621459.1987.10478385,https://www.stat.cmu.edu/~brian/780/bibliography/04%20Blockmodels/Wang%20-%201987%20-%20Stochastic%20blockmodels%20for%20directed%20graphs.pdf,EvolveGCN: Evolving graph convolutional networks for dynamic graphs,The paper uses the method in https://arxiv.org/abs/1805.11273 to get the data from the SBM,5,802,sbm
BC-OTC,AAAI,10.1609/aaai.v34i04.5984,10.1109/ICDM.2016.0033,https://snap.stanford.edu/data/soc-sign-bitcoin-otc.html,EvolveGCN: Evolving graph convolutional networks for dynamic graphs,,5,802,bc-otc
BC-Alpha,AAAI,10.1609/aaai.v34i04.5984,10.1109/ICDM.2016.0033,http://snap.stanford.edu/data/soc-sign-bitcoin-alpha.html,EvolveGCN: Evolving graph convolutional networks for dynamic graphs,,5,802,bc-alpha
UCI,AAAI,10.1609/aaai.v34i04.5984,N/A,http://konect.cc/networks/opsahl-ucsocial/,EvolveGCN: Evolving graph convolutional networks for dynamic graphs,Broken URL for UCI in original paper; found a replacement one online,5,802,uci
Elliptic,AAAI,10.1609/aaai.v34i04.5984,10.48550/arXiv.1908.02591,https://arxiv.org/pdf/1908.02591,EvolveGCN: Evolving graph convolutional networks for dynamic graphs,Link used in the original paper: https://www.kaggle.com/datasets/ellipticco/elliptic-data-set,5,802,elliptic
PIQA,AAAI,10.1609/aaai.v34i05.6239,10.1609/aaai.v34i05.6239,https://ojs.aaai.org/index.php/AAAI/article/view/6239,PIQA: Reasoning about physical commonsense in natural language,,5,653,piqa
AG's News,AAAI,10.1609/aaai.v34i05.6311,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Is BERT really robust? A strong baseline for natural language attack on text classification and entailment,,5,746,ag's news
Fake,AAAI,10.1609/aaai.v34i05.6311,N/A,https://www.kaggle.com/c/fake-news/data,Is BERT really robust? A strong baseline for natural language attack on text classification and entailment,,5,746,fake
MR,AAAI,10.1609/aaai.v34i05.6311,10.3115/1219840.1219855,https://dl.acm.org/doi/pdf/10.3115/1219840.1219855,Is BERT really robust? A strong baseline for natural language attack on text classification and entailment,,5,746,mr
IMDB,AAAI,10.1609/aaai.v34i05.6311,https://datasets.imdbws.com/,https://datasets.imdbws.com/,Is BERT really robust? A strong baseline for natural language attack on text classification and entailment,,5,746,imdb
Yelp-2,AAAI,10.1609/aaai.v34i05.6311,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Is BERT really robust? A strong baseline for natural language attack on text classification and entailment,,5,746,yelp-2
SNLI,AAAI,10.1609/aaai.v34i05.6311,10.18653/v1/D15-1075,https://aclanthology.org/D15-1075.pdf,Is BERT really robust? A strong baseline for natural language attack on text classification and entailment,,5,746,snli
Multi-NLI,AAAI,10.1609/aaai.v34i05.6311,10.18653/v1/N18-1101,https://aclanthology.org/N18-1101.pdf,Is BERT really robust? A strong baseline for natural language attack on text classification and entailment,,5,746,multi-nli
Conceptual Captions,AAAI,10.1609/aaai.v34i07.6795,N/A,https://aclanthology.org/P18-1238.pdf,Unicoder-VL: A universal encoder for vision and language by cross-modal pre-training,,5,587,conceptual captions
SBU,AAAI,10.1609/aaai.v34i07.6795,N/A,https://papers.nips.cc/paper_files/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf,Unicoder-VL: A universal encoder for vision and language by cross-modal pre-training,,5,587,sbu
COCO,AAAI,10.1609/aaai.v34i07.6795,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Unicoder-VL: A universal encoder for vision and language by cross-modal pre-training,,5,587,coco
Flickr30k,AAAI,10.1609/aaai.v34i07.6795,10.1162/tacl_a_00166,https://aclanthology.org/Q14-1006.pdf,Unicoder-VL: A universal encoder for vision and language by cross-modal pre-training,The dataset paper linked here was not mentioned in the original paper but found in linked papers,5,587,flickr30k
VCR,AAAI,10.1609/aaai.v34i07.6795,10.48550/arXiv.1811.10830,https://arxiv.org/pdf/1811.10830,Unicoder-VL: A universal encoder for vision and language by cross-modal pre-training,,5,587,vcr
SynthText,AAAI,10.1609/aaai.v34i07.6812,10.1109/CVPR.2016.254,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7780623,Real-time scene text detection with differentiable binarization,,5,659,synthtext
RESIDE,AAAI,10.1609/aaai.v34i07.6865,10.48550/arXiv.1712.04143,https://arxiv.org/abs/1712.04143,FFA-Net: Feature fusion attention network for single image dehazing,,15,1272,reside
SOTS,AAAI,10.1609/aaai.v34i07.6865,N/A,N/A,FFA-Net: Feature fusion attention network for single image dehazing,No dataset paper could be found,15,1272,sots
ECSSD,AAAI,10.1609/aaai.v34i07.6916,10.1109/TPAMI.2015.2465960,https://arxiv.org/pdf/1408.5418,"F3Net: Fusion, feedback and focus for salient object detection","The original paper cites the CSSD dataset, but is actually referring to the ECSSD",5,899,ecssd
DUT-OMRON,AAAI,10.1609/aaai.v34i07.6916,10.1109/CVPR.2013.407,https://openaccess.thecvf.com/content_cvpr_2013/papers/Yang_Saliency_Detection_via_2013_CVPR_paper.pdf,"F3Net: Fusion, feedback and focus for salient object detection",,5,899,dut-omron
HKU-IS,AAAI,10.1609/aaai.v34i07.6916,10.1109/CVPR.2015.7299184,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7299184,"F3Net: Fusion, feedback and focus for salient object detection",,5,899,hku-is
DUTS,AAAI,10.1609/aaai.v34i07.6916,10.1109/CVPR.2017.404,https://saliencydetection.net/duts/,"F3Net: Fusion, feedback and focus for salient object detection",,5,899,duts
ILSVRC 2014,AAAI,10.1609/aaai.v34i07.6944,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,SiamFC++: Towards robust and accurate visual tracking with target estimation guidelines,,5,835,ilsvrc 2014
xiamen,AAAI,10.1609/aaai.v34i07.6944,10.1109/ICWS.2017.106,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=8029849,SiamFC++: Towards robust and accurate visual tracking with target estimation guidelines,,5,835,xiamen
Youtube-BB,AAAI,10.1609/aaai.v34i07.6944,10.48550/arXiv.1702.00824,https://arxiv.org/pdf/1702.00824,SiamFC++: Towards robust and accurate visual tracking with target estimation guidelines,,5,835,youtube-bb
LaSOT,AAAI,10.1609/aaai.v34i07.6944,10.1109/CVPR.2019.00552,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=8954084,SiamFC++: Towards robust and accurate visual tracking with target estimation guidelines,,5,835,lasot
GOT-10k,AAAI,10.1609/aaai.v34i07.6944,10.1109/TPAMI.2019.2957464,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=8922619,SiamFC++: Towards robust and accurate visual tracking with target estimation guidelines,,5,835,got-10k
PASCAL VOC 2007,AAAI,10.1609/aaai.v34i07.6999,10.1007/s11263-009-0275-4,https://link.springer.com/article/10.1007/s11263-009-0275-4#preview,Distance-IoU loss: Faster and better learning for bounding box regression,,15,3534,pascal voc 2007
COCO,AAAI,10.1609/aaai.v34i07.6999,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Distance-IoU loss: Faster and better learning for bounding box regression,,15,3534,coco
CIFAR-10,AAAI,10.1609/aaai.v34i07.7000,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Random erasing data augmentation,,15,2311,cifar-10
CIFAR-100,AAAI,10.1609/aaai.v34i07.7000,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Random erasing data augmentation,,15,2311,cifar-100
Fashion-MNIST,AAAI,10.1609/aaai.v34i07.7000,10.48550/arXiv.1708.07747,https://arxiv.org/pdf/1708.07747,Random erasing data augmentation,,15,2311,fashion-mnist
PASCAL VOC 2007,AAAI,10.1609/aaai.v34i07.7000,10.1007/s11263-009-0275-4,https://link.springer.com/article/10.1007/s11263-009-0275-4#preview,Random erasing data augmentation,,15,2311,pascal voc 2007
Market-1501,AAAI,10.1609/aaai.v34i07.7000,10.1109/ICCV.2015.133,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=7410490,Random erasing data augmentation,,15,2311,market-1501
DukeMTMC-reID,AAAI,10.1609/aaai.v34i07.7000,10.48550/arXiv.1609.01775,https://arxiv.org/pdf/1609.01775,Random erasing data augmentation,,15,2311,dukemtmc-reid
CUHK03,AAAI,10.1609/aaai.v34i07.7000,10.1109/CVPR.2014.27,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=6909421,Random erasing data augmentation,,15,2311,cuhk03
Conceptual Captions,AAAI,10.1609/aaai.v34i07.7005,N/A,https://aclanthology.org/P18-1238.pdf,Unified vision-language pre-training for image captioning and VQA,,5,661,conceptual captions
VQA 2.0,AAAI,10.1609/aaai.v34i07.7005,10.1109/CVPR.2017.670,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8100153&tag=1,Unified vision-language pre-training for image captioning and VQA,,5,661,vqa 2.0
Flickr30k,AAAI,10.1609/aaai.v34i07.7005,10.1162/tacl_a_00166,https://aclanthology.org/Q14-1006.pdf,Unified vision-language pre-training for image captioning and VQA,,5,661,flickr30k
ETT,AAAI,10.1609/aaai.v35i12.17325,10.1609/aaai.v35i12.17325,https://ojs.aaai.org/index.php/AAAI/article/view/17325,Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting,Dataset was briefly described in the original paper and a GitHub link was provided,15,3335,ett
ECL / Electricity,AAAI,10.1609/aaai.v35i12.17325,10.24432/C58C86,https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014,Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting,,15,3335,ecl / electricity
US Weather,AAAI,10.1609/aaai.v35i12.17325,N/A,https://www.ncei.noaa.gov/data/local-climatological-data/,Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting,,15,3335,us weather
KITTI,AAAI,10.1609/aaai.v35i2.16207,10.1109/CVPR.2012.6248074,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6248074,Voxel R-CNN: Towards High Performance Voxel-based 3D Object Detection,,5,651,kitti
Waymo,AAAI,10.1609/aaai.v35i2.16207,10.1109/CVPR42600.2020.00252,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9156973,Voxel R-CNN: Towards High Performance Voxel-based 3D Object Detection,,5,651,waymo
DOTA,AAAI,10.1609/aaai.v35i4.16426,10.1109/CVPR.2018.00418,https://arxiv.org/pdf/1711.10398,R3Det: Refined Single-Stage Detector with Feature Refinement for Rotating Object,,5,723,dota
UCAS-AOD,AAAI,10.1609/aaai.v35i4.16426,10.1109/ICIP.2015.7351502,https://ieeexplore.ieee.org/document/7351502,R3Det: Refined Single-Stage Detector with Feature Refinement for Rotating Object,,5,723,ucas-aod
HRSC2016,AAAI,10.1609/aaai.v35i4.16426,10.5220/0006120603240331,https://pdfs.semanticscholar.org/e6a3/2b4df848fd74b43486c5232ebd362eb90416.pdf,R3Det: Refined Single-Stage Detector with Feature Refinement for Rotating Object,,5,723,hrsc2016
ICDAR2015-Challenge-4,AAAI,10.1609/aaai.v35i4.16426,10.1109/ICDAR.2015.7333942,https://ieeexplore.ieee.org/document/7333942,R3Det: Refined Single-Stage Detector with Feature Refinement for Rotating Object,,5,723,icdar2015-challenge-4
SWaT,AAAI,10.1609/aaai.v35i5.16523,10.1109/CySWater.2016.7469060,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/7469060,Graph Neural Network-Based Anomaly Detection in Multivariate Time Series,,5,815,swat
WADI,AAAI,10.1609/aaai.v35i5.16523,10.1145/3055366.3055375,https://dl.acm.org/doi/pdf/10.1145/3055366.3055375,Graph Neural Network-Based Anomaly Detection in Multivariate Time Series,,5,815,wadi
PEMS03,AAAI,10.1609/aaai.v35i5.16542,10.1609/aaai.v34i01.5438,https://ojs.aaai.org/index.php/AAAI/article/view/5438,Spatial-Temporal Fusion Graph Neural Networks for Traffic Flow Forecasting,,5,656,pems03
PEMS04,AAAI,10.1609/aaai.v35i5.16542,10.1609/aaai.v34i01.5438,https://ojs.aaai.org/index.php/AAAI/article/view/5438,Spatial-Temporal Fusion Graph Neural Networks for Traffic Flow Forecasting,,5,656,pems04
PEMS07,AAAI,10.1609/aaai.v35i5.16542,10.1609/aaai.v34i01.5438,https://ojs.aaai.org/index.php/AAAI/article/view/5438,Spatial-Temporal Fusion Graph Neural Networks for Traffic Flow Forecasting,,5,656,pems07
PEMS08,AAAI,10.1609/aaai.v35i5.16542,10.1609/aaai.v34i01.5438,https://ojs.aaai.org/index.php/AAAI/article/view/5438,Spatial-Temporal Fusion Graph Neural Networks for Traffic Flow Forecasting,,5,656,pems08
Syn,AAAI,10.1609/aaai.v35i8.16826,10.48550/arXiv.1802.07814,https://arxiv.org/pdf/1802.07814,TabNet: Attentive Interpretable Tabular Learning,Paper uses 6 datasets from this dataset paper and calls them Syn1-6,5,812,syn
Covertype,AAAI,10.1609/aaai.v35i8.16826,10.24432/C50K5N,https://archive.ics.uci.edu/dataset/31/covertype,TabNet: Attentive Interpretable Tabular Learning,,5,812,covertype
Poker Hand,AAAI,10.1609/aaai.v35i8.16826,10.24432/C5KW38,https://archive.ics.uci.edu/dataset/158/poker+hand,TabNet: Attentive Interpretable Tabular Learning,,5,812,poker hand
HIGGS,AAAI,10.1609/aaai.v35i8.16826,10.24432/C5V312,https://archive.ics.uci.edu/dataset/280/higgs,TabNet: Attentive Interpretable Tabular Learning,,5,812,higgs
SACROS,AAAI,10.1609/aaai.v35i8.16826,10.5555/645529.657811,https://homepages.inf.ed.ac.uk/svijayak/publications/vijayakumar-ICML2000.pdf,TabNet: Attentive Interpretable Tabular Learning,,5,812,sacros
Rossmann Store Sales,AAAI,10.1609/aaai.v35i8.16826,N/A,https://www.kaggle.com/c/rossmann-store-sales,TabNet: Attentive Interpretable Tabular Learning,,5,812,rossmann store sales
GlaS,AAAI,10.1609/aaai.v36i3.20144,10.1016/j.media.2016.08.008,https://www.sciencedirect.com/science/article/pii/S1361841516301542,UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-Wise Perspective with Transformer,,5,578,glas
MoNuSeg,AAAI,10.1609/aaai.v36i3.20144,10.1109/TMI.2017.2677499,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872382,UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-Wise Perspective with Transformer,,5,578,monuseg
Synapse,AAAI,10.1609/aaai.v36i3.20144,10.7303/syn3193805,https://www.synapse.org/Synapse:syn3193805/wiki/89480,UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-Wise Perspective with Transformer,,5,578,synapse
URankerSet,AAAI,10.1609/aaai.v37i1.25147,10.1609/aaai.v37i1.25147,https://ojs.aaai.org/index.php/AAAI/article/view/25147,Underwater Ranker: Learn Which Is Better and How to Be Better,The dataset was introduced in the original paper,2,87,urankerset
CHAMELEON,AAAI,10.1609/aaai.v37i1.25167,N/A,https://www.polsl.pl/rau6/chameleon-database-animal-camouflage-analysis/,High-Resolution Iterative Feedback Network for Camouflaged Object Detection,,2,86,chameleon
CAMO,AAAI,10.1609/aaai.v37i1.25167,10.1016/j.cviu.2019.04.006,https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S1077314219300608,High-Resolution Iterative Feedback Network for Camouflaged Object Detection,,2,86,camo
COD10K,AAAI,10.1609/aaai.v37i1.25167,10.1109/CVPR42600.2020.00285,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=9156837,High-Resolution Iterative Feedback Network for Camouflaged Object Detection,,2,86,cod10k
NC4K,AAAI,10.1609/aaai.v37i1.25167,10.1109/CVPR46437.2021.01142,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=9577641,High-Resolution Iterative Feedback Network for Camouflaged Object Detection,,2,86,nc4k
nuScenes,AAAI,10.1609/aaai.v37i1.25185,10.1109/CVPR42600.2020.01164,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=9156412,PolarFormer: Multi-Camera 3D Object Detection with Polar Transformer,,2,89,nuscenes
MSMT17,AAAI,10.1609/aaai.v37i1.25225,10.1109/CVPR.2018.00016,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=8578114,CLIP-ReID: Exploiting Vision-Language Model for Image Re-identification without Concrete Text Labels,,2,101,msmt17
Market-1501,AAAI,10.1609/aaai.v37i1.25225,10.1109/ICCV.2015.133,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=7410490,CLIP-ReID: Exploiting Vision-Language Model for Image Re-identification without Concrete Text Labels,,2,101,market-1501
DukeMTMC-reID,AAAI,10.1609/aaai.v37i1.25225,10.48550/arXiv.1609.01775,https://arxiv.org/pdf/1609.01775,CLIP-ReID: Exploiting Vision-Language Model for Image Re-identification without Concrete Text Labels,,2,101,dukemtmc-reid
Occluded-DukeMTMC,AAAI,10.1609/aaai.v37i1.25225,10.1109/ICCV.2019.00063,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=9010704,CLIP-ReID: Exploiting Vision-Language Model for Image Re-identification without Concrete Text Labels,,2,101,occluded-dukemtmc
VeRi,AAAI,10.1609/aaai.v37i1.25225,10.1109/ICME.2016.7553002,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=7553002,CLIP-ReID: Exploiting Vision-Language Model for Image Re-identification without Concrete Text Labels,,2,101,veri
VehicleID,AAAI,10.1609/aaai.v37i1.25225,10.1109/CVPR.2016.238,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=7780607,CLIP-ReID: Exploiting Vision-Language Model for Image Re-identification without Concrete Text Labels,,2,101,vehicleid
GLUE,AAAI,10.1609/aaai.v37i11.26505,10.18653/v1/W18-5446,https://aclanthology.org/W18-5446.pdf,On the Effectiveness of Parameter-Efficient Fine-Tuning,,2,89,glue
SuperGLUE,AAAI,10.1609/aaai.v37i11.26505,10.48550/arXiv.1905.00537,https://papers.neurips.cc/paper_files/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf,On the Effectiveness of Parameter-Efficient Fine-Tuning,,2,89,superglue
CoLA,AAAI,10.1609/aaai.v37i11.26505,10.48550/arXiv.1805.12471,https://arxiv.org/pdf/1805.12471,On the Effectiveness of Parameter-Efficient Fine-Tuning,,2,89,cola
STSb,AAAI,10.1609/aaai.v37i11.26505,10.48550/arXiv.1708.00055,https://arxiv.org/pdf/1708.00055,On the Effectiveness of Parameter-Efficient Fine-Tuning,,2,89,stsb
MRPC,AAAI,10.1609/aaai.v37i11.26505,https://aclanthology.org/I05-5002/,https://aclanthology.org/I05-5002.pdf,On the Effectiveness of Parameter-Efficient Fine-Tuning,,2,89,mrpc
RTE-1,AAAI,10.1609/aaai.v37i11.26505,10.1007/11736790_9,https://link.springer.com/chapter/10.1007/11736790_9#preview,On the Effectiveness of Parameter-Efficient Fine-Tuning,Original paper only mentions RTE but cites this,2,89,rte-1
RTE-5,AAAI,10.1609/aaai.v37i11.26505,N/A,https://tac.nist.gov/publications/2009/additional.papers/RTE5_overview.proceedings.pdf,On the Effectiveness of Parameter-Efficient Fine-Tuning,Original paper only mentions RTE but cites this,2,89,rte-5
CommitmentBank,AAAI,10.1609/aaai.v37i11.26505,10.18148/sub/2019.v23i2.601,https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601,On the Effectiveness of Parameter-Efficient Fine-Tuning,,2,89,commitmentbank
COPA,AAAI,10.1609/aaai.v37i11.26505,N/A,https://cdn.aaai.org/ocs/2418/2418-10878-1-PB.pdf,On the Effectiveness of Parameter-Efficient Fine-Tuning,,2,89,copa
Winograd,AAAI,10.1609/aaai.v37i11.26505,N/A,https://cdn.aaai.org/ocs/4492/4492-21843-1-PB.pdf,On the Effectiveness of Parameter-Efficient Fine-Tuning,,2,89,winograd
TrOCR-PDF,AAAI,10.1609/aaai.v37i11.26538,10.1609/aaai.v37i11.26538,https://ojs.aaai.org/index.php/AAAI/article/view/26538,TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models,Dataset is created in the original paper,2,152,trocr-pdf
TrOCR-handwritten,AAAI,10.1609/aaai.v37i11.26538,10.1609/aaai.v37i11.26538,https://ojs.aaai.org/index.php/AAAI/article/view/26538,TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models,Dataset is created in the original paper,2,152,trocr-handwritten
IIIT-HWS,AAAI,10.1609/aaai.v37i11.26538,10.48550/arXiv.1608.04224,https://arxiv.org/pdf/1608.04224,TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models,,2,152,iiit-hws
TrOCR-Receipt,AAAI,10.1609/aaai.v37i11.26538,10.1609/aaai.v37i11.26538,https://ojs.aaai.org/index.php/AAAI/article/view/26538,TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models,,2,152,trocr-receipt
MJSynth,AAAI,10.1609/aaai.v37i11.26538,?,?,TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models,,2,152,mjsynth
SynthText,AAAI,10.1609/aaai.v37i11.26538,10.1109/CVPR.2016.254,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7780623,TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models,,2,152,synthtext
SROIE,AAAI,10.1609/aaai.v37i11.26538,N/A,https://paperswithcode.com/dataset/sroie,TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models,Dataset is not linked in the paper,2,152,sroie
IAM Handwriting,AAAI,10.1609/aaai.v37i11.26538,10.1007/s100320200071,https://link.springer.com/article/10.1007/s100320200071#preview,TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models,Dataset is not linked in the paper. Paper collects training set using the method in: https://github.com/jpuigcerver/Laia/tree/master/egs/iam.,2,152,iam handwriting
IIIT-5K,AAAI,10.1609/aaai.v37i11.26538,10.5244/C.26.127,https://www.bmva-archive.org.uk/bmvc/2012/BMVC/paper127/paper127.pdf,TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models,Dataset is wrongly referenced,2,152,iiit-5k
SVT,AAAI,10.1609/aaai.v37i11.26538,10.1007/978-3-642-15549-9_43,https://link.springer.com/chapter/10.1007/978-3-642-15549-9_43#preview,TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models,"In the original paper, the dataset is linked to a paper from which the current dataset paper was taken. The original paper also refers to a dataset ""SVT-647"" and it is unclear what the numbers are refering to.",2,152,svt
ICDAR2013,AAAI,10.1609/aaai.v37i11.26538,10.1109/ICDAR.2013.221,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=6628859,TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models,"The original paper specifies datasets IC13-857 and IC13-1015, but it is unclear to what subsets of ICDAR2013 they are refering to",2,152,icdar2013
ICDAR2015-Challenge-4,AAAI,10.1609/aaai.v37i11.26538,10.1109/ICDAR.2015.7333942,https://ieeexplore.ieee.org/document/7333942,TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models,"The original paper specifies datasets IC15-1811 and IC15-2077, but it is unclear to what subsets of ICDAR2015 they are refering to",2,152,icdar2015-challenge-4
SVTP,AAAI,10.1609/aaai.v37i11.26538,10.1109/ICCV.2013.76,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=6751180,TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models,"The original paper references ""SVTP-645"" - it is unclear what the numbers are refering to.",2,152,svtp
CUTE80,AAAI,10.1609/aaai.v37i11.26538,10.1016/j.eswa.2014.07.008,https://www.sciencedirect.com/science/article/pii/S0957417414004060,TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models,"The original paper references ""CT80-288"" - it is unclear what the numbers are refering to.",2,152,cute80
nuScenes,AAAI,10.1609/aaai.v37i2.25233,10.1109/CVPR42600.2020.01164,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=9156412,BEVDepth: Acquisition of Reliable Depth for Multi-View 3D Object Detection,,2,287,nuscenes
nuScenes,AAAI,10.1609/aaai.v37i2.25234,10.1109/CVPR42600.2020.01164,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=9156412,BEVStereo: Enhancing Depth Estimation in Multi-View 3D Object Detection with Temporal Stereo,,2,96,nuscenes
CIFAR-100,AAAI,10.1609/aaai.v37i2.25236,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Curriculum Temperature for Knowledge Distillation,,2,103,cifar-100
ImageNet,AAAI,10.1609/aaai.v37i2.25236,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Curriculum Temperature for Knowledge Distillation,,2,103,imagenet
COCO,AAAI,10.1609/aaai.v37i2.25236,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Curriculum Temperature for Knowledge Distillation,,2,103,coco
SYSU-MM01,AAAI,10.1609/aaai.v37i2.25273,10.1109/ICCV.2017.575,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=8237837,Learning Progressive Modality-Shared Transformers for Effective Visible-Infrared Person Re-identification,,2,105,sysu-mm01
RegDB,AAAI,10.1609/aaai.v37i2.25273,10.3390/s17030605,https://www.mdpi.com/1424-8220/17/3/605,Learning Progressive Modality-Shared Transformers for Effective Visible-Infrared Person Re-identification,,2,105,regdb
GoPro,AAAI,10.1609/aaai.v37i2.25281,10.1109/CVPR.2017.35,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=8099518,Intriguing Findings of Frequency Selection for Image Deblurring,,2,108,gopro
HIDE,AAAI,10.1609/aaai.v37i2.25281,10.1109/ICCV.2019.00567,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?arnumber=9010839,Intriguing Findings of Frequency Selection for Image Deblurring,,2,108,hide
RealBlur,AAAI,10.1609/aaai.v37i2.25281,N/A,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700188.pdf,Intriguing Findings of Frequency Selection for Image Deblurring,,2,108,realblur
REDS,AAAI,10.1609/aaai.v37i2.25281,10.1109/CVPRW.2019.00251,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=9025509,Intriguing Findings of Frequency Selection for Image Deblurring,,2,108,reds
LIVE In the Wild,AAAI,10.1609/aaai.v37i2.25353,10.1109/TIP.2015.2500021,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=7327186,Exploring CLIP for Assessing the Look and Feel of Images,,2,222,live in the wild
KonIQ-10k,AAAI,10.1609/aaai.v37i2.25353,10.1109/TIP.2020.2967829,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=8968750,Exploring CLIP for Assessing the Look and Feel of Images,,2,222,koniq-10k
SPAQ,AAAI,10.1609/aaai.v37i2.25353,10.1109/CVPR42600.2020.00373,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=9156490,Exploring CLIP for Assessing the Look and Feel of Images,,2,222,spaq
LOL,AAAI,10.1609/aaai.v37i2.25353,10.48550/arXiv.1808.04560,https://arxiv.org/abs/1808.04560,Exploring CLIP for Assessing the Look and Feel of Images,,2,222,lol
MIT-Adobe FiveK,AAAI,10.1609/aaai.v37i2.25353,10.1109/CVPR.2011.5995332,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=5995332,Exploring CLIP for Assessing the Look and Feel of Images,,2,222,mit-adobe fivek
CLIP User Study,AAAI,10.1609/aaai.v37i2.25353,10.1609/aaai.v37i2.25353,https://ojs.aaai.org/index.php/AAAI/article/view/25353,Exploring CLIP for Assessing the Look and Feel of Images,Dataset was constructed in the paper itself,2,222,clip user study
UHD-LOL,AAAI,10.1609/aaai.v37i3.25364,10.1609/aaai.v37i3.25364,https://ojs.aaai.org/index.php/AAAI/article/view/25364,Ultra-High-Definition Low-Light Image Enhancement: A Benchmark and Transformer-Based Method,Introduced and used in the paper,2,200,uhd-lol
DARK FACE,AAAI,10.1609/aaai.v37i3.25364,10.1109/TIP.2020.2981922,https://www.researchgate.net/publication/340238487_Advancing_Image_Understanding_in_Poor_Visibility_Environments_A_Collective_Benchmark_Study,Ultra-High-Definition Low-Light Image Enhancement: A Benchmark and Transformer-Based Method,,2,200,dark face
LOL,AAAI,10.1609/aaai.v37i3.25364,10.48550/arXiv.1808.04560,https://arxiv.org/abs/1808.04560,Ultra-High-Definition Low-Light Image Enhancement: A Benchmark and Transformer-Based Method,,2,200,lol
MIT-Adobe FiveK,AAAI,10.1609/aaai.v37i3.25364,10.1109/CVPR.2011.5995332,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=5995332,Ultra-High-Definition Low-Light Image Enhancement: A Benchmark and Transformer-Based Method,,2,200,mit-adobe fivek
BikeNYC-1,AAAI,10.1609/aaai.v37i4.25555,10.1609/aaai.v31i1.10735,https://ojs.aaai.org/index.php/AAAI/article/view/10735,Spatio-Temporal Self-Supervised Learning for Traffic Flow Prediction,,2,131,bikenyc-1
BikeNYC-2,AAAI,10.1609/aaai.v37i4.25555,10.1609/aaai.v33i01.33015668,https://ojs.aaai.org/index.php/AAAI/article/view/4511,Spatio-Temporal Self-Supervised Learning for Traffic Flow Prediction,,2,131,bikenyc-2
TaxiBJ,AAAI,10.1609/aaai.v37i4.25555,10.1609/aaai.v31i1.10735,https://ojs.aaai.org/index.php/AAAI/article/view/10735,Spatio-Temporal Self-Supervised Learning for Traffic Flow Prediction,,2,131,taxibj
TaxiNYC,AAAI,10.1609/aaai.v37i4.25555,10.1609/aaai.v33i01.33015668,https://ojs.aaai.org/index.php/AAAI/article/view/4511,Spatio-Temporal Self-Supervised Learning for Traffic Flow Prediction,,2,131,taxinyc
PEMS04,AAAI,10.1609/aaai.v37i4.25556,10.1609/aaai.v34i01.5438,https://ojs.aaai.org/index.php/AAAI/article/view/5438,PDFormer: Propagation Delay-Aware Dynamic Long-Range Transformer for Traffic Flow Prediction,,2,192,pems04
PEMS07,AAAI,10.1609/aaai.v37i4.25556,10.1609/aaai.v34i01.5438,https://ojs.aaai.org/index.php/AAAI/article/view/5438,PDFormer: Propagation Delay-Aware Dynamic Long-Range Transformer for Traffic Flow Prediction,,2,192,pems07
PEMS08,AAAI,10.1609/aaai.v37i4.25556,10.1609/aaai.v34i01.5438,https://ojs.aaai.org/index.php/AAAI/article/view/5438,PDFormer: Propagation Delay-Aware Dynamic Long-Range Transformer for Traffic Flow Prediction,,2,192,pems08
NYTaxi,AAAI,10.1609/aaai.v37i4.25556,N/A,https://bigscity-libcity-docs.readthedocs.io/en/latest/user_guide/data/raw_data.html#nyctaxi,PDFormer: Propagation Delay-Aware Dynamic Long-Range Transformer for Traffic Flow Prediction,The original paper cites another paper which does not contain this dataset.,2,192,nytaxi
BIKECHI,AAAI,10.1609/aaai.v37i4.25556,10.1145/3474717.3483923,https://bigscity-libcity-docs.readthedocs.io/en/latest/user_guide/data/raw_data.html#bikechi,PDFormer: Propagation Delay-Aware Dynamic Long-Range Transformer for Traffic Flow Prediction,"The link to this dataset was very hard to find as the paper that is referenced mentions a website which contains this dataset. The dataset description was itself very hard to find. Furthermore, the original paper calls this dataset CHBike, making it harder to find the original dataset.",2,192,bikechi
T-Drive,AAAI,10.1609/aaai.v37i4.25556,10.1145/1869790.1869807,https://dl.acm.org/doi/pdf/10.1145/1869790.1869807,PDFormer: Propagation Delay-Aware Dynamic Long-Range Transformer for Traffic Flow Prediction,"In the original paper, the dataset is linked to a paper from which the current dataset paper was taken",2,192,t-drive
ETT,AAAI,10.1609/aaai.v37i6.25854,10.1609/aaai.v35i12.17325,https://ojs.aaai.org/index.php/AAAI/article/view/17325,NHITS: Neural Hierarchical Interpolation for Time Series Forecasting,No dataset link or reference was provided,2,212,ett
Exchange-Rate,AAAI,10.1609/aaai.v37i6.25854,10.48550/arXiv.1703.07015,https://arxiv.org/pdf/1703.07015,NHITS: Neural Hierarchical Interpolation for Time Series Forecasting,No dataset link or reference was provided,2,212,exchange-rate
ECL / Electricity,AAAI,10.1609/aaai.v37i6.25854,10.24432/C58C86,https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014,NHITS: Neural Hierarchical Interpolation for Time Series Forecasting,No dataset link or reference was provided,2,212,ecl / electricity
Traffic,AAAI,10.1609/aaai.v37i6.25854,N/A,N/A,NHITS: Neural Hierarchical Interpolation for Time Series Forecasting,No dataset link or reference was provided,2,212,traffic
Jena Weather,AAAI,10.1609/aaai.v37i6.25854,N/A,https://www.bgc-jena.mpg.de/wetter/,NHITS: Neural Hierarchical Interpolation for Time Series Forecasting,No dataset link or reference was provided,2,212,jena weather
ILI,AAAI,10.1609/aaai.v37i6.25854,N/A,https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html,NHITS: Neural Hierarchical Interpolation for Time Series Forecasting,No dataset link or reference was provided,2,212,ili
Adult,AAAI,10.1609/aaai.v37i6.25911,10.24432/C5XW20,https://archive.ics.uci.edu/dataset/2/adult,FairFed: Enabling Group Fairness in Federated Learning,,2,86,adult
COMPAS,AAAI,10.1609/aaai.v37i6.25911,N/A,https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm,FairFed: Enabling Group Fairness in Federated Learning,,2,86,compas
ACSIncome,AAAI,10.1609/aaai.v37i6.25911,10.48550/arXiv.2108.04884,https://proceedings.neurips.cc/paper/2021/file/32e54441e6382a7fbacbbbaf3c450059-Paper.pdf,FairFed: Enabling Group Fairness in Federated Learning,,2,86,acsincome
TILES-2018,AAAI,10.1609/aaai.v37i6.25911,10.1038/s41597-020-00655-3,https://www.nature.com/articles/s41597-020-00655-3.pdf,FairFed: Enabling Group Fairness in Federated Learning,,2,86,tiles-2018
METR-LA,AAAI,10.1609/aaai.v37i7.25976,https://openreview.net/forum?id=SJiHXGWAZ,https://openreview.net/pdf?id=SJiHXGWAZ,Spatio-Temporal Meta-Graph Learning for Traffic Forecasting,,2,134,metr-la
PEMS-BAY,AAAI,10.1609/aaai.v37i7.25976,https://openreview.net/forum?id=SJiHXGWAZ,https://openreview.net/pdf?id=SJiHXGWAZ,Spatio-Temporal Meta-Graph Learning for Traffic Forecasting,,2,134,pems-bay
EXPY-TKY,AAAI,10.1609/aaai.v37i7.25976,10.1609/aaai.v37i7.25976,https://ojs.aaai.org/index.php/AAAI/article/view/25976,Spatio-Temporal Meta-Graph Learning for Traffic Forecasting,Introduced and used in the paper,2,134,expy-tky
MUTAG,AAAI,10.1609/aaai.v37i8.26187,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Federated Learning on Non-IID Graphs via Structural Knowledge Sharing,"In the original paper, the dataset is linked to a paper from which the current dataset paper was taken",2,92,mutag
NCI1,AAAI,10.1609/aaai.v37i8.26187,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Federated Learning on Non-IID Graphs via Structural Knowledge Sharing,"In the original paper, the dataset is linked to a paper from which the current dataset paper was taken",2,92,nci1
DD,AAAI,10.1609/aaai.v37i8.26187,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Federated Learning on Non-IID Graphs via Structural Knowledge Sharing,"In the original paper, the dataset is linked to a paper from which the current dataset paper was taken",2,92,dd
PROTEINS,AAAI,10.1609/aaai.v37i8.26187,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Federated Learning on Non-IID Graphs via Structural Knowledge Sharing,"In the original paper, the dataset is linked to a paper from which the current dataset paper was taken",2,92,proteins
BZR,AAAI,10.1609/aaai.v37i8.26187,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Federated Learning on Non-IID Graphs via Structural Knowledge Sharing,"In the original paper, the dataset is linked to a paper from which the current dataset paper was taken",2,92,bzr
COX2,AAAI,10.1609/aaai.v37i8.26187,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Federated Learning on Non-IID Graphs via Structural Knowledge Sharing,"In the original paper, the dataset is linked to a paper from which the current dataset paper was taken",2,92,cox2
DHFR,AAAI,10.1609/aaai.v37i8.26187,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Federated Learning on Non-IID Graphs via Structural Knowledge Sharing,"In the original paper, the dataset is linked to a paper from which the current dataset paper was taken",2,92,dhfr
PTC MR,AAAI,10.1609/aaai.v37i8.26187,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Federated Learning on Non-IID Graphs via Structural Knowledge Sharing,"In the original paper, the dataset is linked to a paper from which the current dataset paper was taken",2,92,ptc mr
AIDS,AAAI,10.1609/aaai.v37i8.26187,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Federated Learning on Non-IID Graphs via Structural Knowledge Sharing,"In the original paper, the dataset is linked to a paper from which the current dataset paper was taken",2,92,aids
ENZYMES,AAAI,10.1609/aaai.v37i8.26187,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Federated Learning on Non-IID Graphs via Structural Knowledge Sharing,"In the original paper, the dataset is linked to a paper from which the current dataset paper was taken",2,92,enzymes
COLLAB,AAAI,10.1609/aaai.v37i8.26187,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Federated Learning on Non-IID Graphs via Structural Knowledge Sharing,"In the original paper, the dataset is linked to a paper from which the current dataset paper was taken",2,92,collab
IMDB-BINARY,AAAI,10.1609/aaai.v37i8.26187,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Federated Learning on Non-IID Graphs via Structural Knowledge Sharing,"In the original paper, the dataset is linked to a paper from which the current dataset paper was taken",2,92,imdb-binary
IMDB-MULTI,AAAI,10.1609/aaai.v37i8.26187,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Federated Learning on Non-IID Graphs via Structural Knowledge Sharing,"In the original paper, the dataset is linked to a paper from which the current dataset paper was taken",2,92,imdb-multi
Letter-low,AAAI,10.1609/aaai.v37i8.26187,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Federated Learning on Non-IID Graphs via Structural Knowledge Sharing,,2,92,letter-low
Letter-high,AAAI,10.1609/aaai.v37i8.26187,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Federated Learning on Non-IID Graphs via Structural Knowledge Sharing,,2,92,letter-high
Letter-med,AAAI,10.1609/aaai.v37i8.26187,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Federated Learning on Non-IID Graphs via Structural Knowledge Sharing,,2,92,letter-med
DBLP-HGB,AAAI,10.1609/aaai.v37i9.26283,10.1145/3447548.3467350,https://dl-acm-org.tudelft.idm.oclc.org/doi/pdf/10.1145/3447548.3467350,Simple and Efficient Heterogeneous Graph Neural Network,,2,117,dblp-hgb
IMDB-HGB,AAAI,10.1609/aaai.v37i9.26283,10.1145/3447548.3467350,https://dl-acm-org.tudelft.idm.oclc.org/doi/pdf/10.1145/3447548.3467350,Simple and Efficient Heterogeneous Graph Neural Network,,2,117,imdb-hgb
ACM-HGB,AAAI,10.1609/aaai.v37i9.26283,10.1145/3447548.3467350,https://dl-acm-org.tudelft.idm.oclc.org/doi/pdf/10.1145/3447548.3467350,Simple and Efficient Heterogeneous Graph Neural Network,,2,117,acm-hgb
Freebase-HGB,AAAI,10.1609/aaai.v37i9.26283,10.1145/3447548.3467350,https://dl-acm-org.tudelft.idm.oclc.org/doi/pdf/10.1145/3447548.3467350,Simple and Efficient Heterogeneous Graph Neural Network,,2,117,freebase-hgb
ogbn-mag,AAAI,10.1609/aaai.v37i9.26283,10.48550/arXiv.2005.00691,https://arxiv.org/pdf/2005.00687,Simple and Efficient Heterogeneous Graph Neural Network,The original paper cites a paper from which this dataset paper is taken,2,117,ogbn-mag
ETT,AAAI,10.1609/aaai.v37i9.26317,10.1609/aaai.v35i12.17325,https://ojs.aaai.org/index.php/AAAI/article/view/17325,Are Transformers Effective for Time Series Forecasting?,,5,1137,ett
Traffic,AAAI,10.1609/aaai.v37i9.26317,N/A,N/A,Are Transformers Effective for Time Series Forecasting?,,5,1137,traffic
ECL / Electricity,AAAI,10.1609/aaai.v37i9.26317,10.24432/C58C86,https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014,Are Transformers Effective for Time Series Forecasting?,"Dataset ""Electricity"" is used; unclear to which dataset it is referring to, but ECL was linked in one of the referenced papers",5,1137,ecl / electricity
Jena Weather,AAAI,10.1609/aaai.v37i9.26317,N/A,https://www.bgc-jena.mpg.de/wetter/,Are Transformers Effective for Time Series Forecasting?,"Dataset ""Weather"" is used; check appendix for specification",5,1137,jena weather
ILI,AAAI,10.1609/aaai.v37i9.26317,N/A,https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html,Are Transformers Effective for Time Series Forecasting?,,5,1137,ili
Exchange-Rate,AAAI,10.1609/aaai.v37i9.26317,10.48550/arXiv.1703.07015,https://arxiv.org/pdf/1703.07015,Are Transformers Effective for Time Series Forecasting?,,5,1137,exchange-rate
MNIST,AAAI,10.1609/aaai.v37i9.26330,10.1109/5.726791,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=726791,FedALA: Adaptive Local Aggregation for Personalized Federated Learning,,2,172,mnist
CIFAR-10,AAAI,10.1609/aaai.v37i9.26330,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,FedALA: Adaptive Local Aggregation for Personalized Federated Learning,,2,172,cifar-10
CIFAR-100,AAAI,10.1609/aaai.v37i9.26330,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,FedALA: Adaptive Local Aggregation for Personalized Federated Learning,,2,172,cifar-100
Tiny ImageNet,AAAI,10.1609/aaai.v37i9.26330,N/A,https://paperswithcode.com/dataset/tiny-imagenet,FedALA: Adaptive Local Aggregation for Personalized Federated Learning,The dataset paper referenced in the original paper is not the source for Tiny ImageNet,2,172,tiny imagenet
AG's News,AAAI,10.1609/aaai.v37i9.26330,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,FedALA: Adaptive Local Aggregation for Personalized Federated Learning,,2,172,ag's news
RGB,AAAI,10.1609/aaai.v38i16.29728,10.1609/aaai.v38i16.29728,https://ojs.aaai.org/index.php/AAAI/article/view/29728,Benchmarking Large Language Models in Retrieval-Augmented Generation,Introduced and used in the paper,2,125,rgb
COCO-Stuff,AAAI,10.1609/aaai.v38i5.28226,10.48550/arXiv.1612.03716,https://openaccess.thecvf.com/content_cvpr_2018/papers/Caesar_COCO-Stuff_Thing_and_CVPR_2018_paper.pdf,T2I-Adapter: Learning Adapters to Dig Out More Controllable Ability for Text-to-Image Diffusion Models,,2,118,coco-stuff
LAION-Aesthetic,AAAI,10.1609/aaai.v38i5.28226,https://github.com/LAION-AI/laion-datasets/blob/main/laion-aesthetic.md,https://github.com/LAION-AI/laion-datasets/blob/main/laion-aesthetic.md,T2I-Adapter: Learning Adapters to Dig Out More Controllable Ability for Text-to-Image Diffusion Models,Paper cites dataset LAION-5B but this is the subset it is refering to,2,118,laion-aesthetic
COCO,AAAI,10.1609/aaai.v38i5.28226,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,T2I-Adapter: Learning Adapters to Dig Out More Controllable Ability for Text-to-Image Diffusion Models,Dataset was used for testing,2,118,coco
SQuAD,ACL,10.1162/tacl_a_00300,10.18653/v1/D16-1264,https://arxiv.org/pdf/1606.05250,Spanbert: Improving pre-training by representing and predicting spans,,5,1355,squad
SQuADv2,ACL,10.1162/tacl_a_00300,10.48550/arXiv.1806.03822,https://arxiv.org/pdf/1806.03822,Spanbert: Improving pre-training by representing and predicting spans,,5,1355,squadv2
MRQA,ACL,10.1162/tacl_a_00300,Not Found,Not Found,Spanbert: Improving pre-training by representing and predicting spans,,5,1355,mrqa
NewsQA,ACL,10.1162/tacl_a_00300,10.18653/v1/W17-2623,https://aclanthology.org/W17-2623.pdf,Spanbert: Improving pre-training by representing and predicting spans,,5,1355,newsqa
SearchQA,ACL,10.1162/tacl_a_00300,10.48550/arXiv.1704.05179,https://arxiv.org/pdf/1704.05179v3,Spanbert: Improving pre-training by representing and predicting spans,,5,1355,searchqa
TriviaQA,ACL,10.1162/tacl_a_00300,10.18653/v1/P17-1147,https://aclanthology.org/P17-1147.pdf,Spanbert: Improving pre-training by representing and predicting spans,,5,1355,triviaqa
HotpotQA,ACL,10.1162/tacl_a_00300,10.18653/v1/D18-1259,https://aclanthology.org/D18-1259.pdf,Spanbert: Improving pre-training by representing and predicting spans,,5,1355,hotpotqa
NQ,ACL,10.1162/tacl_a_00300,10.1162/tacl_a_00276,https://aclanthology.org/Q19-1026.pdf,Spanbert: Improving pre-training by representing and predicting spans,,5,1355,nq
CoNLL-2012,ACL,10.1162/tacl_a_00300,N/A,https://aclanthology.org/W12-4501/,Spanbert: Improving pre-training by representing and predicting spans,,5,1355,conll-2012
TACRED,ACL,10.1162/tacl_a_00300,10.18653/v1/D17-1004,https://aclanthology.org/D17-1004.pdf,Spanbert: Improving pre-training by representing and predicting spans,,5,1355,tacred
GLUE,ACL,10.1162/tacl_a_00300,10.18653/v1/W18-5446,https://aclanthology.org/W18-5446.pdf,Spanbert: Improving pre-training by representing and predicting spans,,5,1355,glue
BookCorpus,ACL,10.1162/tacl_a_00300,10.1109/ICCV.2015.11,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=7410368,Spanbert: Improving pre-training by representing and predicting spans,,5,1355,bookcorpus
English Wikipedia,ACL,10.1162/tacl_a_00300,N/A,N/A,Spanbert: Improving pre-training by representing and predicting spans,"same as BERT, unspecified",5,1355,english wikipedia
LAMA,ACL,10.1162/tacl_a_00324,10.18653/v1/D19-1250,https://aclanthology.org/D19-1250.pdf,How can we know what language models know?,,5,957,lama
CCNet,ACL,10.1162/tacl_a_00343,10.48550/arXiv.1911.00359,https://arxiv.org/pdf/1911.00359,Multilingual denoising pre-training for neural machine translation,,5,1224,ccnet
IWSLT17,ACL,10.1162/tacl_a_00343,?,https://aclanthology.org/2017.iwslt-1.1.pdf,Multilingual denoising pre-training for neural machine translation,,5,1224,iwslt17
IWSLT15,ACL,10.1162/tacl_a_00343,?,https://aclanthology.org/2015.iwslt-evaluation.1.pdf,Multilingual denoising pre-training for neural machine translation,,5,1224,iwslt15
IWSLT14,ACL,10.1162/tacl_a_00343,?,https://aclanthology.org/2014.iwslt-evaluation.1.pdf,Multilingual denoising pre-training for neural machine translation,,5,1224,iwslt14
IWSLT13,ACL,10.1162/tacl_a_00343,?,https://aclanthology.org/2013.iwslt-evaluation.1.pdf,Multilingual denoising pre-training for neural machine translation,,5,1224,iwslt13
IWSLT12,ACL,10.1162/tacl_a_00343,?,https://aclanthology.org/2012.iwslt-evaluation.1.pdf,Multilingual denoising pre-training for neural machine translation,,5,1224,iwslt12
IWSLT11,ACL,10.1162/tacl_a_00343,?,https://aclanthology.org/2011.iwslt-evaluation.1.pdf,Multilingual denoising pre-training for neural machine translation,,5,1224,iwslt11
IWSLT10,ACL,10.1162/tacl_a_00343,?,https://aclanthology.org/2010.iwslt-evaluation.1.pdf,Multilingual denoising pre-training for neural machine translation,,5,1224,iwslt10
CCNet,ACL,10.1162/tacl_a_00343,10.48550/arXiv.1911.00359,https://arxiv.org/pdf/1911.00359,Multilingual denoising pre-training for neural machine translation,,5,1224,ccnet
WAT19,ACL,10.1162/tacl_a_00343,N/A,https://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/#research-paper.html,Multilingual denoising pre-training for neural machine translation,,5,1224,wat19
FLORES,ACL,10.1162/tacl_a_00343,10.18653/v1/D19-1632,https://aclanthology.org/D19-1632.pdf,Multilingual denoising pre-training for neural machine translation,,5,1224,flores
IITB,ACL,10.1162/tacl_a_00343,10.48550/arXiv.1710.02855,https://arxiv.org/pdf/1710.02855,Multilingual denoising pre-training for neural machine translation,,5,1224,iitb
WMT19,ACL,10.1162/tacl_a_00343,10.18653/v1/W19-5301,https://aclanthology.org/W19-5301.pdf,Multilingual denoising pre-training for neural machine translation,WMT not referenced!!!,5,1224,wmt19
WMT18,ACL,10.1162/tacl_a_00343,10.18653/v1/W18-6401,https://aclanthology.org/W18-6401.pdf,Multilingual denoising pre-training for neural machine translation,,5,1224,wmt18
WMT17,ACL,10.1162/tacl_a_00343,10.18653/v1/W17-4717,https://aclanthology.org/W17-4717/,Multilingual denoising pre-training for neural machine translation,,5,1224,wmt17
WMT16,ACL,10.1162/tacl_a_00343,10.18653/v1/W16-2301,https://aclanthology.org/W16-2301.pdf,Multilingual denoising pre-training for neural machine translation,,5,1224,wmt16
WikiText-103,ACL,10.1162/tacl_a_00605,10.48550/arXiv.1609.07843,https://arxiv.org/pdf/1609.07843,In-Context Retrieval-Augmented Language Models,,2,124,wikitext-103
RealNews,ACL,10.1162/tacl_a_00605,10.48550/arXiv.1905.12616,https://arxiv.org/pdf/1905.12616,In-Context Retrieval-Augmented Language Models,,2,124,realnews
ArXiv,ACL,10.1162/tacl_a_00605,10.48550/arXiv.2101.00027,https://arxiv.org/pdf/2101.00027,In-Context Retrieval-Augmented Language Models,,2,124,arxiv
Stack Exchange,ACL,10.1162/tacl_a_00605,10.48550/arXiv.2101.00027,https://arxiv.org/pdf/2101.00027,In-Context Retrieval-Augmented Language Models,,2,124,stack exchange
FreeLaw,ACL,10.1162/tacl_a_00605,10.48550/arXiv.2101.00027,https://arxiv.org/pdf/2101.00027,In-Context Retrieval-Augmented Language Models,,2,124,freelaw
NQ,ACL,10.1162/tacl_a_00605,10.1162/tacl_a_00276,https://aclanthology.org/Q19-1026.pdf,In-Context Retrieval-Augmented Language Models,,2,124,nq
TriviaQA,ACL,10.1162/tacl_a_00605,10.18653/v1/P17-1147,https://aclanthology.org/P17-1147.pdf,In-Context Retrieval-Augmented Language Models,,2,124,triviaqa
OpenDomainQA,ACL,10.1162/tacl_a_00605,10.18653/v1/P17-1171,https://aclanthology.org/P17-1171.pdf,In-Context Retrieval-Augmented Language Models,,2,124,opendomainqa
LLMSUM,ACL,10.1162/tacl_a_00632,10.18653/v1/2023.eacl-main.148,https://aclanthology.org/2024.tacl-1.3.pdf,Benchmarking Large Language Models for News Summarization,,2,110,llmsum
NaturalQuestions-Open,ACL,10.1162/tacl_a_00638,10.18653/v1/P19-1612,https://aclanthology.org/P19-1612.pdf,Lost in the Middle: How Language Models Use Long Contexts,,2,234,naturalquestions-open
UDv2.5,ACL,10.18653/v1/2020.acl-demos.14,N/A,https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3105,Stanza: A Python natural language processing toolkit for many human languages,,5,1060,udv2.5
CoNLL-2002,ACL,10.18653/v1/2020.acl-demos.14,N/A,https://aclanthology.org/W02-2024.pdf,Stanza: A Python natural language processing toolkit for many human languages,,5,1060,conll-2002
CoNLL-2003,ACL,10.18653/v1/2020.acl-demos.14,N/A,https://aclanthology.org/W03-0419/,Stanza: A Python natural language processing toolkit for many human languages,,5,1060,conll-2003
WikiNER,ACL,10.18653/v1/2020.acl-demos.14,10.1016/j.artint.2012.03.006,https://www.sciencedirect.com/science/article/pii/S0004370212000276?ref=pdf_download&fr=RR-2&rr=93ed42b46d1d0e7f,Stanza: A Python natural language processing toolkit for many human languages,,5,1060,wikiner
GermEval14,ACL,10.18653/v1/2020.acl-demos.14,N/A,http://www.lrec-conf.org/proceedings/lrec2014/pdf/276_Paper.pdf,Stanza: A Python natural language processing toolkit for many human languages,,5,1060,germeval14
AQMAR,ACL,10.18653/v1/2020.acl-demos.14,N/A,https://aclanthology.org/E12-1017.pdf,Stanza: A Python natural language processing toolkit for many human languages,,5,1060,aqmar
AnCora,ACL,10.18653/v1/2020.acl-demos.14,N/A,http://www.lrec-conf.org/proceedings/lrec2008/pdf/35_paper.pdf,Stanza: A Python natural language processing toolkit for many human languages,,5,1060,ancora
OntoNotes,ACL,10.18653/v1/2020.acl-demos.14,?,https://aclanthology.org/W13-3516.pdf,Stanza: A Python natural language processing toolkit for many human languages,,5,1060,ontonotes
WMT19,ACL,10.18653/v1/2020.acl-demos.14,10.18653/v1/W19-5301,https://aclanthology.org/W19-5301.pdf,Stanza: A Python natural language processing toolkit for many human languages,,5,1060,wmt19
1B Word,ACL,10.18653/v1/2020.acl-demos.14,10.48550/arXiv.1312.3005,https://arxiv.org/pdf/1312.3005,Stanza: A Python natural language processing toolkit for many human languages,,5,1060,1b word
Chinese Gigaword,ACL,10.18653/v1/2020.acl-demos.14,10.35111/102m-dr17,https://catalog.ldc.upenn.edu/LDC2011T13,Stanza: A Python natural language processing toolkit for many human languages,,5,1060,chinese gigaword
Wikipedia dumps,ACL,10.18653/v1/2020.acl-demos.14,Not Found,Not Found,Stanza: A Python natural language processing toolkit for many human languages,Unspecified,5,1060,wikipedia dumps
Common crawl dumps,ACL,10.18653/v1/2020.acl-demos.14,Not Found,Not Found,Stanza: A Python natural language processing toolkit for many human languages,Unspecified,5,1060,common crawl dumps
160GB of text,ACL,10.18653/v1/2020.acl-main.703,Not Found,Not Found,"BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",actual (pre-)training data is referenced in another paper,15,5532,160gb of text
SQuAD,ACL,10.18653/v1/2020.acl-main.703,10.18653/v1/D16-1264,https://arxiv.org/pdf/1606.05250,"BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",,15,5532,squad
Multi-NLI,ACL,10.18653/v1/2020.acl-main.703,10.18653/v1/N18-1101,https://aclanthology.org/N18-1101.pdf,"BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",,15,5532,multi-nli
ELI5,ACL,10.18653/v1/2020.acl-main.703,10.18653/v1/P19-1346,https://aclanthology.org/P19-1346.pdf,"BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",,15,5532,eli5
XSum,ACL,10.18653/v1/2020.acl-main.703,10.18653/v1/D18-1206,https://aclanthology.org/D18-1206.pdf,"BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",,15,5532,xsum
ConvAI2,ACL,10.18653/v1/2020.acl-main.703,10.48550/arXiv.1902.00098,https://arxiv.org/pdf/1902.00098,"BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",,15,5532,convai2
CNN/DM,ACL,10.18653/v1/2020.acl-main.703,10.48550/arXiv.1506.03340,https://arxiv.org/pdf/1506.03340,"BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",,15,5532,cnn/dm
AG's News,ACL,10.18653/v1/2020.acl-main.740,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Don't stop pretraining: Adapt language models to domains and tasks,,5,1243,ag's news
IMDB Reviews,ACL,10.18653/v1/2020.acl-main.740,N/A,https://aclanthology.org/P11-1015,Don't stop pretraining: Adapt language models to domains and tasks,,5,1243,imdb reviews
S2ORC,ACL,10.18653/v1/2020.acl-main.740,10.48550/arXiv.1911.02782,https://arxiv.org/pdf/1911.02782,Don't stop pretraining: Adapt language models to domains and tasks,,5,1243,s2orc
RealNews,ACL,10.18653/v1/2020.acl-main.740,10.48550/arXiv.1905.12616,https://arxiv.org/pdf/1905.12616,Don't stop pretraining: Adapt language models to domains and tasks,,5,1243,realnews
ChemProt,ACL,10.18653/v1/2020.acl-main.740,10.1093/database/bav123,https://pmc.ncbi.nlm.nih.gov/articles/PMC4752971/,Don't stop pretraining: Adapt language models to domains and tasks,,5,1243,chemprot
RCT,ACL,10.18653/v1/2020.acl-main.740,10.48550/arXiv.1710.06071,https://aclanthology.org/I17-2052.pdf,Don't stop pretraining: Adapt language models to domains and tasks,,5,1243,rct
ACL-ARC,ACL,10.18653/v1/2020.acl-main.740,10.1162/tacl_a_00028,https://aclanthology.org/Q18-1028.pdf,Don't stop pretraining: Adapt language models to domains and tasks,,5,1243,acl-arc
SciERC,ACL,10.18653/v1/2020.acl-main.740,10.18653/v1/D18-1360,https://aclanthology.org/D18-1360.pdf,Don't stop pretraining: Adapt language models to domains and tasks,,5,1243,scierc
HyperPartisan,ACL,10.18653/v1/2020.acl-main.740,10.18653/v1/S19-2145,https://aclanthology.org/S19-2145.pdf,Don't stop pretraining: Adapt language models to domains and tasks,,5,1243,hyperpartisan
Helpfulness,ACL,10.18653/v1/2020.acl-main.740,10.48550/arXiv.1506.04757,https://arxiv.org/pdf/1506.04757,Don't stop pretraining: Adapt language models to domains and tasks,,5,1243,helpfulness
web crawling,ACL,10.18653/v1/2020.acl-main.747,Not Found,Not Found,Unsupervised cross-lingual representation learning at scale,web crawling according to a procedure in a specified paper,15,3284,web crawling
XNLI,ACL,10.18653/v1/2020.acl-main.747,10.18653/v1/D18-1269,https://aclanthology.org/D18-1269.pdf,Unsupervised cross-lingual representation learning at scale,,15,3284,xnli
CoNLL-2002,ACL,10.18653/v1/2020.acl-main.747,N/A,https://aclanthology.org/W02-2024.pdf,Unsupervised cross-lingual representation learning at scale,,15,3284,conll-2002
CoNLL-2003,ACL,10.18653/v1/2020.acl-main.747,N/A,https://aclanthology.org/W03-0419/,Unsupervised cross-lingual representation learning at scale,,15,3284,conll-2003
MLQA,ACL,10.18653/v1/2020.acl-main.747,10.18653/v1/P19-1484,https://aclanthology.org/P19-1484.pdf,Unsupervised cross-lingual representation learning at scale,,15,3284,mlqa
GLUE,ACL,10.18653/v1/2020.acl-main.747,10.18653/v1/W18-5446,https://aclanthology.org/W18-5446.pdf,Unsupervised cross-lingual representation learning at scale,,15,3284,glue
N/A,ACL,10.18653/v1/2020.emnlp-demos.6,10.48550/arXiv.2009.01325,https://arxiv.org/pdf/2009.01325,Transformers: State-of-the-Art Natural Language Processing,Toolkit paper,15,7782,n/a
LAMA,ACL,10.18653/v1/2020.emnlp-main.346,10.18653/v1/D19-1250,https://aclanthology.org/D19-1250.pdf,AUTOPROMPT: Eliciting knowledge from language models with automatically generated prompts,,5,859,lama
SST-2,ACL,10.18653/v1/2020.emnlp-main.346,N/A,https://aclanthology.org/D13-1170.pdf,AUTOPROMPT: Eliciting knowledge from language models with automatically generated prompts,,5,859,sst-2
GLUE,ACL,10.18653/v1/2020.emnlp-main.346,10.18653/v1/W18-5446,https://aclanthology.org/W18-5446.pdf,AUTOPROMPT: Eliciting knowledge from language models with automatically generated prompts,,5,859,glue
SICK,ACL,10.18653/v1/2020.emnlp-main.346,N/A,http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf,AUTOPROMPT: Eliciting knowledge from language models with automatically generated prompts,,5,859,sick
LPAQA,ACL,10.18653/v1/2020.emnlp-main.346,10.1162/tacl_a_00324,https://aclanthology.org/2020.tacl-1.28.pdf,AUTOPROMPT: Eliciting knowledge from language models with automatically generated prompts,,5,859,lpaqa
T-REx,ACL,10.18653/v1/2020.emnlp-main.346,N/A,https://aclanthology.org/L18-1544.pdf,AUTOPROMPT: Eliciting knowledge from language models with automatically generated prompts,,5,859,t-rex
English Wikipedia dump 18 dec 2018,ACL,10.18653/v1/2020.emnlp-main.550,Not Found,Not Found,Dense passage retrieval for open-domain question answering,i didn't manage to find it,5,1766,english wikipedia dump 18 dec 2018
CuratedTREC,ACL,10.18653/v1/2020.emnlp-main.550,10.1007/978-3-319-24027-5_20,http://ailao.eu/yodaqa/yodaqa-clef2015.pdf,Dense passage retrieval for open-domain question answering,,5,1766,curatedtrec
NQ,ACL,10.18653/v1/2020.emnlp-main.550,10.1162/tacl_a_00276,https://aclanthology.org/Q19-1026.pdf,Dense passage retrieval for open-domain question answering,,5,1766,nq
TriviaQA,ACL,10.18653/v1/2020.emnlp-main.550,10.18653/v1/P17-1147,https://aclanthology.org/P17-1147.pdf,Dense passage retrieval for open-domain question answering,,5,1766,triviaqa
WQ,ACL,10.18653/v1/2020.emnlp-main.550,N/A,https://aclanthology.org/D13-1160.pdf,Dense passage retrieval for open-domain question answering,,5,1766,wq
SQuAD,ACL,10.18653/v1/2020.emnlp-main.550,10.18653/v1/D16-1264,https://arxiv.org/pdf/1606.05250,Dense passage retrieval for open-domain question answering,,5,1766,squad
CodeSearchNet,ACL,10.18653/v1/2020.findings-emnlp.139,10.48550/arXiv.1909.09436,https://arxiv.org/pdf/1909.09436,CodeBERT: A pre-trained model for programming and natural languages,,5,1023,codesearchnet
SNLI,ACL,10.18653/v1/2021.acl-long.295,10.18653/v1/D15-1075,https://aclanthology.org/D15-1075.pdf,Making pre-trained language models better few-shot learners,,5,1012,snli
GLUE,ACL,10.18653/v1/2021.acl-long.295,10.18653/v1/W18-5446,https://aclanthology.org/W18-5446.pdf,Making pre-trained language models better few-shot learners,the datasets within glue are mentioned but I will leave them out,5,1012,glue
MR,ACL,10.18653/v1/2021.acl-long.295,10.3115/1219840.1219855,https://dl.acm.org/doi/pdf/10.3115/1219840.1219855,Making pre-trained language models better few-shot learners,,5,1012,mr
CR,ACL,10.18653/v1/2021.acl-long.295,10.1145/1014052.1014073,https://www.cs.uic.edu/~liub/publications/kdd04-revSummary.pdf,Making pre-trained language models better few-shot learners,,5,1012,cr
MPQA,ACL,10.18653/v1/2021.acl-long.295,10.1007/s10579-005-7880-9,https://www.cs.cornell.edu/home/cardie/papers/lre05withappendix.pdf,Making pre-trained language models better few-shot learners,,5,1012,mpqa
Subj,ACL,10.18653/v1/2021.acl-long.295,10.3115/1218955.1218990,https://aclanthology.org/P04-1035.pdf,Making pre-trained language models better few-shot learners,,5,1012,subj
TREC,ACL,10.18653/v1/2021.acl-long.295,N/A,https://aclanthology.org/C02-1150.pdf,Making pre-trained language models better few-shot learners,,5,1012,trec
SST-5,ACL,10.18653/v1/2021.acl-long.295,N/A,https://aclanthology.org/D13-1170.pdf,Making pre-trained language models better few-shot learners,,5,1012,sst-5
E2E,ACL,10.18653/v1/2021.acl-long.353,10.48550/arXiv.1706.09254,https://arxiv.org/pdf/1706.09254,Prefix-tuning: Optimizing continuous prompts for generation,,5,1796,e2e
WebNLG 2017,ACL,10.18653/v1/2021.acl-long.353,10.18653/v1/W17-3518,https://aclanthology.org/W17-3518.pdf,Prefix-tuning: Optimizing continuous prompts for generation,,5,1796,webnlg 2017
DART,ACL,10.18653/v1/2021.acl-long.353,10.48550/arXiv.2007.02871,https://arxiv.org/pdf/2007.02871,Prefix-tuning: Optimizing continuous prompts for generation,,5,1796,dart
XSUM,ACL,10.18653/v1/2021.acl-long.353,10.18653/v1/D18-1206,https://aclanthology.org/D18-1206.pdf,Prefix-tuning: Optimizing continuous prompts for generation,,5,1796,xsum
DART,ACL,10.18653/v1/2021.acl-long.353,10.48550/arXiv.2007.02871,https://arxiv.org/pdf/2007.02871,Prefix-tuning: Optimizing continuous prompts for generation,,5,1796,dart
Yelp-5,ACL,10.18653/v1/2021.eacl-main.20,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Exploiting cloze questions for few shot text classification and natural language inference,,5,917,yelp-5
AG's News,ACL,10.18653/v1/2021.eacl-main.20,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Exploiting cloze questions for few shot text classification and natural language inference,,5,917,ag's news
Yahoo answers,ACL,10.18653/v1/2021.eacl-main.20,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Exploiting cloze questions for few shot text classification and natural language inference,,5,917,yahoo answers
Multi-NLI,ACL,10.18653/v1/2021.eacl-main.20,10.18653/v1/N18-1101,https://aclanthology.org/N18-1101.pdf,Exploiting cloze questions for few shot text classification and natural language inference,,5,917,multi-nli
SuperGLUE,ACL,10.18653/v1/2021.emnlp-main.243,10.48550/arXiv.1905.00537,https://papers.neurips.cc/paper_files/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf,The Power of Scale for Parameter-Efficient Prompt Tuning,,5,1561,superglue
SQuAD,ACL,10.18653/v1/2021.emnlp-main.243,10.18653/v1/D16-1264,https://arxiv.org/pdf/1606.05250,The Power of Scale for Parameter-Efficient Prompt Tuning,,5,1561,squad
TextbookQA,ACL,10.18653/v1/2021.emnlp-main.243,10.1109/CVPR.2017.571,https://openaccess.thecvf.com/content_cvpr_2017/papers/Kembhavi_Are_You_Smarter_CVPR_2017_paper.pdf,The Power of Scale for Parameter-Efficient Prompt Tuning,,5,1561,textbookqa
RACE,ACL,10.18653/v1/2021.emnlp-main.243,10.18653/v1/D17-1082,https://aclanthology.org/D17-1082.pdf,The Power of Scale for Parameter-Efficient Prompt Tuning,,5,1561,race
BioASQ,ACL,10.18653/v1/2021.emnlp-main.243,10.1186/s12859-015-0564-6,https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0564-6,The Power of Scale for Parameter-Efficient Prompt Tuning,,5,1561,bioasq
RE,ACL,10.18653/v1/2021.emnlp-main.243,10.18653/v1/K17-1034,https://aclanthology.org/K17-1034.pdf,The Power of Scale for Parameter-Efficient Prompt Tuning,,5,1561,re
DuoRC,ACL,10.18653/v1/2021.emnlp-main.243,10.18653/v1/P18-1156,https://aclanthology.org/P18-1156.pdf,The Power of Scale for Parameter-Efficient Prompt Tuning,,5,1561,duorc
DROP,ACL,10.18653/v1/2021.emnlp-main.243,10.18653/v1/N19-1246,https://aclanthology.org/N19-1246.pdf,The Power of Scale for Parameter-Efficient Prompt Tuning,,5,1561,drop
QQP,ACL,10.18653/v1/2021.emnlp-main.552,N/A,https://www.quora.com/q/quoradata/,SimCSE: Simple Contrastive Learning of Sentence Embeddings,,5,1801,qqp
Flickr30k,ACL,10.18653/v1/2021.emnlp-main.552,10.1162/tacl_a_00166,https://aclanthology.org/Q14-1006.pdf,SimCSE: Simple Contrastive Learning of Sentence Embeddings,,5,1801,flickr30k
ParaNMT,ACL,10.18653/v1/2021.emnlp-main.552,10.18653/v1/P18-1042,https://aclanthology.org/P18-1042.pdf,SimCSE: Simple Contrastive Learning of Sentence Embeddings,,5,1801,paranmt
Multi-NLI,ACL,10.18653/v1/2021.emnlp-main.552,10.18653/v1/N18-1101,https://aclanthology.org/N18-1101.pdf,SimCSE: Simple Contrastive Learning of Sentence Embeddings,,5,1801,multi-nli
SNLI,ACL,10.18653/v1/2021.emnlp-main.552,10.18653/v1/D15-1075,https://aclanthology.org/D15-1075.pdf,SimCSE: Simple Contrastive Learning of Sentence Embeddings,,5,1801,snli
ANLI,ACL,10.18653/v1/2021.emnlp-main.552,10.18653/v1/2020.acl-main.441,https://aclanthology.org/2020.acl-main.441.pdf,SimCSE: Simple Contrastive Learning of Sentence Embeddings,,5,1801,anli
STSb,ACL,10.18653/v1/2021.emnlp-main.552,10.48550/arXiv.1708.00055,https://arxiv.org/pdf/1708.00055,SimCSE: Simple Contrastive Learning of Sentence Embeddings,,5,1801,stsb
SICK,ACL,10.18653/v1/2021.emnlp-main.552,N/A,http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf,SimCSE: Simple Contrastive Learning of Sentence Embeddings,,5,1801,sick
STS2012,ACL,10.18653/v1/2021.emnlp-main.552,N/A,https://aclanthology.org/S12-1051.pdf,SimCSE: Simple Contrastive Learning of Sentence Embeddings,,5,1801,sts2012
STS2013,ACL,10.18653/v1/2021.emnlp-main.552,N/A,https://aclanthology.org/S13-1004.pdf,SimCSE: Simple Contrastive Learning of Sentence Embeddings,,5,1801,sts2013
STS2014,ACL,10.18653/v1/2021.emnlp-main.552,10.3115/v1/S14-2010,https://aclanthology.org/S14-2010.pdf,SimCSE: Simple Contrastive Learning of Sentence Embeddings,,5,1801,sts2014
STS2015,ACL,10.18653/v1/2021.emnlp-main.552,10.18653/v1/S15-2045,https://aclanthology.org/S15-2045.pdf,SimCSE: Simple Contrastive Learning of Sentence Embeddings,,5,1801,sts2015
STS2016,ACL,10.18653/v1/2021.emnlp-main.552,10.18653/v1/S16-1081,https://aclanthology.org/S16-1081.pdf,SimCSE: Simple Contrastive Learning of Sentence Embeddings,,5,1801,sts2016
mC4,ACL,10.18653/v1/2021.naacl-main.41,N/A,https://aclanthology.org/2021.naacl-main.41.pdf,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,,5,1264,mc4
C4,ACL,10.18653/v1/2021.naacl-main.41,10.48550/arXiv.1910.10683,https://arxiv.org/pdf/1910.10683,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,,5,1264,c4
XTREME,ACL,10.18653/v1/2021.naacl-main.41,10.48550/arXiv.2003.11080,https://arxiv.org/pdf/2003.11080,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,,5,1264,xtreme
XNLI,ACL,10.18653/v1/2021.naacl-main.41,10.18653/v1/D18-1269,https://aclanthology.org/D18-1269.pdf,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,,5,1264,xnli
MLQA,ACL,10.18653/v1/2021.naacl-main.41,10.18653/v1/P19-1484,https://aclanthology.org/P19-1484.pdf,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,,5,1264,mlqa
XQuAD,ACL,10.18653/v1/2021.naacl-main.41,10.18653/v1/2020.acl-main.421,https://aclanthology.org/2020.acl-main.421.pdf,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,,5,1264,xquad
TyDi QA,ACL,10.18653/v1/2021.naacl-main.41,10.1162/tacl_a_00317,https://aclanthology.org/2020.tacl-1.30.pdf,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,,5,1264,tydi qa
NER,ACL,10.18653/v1/2021.naacl-main.41,10.18653/v1/P17-1178,https://aclanthology.org/P17-1178.pdf,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,,5,1264,ner
PAWS-X,ACL,10.18653/v1/2021.naacl-main.41,10.18653/v1/D19-1382,https://aclanthology.org/D19-1382.pdf,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,,5,1264,paws-x
SQuAD,ACL,10.18653/v1/2021.naacl-main.41,10.18653/v1/D16-1264,https://arxiv.org/pdf/1606.05250,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,,5,1264,squad
AQuA,ACL,10.18653/v1/2023.acl-long.147,10.18653/v1/P17-1015,https://aclanthology.org/P17-1015.pdf,Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,,2,102,aqua
GSM8K,ACL,10.18653/v1/2023.acl-long.147,10.48550/arXiv.2110.14168,https://arxiv.org/pdf/2110.14168v2,Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,,2,102,gsm8k
MultiArith,ACL,10.18653/v1/2023.acl-long.147,10.18653/v1/D15-1202,https://aclanthology.org/D15-1202.pdf,Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,,2,102,multiarith
AddSub,ACL,10.18653/v1/2023.acl-long.147,N/A,https://github.com/AGI-Edgerunners/LLM-Adapters/tree/main/dataset/AddSub,Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,,2,102,addsub
SingleEq,ACL,10.18653/v1/2023.acl-long.147,10.1162/tacl_a_00160,https://aclanthology.org/Q15-1042.pdf,Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,,2,102,singleeq
SVAMP,ACL,10.18653/v1/2023.acl-long.147,10.18653/v1/2021.naacl-main.168,https://aclanthology.org/2021.naacl-main.168.pdf,Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,,2,102,svamp
CommonsenseQA,ACL,10.18653/v1/2023.acl-long.147,10.18653/v1/N19-1421,https://aclanthology.org/N19-1421.pdf,Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,,2,102,commonsenseqa
StrategyQA,ACL,10.18653/v1/2023.acl-long.147,10.1162/tacl_a_00370,https://aclanthology.org/2021.tacl-1.21.pdf,Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,,2,102,strategyqa
Last Letter,ACL,10.18653/v1/2023.acl-long.147,10.48550/arXiv.2201.11903,https://arxiv.org/pdf/2201.11903,Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,,2,102,last letter
Coin Flip,ACL,10.18653/v1/2023.acl-long.147,10.48550/arXiv.2201.11903,https://arxiv.org/pdf/2201.11903,Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,,2,102,coin flip
CLUTRR,ACL,10.18653/v1/2023.acl-long.291,10.18653/v1/D19-1458,https://aclanthology.org/D19-1458.pdf,Making Large Language Models Better Reasoners with Step-Aware Verifier,,2,95,clutrr
AsDiv,ACL,10.18653/v1/2023.acl-long.291,10.18653/v1/2020.acl-main.92,https://aclanthology.org/2020.acl-main.92.pdf,Making Large Language Models Better Reasoners with Step-Aware Verifier,,2,95,asdiv
SVAMP,ACL,10.18653/v1/2023.acl-long.291,10.18653/v1/2021.naacl-main.168,https://aclanthology.org/2021.naacl-main.168.pdf,Making Large Language Models Better Reasoners with Step-Aware Verifier,,2,95,svamp
SingleEq,ACL,10.18653/v1/2023.acl-long.291,10.1162/tacl_a_00160,https://aclanthology.org/Q15-1042.pdf,Making Large Language Models Better Reasoners with Step-Aware Verifier,,2,95,singleeq
MultiArith,ACL,10.18653/v1/2023.acl-long.291,10.18653/v1/D15-1202,https://aclanthology.org/D15-1202.pdf,Making Large Language Models Better Reasoners with Step-Aware Verifier,,2,95,multiarith
CommonsenseQA,ACL,10.18653/v1/2023.acl-long.291,10.18653/v1/N19-1421,https://aclanthology.org/N19-1421.pdf,Making Large Language Models Better Reasoners with Step-Aware Verifier,,2,95,commonsenseqa
StrategyQA,ACL,10.18653/v1/2023.acl-long.291,10.1162/tacl_a_00370,https://aclanthology.org/2021.tacl-1.21.pdf,Making Large Language Models Better Reasoners with Step-Aware Verifier,,2,95,strategyqa
GSM8K,ACL,10.18653/v1/2023.acl-long.291,10.48550/arXiv.2110.14168,https://arxiv.org/pdf/2110.14168v2,Making Large Language Models Better Reasoners with Step-Aware Verifier,,2,95,gsm8k
PopQA,ACL,10.18653/v1/2023.acl-long.546,10.18653/v1/2023.acl-long.546,https://aclanthology.org/2023.acl-long.546.pdf,When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories,,2,197,popqa
EntityQuestions,ACL,10.18653/v1/2023.acl-long.546,10.18653/v1/2021.emnlp-main.496,https://aclanthology.org/2021.emnlp-main.496.pdf,When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories,,2,197,entityquestions
Self-Instruct,ACL,10.18653/v1/2023.acl-long.754,10.18653/v1/2023.acl-long.754,https://arxiv.org/pdf/2212.10560,SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions,not sure which data,2,392,self-instruct
SuperNI,ACL,10.18653/v1/2023.acl-long.754,10.18653/v1/2022.emnlp-main.340,https://aclanthology.org/2022.emnlp-main.340.pdf,SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions,,2,392,superni
ADE,ACL,10.18653/v1/2023.acl-long.868,10.1016/j.jbi.2012.04.008,https://pubmed-ncbi-nlm-nih-gov.tudelft.idm.oclc.org/22554702/,Revisiting Relation Extraction in the era of Large Language Models,,2,126,ade
CoNLL-2004,ACL,10.18653/v1/2023.acl-long.868,N/A,https://aclanthology.org/W04-2412.pdf,Revisiting Relation Extraction in the era of Large Language Models,,2,126,conll-2004
NYT,ACL,10.18653/v1/2023.acl-long.868,10.1007/978-3-642-15939-8_10,https://www.researchgate.net/publication/220698997_Modeling_Relations_and_Their_Mentions_without_Labeled_Text,Revisiting Relation Extraction in the era of Large Language Models,,2,126,nyt
DocRED,ACL,10.18653/v1/2023.acl-long.868,10.18653/v1/P19-1074,https://aclanthology.org/P19-1074.pdf,Revisiting Relation Extraction in the era of Large Language Models,,2,126,docred
WritingPrompts,ACL,10.18653/v1/2023.acl-long.870,10.18653/v1/P18-1082,https://aclanthology.org/P18-1082.pdf,Can Large Language Models Be an Alternative to Human Evaluation?,,2,176,writingprompts
NQ,ACL,10.18653/v1/2023.acl-long.910,10.1162/tacl_a_00276,https://aclanthology.org/Q19-1026.pdf,"RARR: Researching and Revising What Language Models Say, Using Language Models",,2,107,nq
StrategyQA,ACL,10.18653/v1/2023.acl-long.910,10.1162/tacl_a_00370,https://aclanthology.org/2021.tacl-1.21.pdf,"RARR: Researching and Revising What Language Models Say, Using Language Models",,2,107,strategyqa
QReCC,ACL,10.18653/v1/2023.acl-long.910,10.18653/v1/2021.naacl-main.44,https://aclanthology.org/2021.naacl-main.44.pdf,"RARR: Researching and Revising What Language Models Say, Using Language Models",,2,107,qrecc
MTEB,ACL,10.18653/v1/2023.eacl-main.148,10.18653/v1/2023.eacl-main.148,https://aclanthology.org/2023.eacl-main.148.pdf,MTEB: Massive Text Embedding Benchmark,,2,140,mteb
News Summarization,ACL,10.18653/v1/2023.emnlp-main.153,10.48550/arXiv.2301.13848,https://arxiv.org/pdf/2301.13848,G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment,,2,233,news summarization
SummEval,ACL,10.18653/v1/2023.emnlp-main.153,10.1162/tacl_a_00373,https://aclanthology.org/2021.tacl-1.24.pdf,G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment,,2,233,summeval
Topical-Chat,ACL,10.18653/v1/2023.emnlp-main.153,10.48550/arXiv.2308.11995,https://arxiv.org/pdf/2308.11995,G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment,,2,233,topical-chat
AG's News,ACL,10.18653/v1/2023.emnlp-main.153,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment,,2,233,ag's news
QAGS,ACL,10.18653/v1/2023.emnlp-main.153,10.18653/v1/2020.acl-main.450,https://aclanthology.org/2020.acl-main.450.pdf,G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment,,2,233,qags
COCO,ACL,10.18653/v1/2023.emnlp-main.20,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Evaluating Object Hallucination in Large Vision-Language Models,,2,113,coco
GQA,ACL,10.18653/v1/2023.emnlp-main.20,10.1109/CVPR.2019.00686,https://arxiv.org/pdf/1902.09506,Evaluating Object Hallucination in Large Vision-Language Models,,2,113,gqa
A-OKVQA,ACL,10.18653/v1/2023.emnlp-main.20,10.1007/978-3-031-20074-8_9,https://link.springer.com/chapter/10.1007/978-3-031-20074-8_9,Evaluating Object Hallucination in Large Vision-Language Models,,2,113,a-okvqa
HaluEval,ACL,10.18653/v1/2023.emnlp-main.397,Not Found,Not Found,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,,2,153,halueval
Alpaca,ACL,10.18653/v1/2023.emnlp-main.397,N/A,https://crfm.stanford.edu/2023/03/13/alpaca.html,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,,2,153,alpaca
HotpotQA,ACL,10.18653/v1/2023.emnlp-main.397,10.18653/v1/D18-1259,https://aclanthology.org/D18-1259.pdf,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,,2,153,hotpotqa
CNN/DM,ACL,10.18653/v1/2023.emnlp-main.397,10.48550/arXiv.1506.03340,https://arxiv.org/pdf/1506.03340,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,,2,153,cnn/dm
OpenDialKG,ACL,10.18653/v1/2023.emnlp-main.397,10.18653/v1/P19-1081,https://aclanthology.org/P19-1081.pdf,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,,2,153,opendialkg
2WikiMultihopQA,ACL,10.18653/v1/2023.emnlp-main.495,10.18653/v1/2020.coling-main.580,https://aclanthology.org/2020.coling-main.580.pdf,Active Retrieval Augmented Generation,,2,99,2wikimultihopqa
StrategyQA,ACL,10.18653/v1/2023.emnlp-main.495,10.1162/tacl_a_00370,https://aclanthology.org/2021.tacl-1.21.pdf,Active Retrieval Augmented Generation,,2,99,strategyqa
ASQA,ACL,10.18653/v1/2023.emnlp-main.495,10.18653/v1/2022.emnlp-main.566,https://aclanthology.org/2022.emnlp-main.566.pdf,Active Retrieval Augmented Generation,,2,99,asqa
WikiAsp,ACL,10.18653/v1/2023.emnlp-main.495,10.1162/tacl_a_00362,https://aclanthology.org/2021.tacl-1.13.pdf,Active Retrieval Augmented Generation,,2,99,wikiasp
WikiBio,ACL,10.18653/v1/2023.emnlp-main.557,10.48550/arXiv.1603.07771,https://arxiv.org/pdf/1603.07771,SELFCHECKGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models,,2,109,wikibio
SQuAD,ACL,10.18653/v1/2023.emnlp-main.557,10.18653/v1/D16-1264,https://arxiv.org/pdf/1606.05250,SELFCHECKGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models,,2,109,squad
SQuADv2,ACL,10.18653/v1/2023.emnlp-main.557,10.48550/arXiv.1806.03822,https://arxiv.org/pdf/1806.03822,SELFCHECKGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models,,2,109,squadv2
RACE,ACL,10.18653/v1/2023.emnlp-main.557,10.18653/v1/D17-1082,https://aclanthology.org/D17-1082.pdf,SELFCHECKGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models,,2,109,race
Celebrity Bio Fact Precision,ACL,10.18653/v1/2023.emnlp-main.741,10.18653/v1/2023.emnlp-main.741,https://aclanthology.org/2023.emnlp-main.741.pdf,FACTSCORE: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation,,2,151,celebrity bio fact precision
CommonsenseQA,ACL,10.18653/v1/2023.findings-acl.507,10.18653/v1/N19-1421,https://aclanthology.org/N19-1421.pdf,Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes,,2,100,commonsenseqa
SVAMP,ACL,10.18653/v1/2023.findings-acl.507,10.18653/v1/2021.naacl-main.168,https://aclanthology.org/2021.naacl-main.168.pdf,Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes,,2,100,svamp
ANLI,ACL,10.18653/v1/2023.findings-acl.507,10.18653/v1/2020.acl-main.441,https://aclanthology.org/2020.acl-main.441.pdf,Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes,,2,100,anli
e-SNLI,ACL,10.18653/v1/2023.findings-acl.507,10.48550/arXiv.1812.01193,https://arxiv.org/pdf/1812.01193,Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes,,2,100,e-snli
ASDiv,ACL,10.18653/v1/2023.findings-acl.507,10.18653/v1/2020.acl-main.92,https://aclanthology.org/2020.acl-main.92.pdf,Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes,,2,100,asdiv
BigBench,ACL,10.18653/v1/2023.findings-acl.824,10.48550/arXiv.2206.04615,https://arxiv.org/pdf/2206.04615v3,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,,2,104,bigbench
Musique,ACL,10.18653/v1/2023.findings-acl.824,10.1162/tacl_a_00475,https://aclanthology.org/2022.tacl-1.31.pdf,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,,2,104,musique
PrOntoQA,ACL,10.18653/v1/2023.findings-emnlp.248,10.48550/arXiv.2210.01240,https://arxiv.org/pdf/2210.01240,LOGIC-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning,,2,95,prontoqa
ProofWriter,ACL,10.18653/v1/2023.findings-emnlp.248,10.18653/v1/2021.findings-acl.317,https://aclanthology.org/2021.findings-acl.317.pdf,LOGIC-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning,,2,95,proofwriter
FOLIO,ACL,10.18653/v1/2023.findings-emnlp.248,10.48550/arXiv.2209.00840,https://arxiv.org/pdf/2209.00840,LOGIC-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning,,2,95,folio
LogicalDeduction,ACL,10.18653/v1/2023.findings-emnlp.248,10.48550/arXiv.2206.04615,https://arxiv.org/pdf/2206.04615,LOGIC-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning,,2,95,logicaldeduction
AR-LSAT,ACL,10.18653/v1/2023.findings-emnlp.248,10.18653/v1/2022.findings-naacl.177,https://aclanthology.org/2022.findings-naacl.177.pdf,LOGIC-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning,,2,95,ar-lsat
2WikiMultihopQA,ACL,10.18653/v1/2023.findings-emnlp.378,10.18653/v1/2020.coling-main.580,https://aclanthology.org/2020.coling-main.580.pdf,Measuring and Narrowing the Compositionality Gap in Language Models,,2,96,2wikimultihopqa
Bamboogle,ACL,10.18653/v1/2023.findings-emnlp.378,10.18653/v1/2023.findings-emnlp.378,https://aclanthology.org/2023.findings-emnlp.378.pdf,Measuring and Narrowing the Compositionality Gap in Language Models,,2,96,bamboogle
CompositionalCelebrities,ACL,10.18653/v1/2023.findings-emnlp.378,10.18653/v1/2023.findings-emnlp.378,https://aclanthology.org/2023.findings-emnlp.378.pdf,Measuring and Narrowing the Compositionality Gap in Language Models,,2,96,compositionalcelebrities
SemEval2023-10,ACL,10.18653/v1/2023.semeval-1.305,10.18653/v1/2023.semeval-1.305,https://aclanthology.org/2023.semeval-1.305.pdf,SemEval-2023 Task 10: Explainable Detection of Online Sexism,,2,119,semeval2023-10
SemEval2023-3,ACL,10.18653/v1/2023.semeval-1.317,10.18653/v1/2023.semeval-1.317,https://aclanthology.org/2023.semeval-1.317v2.pdf,"SemEval-2023 Task 3: Detecting the Category, the Framing, and the Persuasion Techniques in Online News in a Multi-lingual Setup",,2,103,semeval2023-3
SICK,ACL,10.18653/v1/d15-1075,N/A,http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf,A large annotated corpus for learning natural language inference,"dataset paper, other datasets used for validation",15,2562,sick
RTE-3,ACL,10.18653/v1/d15-1075,N/A,https://aclanthology.org/W07-1401.pdf,A large annotated corpus for learning natural language inference,,15,2562,rte-3
SNLI,ACL,10.18653/v1/d15-1075,10.18653/v1/D15-1075,https://aclanthology.org/D15-1075.pdf,A large annotated corpus for learning natural language inference,,,,
WMT14,ACL,10.18653/v1/d15-1166,10.3115/v1/W14-3302,https://aclanthology.org/W14-3302.pdf,Effective approaches to attention-based neural machine translation,in the ACL website there are 2 other datasets mentioned that are nowhere to be found in the paper: LSMDC-E and VIST-E,15,4150,wmt14
WMT15,ACL,10.18653/v1/d15-1166,10.18653/v1/W15-3001,https://aclanthology.org/W15-3001.pdf,Effective approaches to attention-based neural machine translation,WMT not referenced once again,15,4150,wmt15
SemEval2014-4,ACL,10.18653/v1/d16-1058,10.3115/v1/S14-2004,https://aclanthology.org/S14-2004.pdf,Attention-based LSTM for aspect-level sentiment classification,,15,2171,semeval2014-4
SQuAD,ACL,10.18653/v1/d16-1264,10.18653/v1/D16-1264,https://arxiv.org/pdf/1606.05250,"SQuad: 100,000+ questions for machine comprehension of text",Dataset paper,15,4049,squad
KFTT,ACL,10.18653/v1/d18-2012,?,https://www.phontron.com/kftt/,SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,interesting reference about preprocessing; training procedure abstracted behind reference,15,2117,kftt
YFCC100M,ACL,10.18653/v1/e17-2068,10.48550/arXiv.1503.01817,https://arxiv.org/pdf/1503.01817,Bag of tricks for efficient text classification,"There are 8 more datasets abstracted away behind a reference, some are mentioned in the ACL website",15,2150,yfcc100m
AG's News,ACL,10.18653/v1/e17-2068,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Bag of tricks for efficient text classification,,15,2150,ag's news
Yelp-2,ACL,10.18653/v1/e17-2068,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Bag of tricks for efficient text classification,,15,2150,yelp-2
Yelp-5,ACL,10.18653/v1/e17-2068,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Bag of tricks for efficient text classification,,15,2150,yelp-5
Amazon-2,ACL,10.18653/v1/e17-2068,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Bag of tricks for efficient text classification,,15,2150,amazon-2
Amazon-5,ACL,10.18653/v1/e17-2068,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Bag of tricks for efficient text classification,,15,2150,amazon-5
DBPedia,ACL,10.18653/v1/e17-2068,N/A,https://pubs.dbs.uni-leipzig.de/mashup/files/10.1.1.69.5249.pdf,Bag of tricks for efficient text classification,,15,2150,dbpedia
Yahoo Answers,ACL,10.18653/v1/e17-2068,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Bag of tricks for efficient text classification,,15,2150,yahoo answers
Sogou,ACL,10.18653/v1/e17-2068,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Bag of tricks for efficient text classification,,15,2150,sogou
CoNLL-2002,ACL,10.18653/v1/n16-1030,N/A,https://aclanthology.org/W02-2024.pdf,Neural architectures for named entity recognition,,15,2608,conll-2002
CoNLL-2003,ACL,10.18653/v1/n16-1030,N/A,https://aclanthology.org/W03-0419/,Neural architectures for named entity recognition,,15,2608,conll-2003
Yelp Reviews Tang,ACL,10.18653/v1/n16-1174,10.18653/v1/D15-1167,https://aclanthology.org/D15-1167.pdf,Hierarchical attention networks for document classification,,15,4279,yelp reviews tang
IMDB Reviews DIao,ACL,10.18653/v1/n16-1174,10.1145/2623330.2623758,https://www.ml.cmu.edu/research/dap-papers/dap-wu_chaoyuan.pdf,Hierarchical attention networks for document classification,,15,4279,imdb reviews diao
Yahoo answers,ACL,10.18653/v1/n16-1174,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Hierarchical attention networks for document classification,,15,4279,yahoo answers
Amazon-5,ACL,10.18653/v1/n16-1174,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Hierarchical attention networks for document classification,,15,4279,amazon-5
SNLI,ACL,10.18653/v1/N18-1101,10.18653/v1/D15-1075,https://aclanthology.org/D15-1075.pdf,A broad-coverage challenge corpus for sentence understanding through inference,Dataset paper; other dataset used for validation,15,2625,snli
1B Word,ACL,10.18653/v1/N18-1202,10.48550/arXiv.1312.3005,https://arxiv.org/pdf/1312.3005,Deep contextualized word representations,,15,6895,1b word
SQuAD,ACL,10.18653/v1/N18-1202,10.18653/v1/D16-1264,https://arxiv.org/pdf/1606.05250,Deep contextualized word representations,,15,6895,squad
SNLI,ACL,10.18653/v1/N18-1202,10.18653/v1/D15-1075,https://aclanthology.org/D15-1075.pdf,Deep contextualized word representations,,15,6895,snli
OntoNotes,ACL,10.18653/v1/N18-1202,?,https://aclanthology.org/W13-3516.pdf,Deep contextualized word representations,,15,6895,ontonotes
CoNLL-2012,ACL,10.18653/v1/N18-1202,N/A,https://aclanthology.org/W12-4501/,Deep contextualized word representations,,15,6895,conll-2012
CoNLL-2003,ACL,10.18653/v1/N18-1202,N/A,https://aclanthology.org/W03-0419/,Deep contextualized word representations,,15,6895,conll-2003
Semcor 3.0,ACL,10.18653/v1/N18-1202,N/A,https://web.eecs.umich.edu/~mihalcea/downloads.html#semcor,Deep contextualized word representations,The paper referenced something else from Miller et al 1994,15,6895,semcor 3.0
PTB,ACL,10.18653/v1/N18-1202,N/A,https://aclanthology.org/J93-2004/,Deep contextualized word representations,,15,6895,ptb
GLUE,ACL,10.18653/v1/N19-1423,10.18653/v1/W18-5446,https://aclanthology.org/W18-5446.pdf,BERT: Pre-training of deep bidirectional transformers for language understanding,,15,44198,glue
SQuAD,ACL,10.18653/v1/N19-1423,10.18653/v1/D16-1264,https://arxiv.org/pdf/1606.05250,BERT: Pre-training of deep bidirectional transformers for language understanding,,15,44198,squad
SQuADv2,ACL,10.18653/v1/N19-1423,10.48550/arXiv.1806.03822,https://arxiv.org/pdf/1806.03822,BERT: Pre-training of deep bidirectional transformers for language understanding,,15,44198,squadv2
SWAG,ACL,10.18653/v1/N19-1423,10.18653/v1/D18-1009,https://aclanthology.org/D18-1009.pdf,BERT: Pre-training of deep bidirectional transformers for language understanding,,15,44198,swag
CoNLL-2003,ACL,10.18653/v1/N19-1423,N/A,https://aclanthology.org/W03-0419/,BERT: Pre-training of deep bidirectional transformers for language understanding,,15,44198,conll-2003
BookCorpus,ACL,10.18653/v1/N19-1423,10.1109/ICCV.2015.11,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=7410368,BERT: Pre-training of deep bidirectional transformers for language understanding,,15,44198,bookcorpus
English Wikipedia,ACL,10.18653/v1/N19-1423,N/A,N/A,BERT: Pre-training of deep bidirectional transformers for language understanding,Literally extracted english wikipedia,15,44198,english wikipedia
WMT14,ACL,10.18653/v1/N19-4009,10.3115/v1/W14-3302,https://aclanthology.org/W14-3302.pdf,"Fairseq: A fast, extensible toolkit for sequence modeling","toolkit paper, but includes evaluations",15,2182,wmt14
WMT16,ACL,10.18653/v1/N19-4009,10.18653/v1/W16-2301,https://aclanthology.org/W16-2301.pdf,"Fairseq: A fast, extensible toolkit for sequence modeling",doesn't reference WMT!!!,15,2182,wmt16
WikiText-103,ACL,10.18653/v1/N19-4009,10.48550/arXiv.1609.07843,https://arxiv.org/pdf/1609.07843,"Fairseq: A fast, extensible toolkit for sequence modeling",not referenced specifically,15,2182,wikitext-103
1B Word,ACL,10.18653/v1/N19-4009,10.48550/arXiv.1312.3005,https://arxiv.org/pdf/1312.3005,"Fairseq: A fast, extensible toolkit for sequence modeling",not referenced specifically,15,2182,1b word
CNN/DM,ACL,10.18653/v1/N19-4009,10.48550/arXiv.1506.03340,https://arxiv.org/pdf/1506.03340,"Fairseq: A fast, extensible toolkit for sequence modeling",,15,2182,cnn/dm
WMT15,ACL,10.18653/v1/P16-1162,10.18653/v1/W15-3001,https://aclanthology.org/W15-3001.pdf,Neural machine translation of rare words with subword units,doesn't reference this!!!,15,4517,wmt15
METEOR,ACL,10.18653/v1/P17-1099,10.3115/v1/W14-3348,https://aclanthology.org/W14-3348.pdf,Get to the point: Summarization with pointer-generator networks,,15,2646,meteor
CNN/DM,ACL,10.18653/v1/P17-1099,10.48550/arXiv.1506.03340,https://arxiv.org/pdf/1506.03340,Get to the point: Summarization with pointer-generator networks,,15,2646,cnn/dm
English Wikipedia,ACL,10.18653/v1/P19-1139,N/A,N/A,ErniE: Enhanced language representation with informative entities,Criminally underspecified,5,866,english wikipedia
GLUE,ACL,10.18653/v1/P19-1139,10.18653/v1/W18-5446,https://aclanthology.org/W18-5446.pdf,ErniE: Enhanced language representation with informative entities,,5,866,glue
TACRED,ACL,10.18653/v1/P19-1139,10.18653/v1/D17-1004,https://aclanthology.org/D17-1004.pdf,ErniE: Enhanced language representation with informative entities,,5,866,tacred
FewRel,ACL,10.18653/v1/P19-1139,10.18653/v1/D18-1514,https://aclanthology.org/D18-1514.pdf,ErniE: Enhanced language representation with informative entities,,5,866,fewrel
FIGER,ACL,10.18653/v1/P19-1139,10.1162/tacl_a_00141,https://aclanthology.org/Q15-1023.pdf,ErniE: Enhanced language representation with informative entities,,5,866,figer
OpenEntity,ACL,10.18653/v1/P19-1139,10.18653/v1/P18-1009,https://aclanthology.org/P18-1009.pdf,ErniE: Enhanced language representation with informative entities,,5,866,openentity
WikiText-103,ACL,10.18653/v1/P19-1285,10.48550/arXiv.1609.07843,https://arxiv.org/pdf/1609.07843,Transformer-XL: Attentive language models beyond a fixed-length context,,5,1294,wikitext-103
WikiText-2,ACL,10.18653/v1/P19-1285,10.48550/arXiv.1609.07843,https://arxiv.org/pdf/1609.07843,Transformer-XL: Attentive language models beyond a fixed-length context,,5,1294,wikitext-2
enwik8,ACL,10.18653/v1/P19-1285,N/A,https://www.mattmahoney.net/dc/textdata.html,Transformer-XL: Attentive language models beyond a fixed-length context,,5,1294,enwik8
text8,ACL,10.18653/v1/P19-1285,N/A,https://www.mattmahoney.net/dc/textdata.html,Transformer-XL: Attentive language models beyond a fixed-length context,,5,1294,text8
PTB,ACL,10.18653/v1/P19-1285,N/A,https://aclanthology.org/J93-2004/,Transformer-XL: Attentive language models beyond a fixed-length context,,5,1294,ptb
1B Word,ACL,10.18653/v1/P19-1285,10.48550/arXiv.1312.3005,https://arxiv.org/pdf/1312.3005,Transformer-XL: Attentive language models beyond a fixed-length context,,5,1294,1b word
SNLI,ACL,10.18653/v1/P19-1356,10.18653/v1/D15-1075,https://aclanthology.org/D15-1075.pdf,What does BERT learn about the structure of language?,,5,944,snli
ProbingTasks,ACL,10.18653/v1/P19-1356,10.18653/v1/P18-1198,https://aclanthology.org/P18-1198.pdf,What does BERT learn about the structure of language?,,5,944,probingtasks
CoNLL-2000,ACL,10.18653/v1/P19-1356,N/A,https://aclanthology.org/W00-0726.pdf,What does BERT learn about the structure of language?,,5,944,conll-2000
CoNLL-2002,ACL,10.18653/v1/P19-1493,N/A,https://aclanthology.org/W02-2024.pdf,How multilingual is multilingual BERT?,,5,778,conll-2002
CoNLL-2003,ACL,10.18653/v1/P19-1493,N/A,https://aclanthology.org/W03-0419/,How multilingual is multilingual BERT?,,5,778,conll-2003
in-house dataset,ACL,10.18653/v1/P19-1493,Not Found,Not Found,How multilingual is multilingual BERT?,bruh,5,778,in-house dataset
UD,ACL,10.18653/v1/P19-1493,N/A,https://aclanthology.org/L16-1262.pdf,How multilingual is multilingual BERT?,,5,778,ud
WMT16,ACL,10.18653/v1/P19-1493,10.18653/v1/W16-2301,https://aclanthology.org/W16-2301.pdf,How multilingual is multilingual BERT?,"they referred WMT16 metrics shared task, but probably referred to WMT in general",5,778,wmt16
IEMOCAP,ACL,10.18653/v1/P19-1656,10.1007/s10579-008-9076-6,https://link-springer-com.tudelft.idm.oclc.org/article/10.1007/s10579-008-9076-6,Multimodal transformer for unaligned multimodal language sequences,,5,1151,iemocap
CMU-MOSI,ACL,10.18653/v1/P19-1656,10.1109/MIS.2016.94,http://multicomp.cs.cmu.edu/wp-content/uploads/2017/09/2016_IS_Zadeh_Multimodal.pdf,Multimodal transformer for unaligned multimodal language sequences,,5,1151,cmu-mosi
CMU-MOSEI,ACL,10.18653/v1/P19-1656,10.18653/v1/P18-1208,https://aclanthology.org/P18-1208.pdf,Multimodal transformer for unaligned multimodal language sequences,,5,1151,cmu-mosei
Word Analogy Task,ACL,10.3115/v1/d14-1162,10.48550/arXiv.1301.3781,https://arxiv.org/pdf/1301.3781,GloVe: Global vectors for word representation,"Quite a vague reference, hard to understand which dataset was used in general",15,27202,word analogy task
WS-353,ACL,10.3115/v1/d14-1162,10.1145/371920.372094,https://theory.stanford.edu/~matias/papers/context_search.pdf,GloVe: Global vectors for word representation,,15,27202,ws-353
SCWS,ACL,10.3115/v1/d14-1162,N/A,https://aclanthology.org/P12-1092.pdf,GloVe: Global vectors for word representation,,15,27202,scws
RW,ACL,10.3115/v1/d14-1162,N/A,https://aclanthology.org/W13-3512.pdf,GloVe: Global vectors for word representation,,15,27202,rw
MC,ACL,10.3115/v1/d14-1162,https://doi.org/10.1080/01690969108406936,https://github.com/MohamedAliHadjTaieb/Semantic-measure-assessment-review-study/blob/master/DataSets/English/Semantic%20Similarity/MC30.csv,GloVe: Global vectors for word representation,,15,27202,mc
RG,ACL,10.3115/v1/d14-1162,10.1145/365628.365657,https://dl.acm.org/doi/pdf/10.1145/365628.365657,GloVe: Global vectors for word representation,,15,27202,rg
CoNLL-2003,ACL,10.3115/v1/d14-1162,N/A,https://aclanthology.org/W03-0419/,GloVe: Global vectors for word representation,,15,27202,conll-2003
Gigaword5,ACL,10.3115/v1/d14-1162,10.35111/wk4f-qt80,https://catalog.ldc.upenn.edu/LDC2011T07,GloVe: Global vectors for word representation,,15,27202,gigaword5
English Wikipedia,ACL,10.3115/v1/d14-1162,N/A,N/A,GloVe: Global vectors for word representation,"Vague references to the date (year), no preprocessing specified",15,27202,english wikipedia
ACE-2003,ACL,10.3115/v1/d14-1162,10.35111/7xtm-ys65,https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/acl2003-multilingual-resources-for-entity-extraction.pdf,GloVe: Global vectors for word representation,,15,27202,ace-2003
ACE phase 2,ACL,10.3115/v1/d14-1162,N/A,N/A,GloVe: Global vectors for word representation,,,27202,
MUC-7,ACL,10.3115/v1/d14-1162,10.35111/jygm-3h55,https://aclanthology.org/M98-1001.pdf,GloVe: Global vectors for word representation,,15,27202,muc-7
WMT14,ACL,10.3115/v1/d14-1179,10.3115/v1/W14-3302,https://aclanthology.org/W14-3302.pdf,Learning phrase representations using RNN encoder-decoder for statistical machine translation,"WMT not referenced, but at least say they use news ",15,11823,wmt14
MR,ACL,10.3115/v1/d14-1181,10.3115/1219840.1219855,https://dl.acm.org/doi/pdf/10.3115/1219840.1219855,Convolutional neural networks for sentence classification,,15,8128,mr
SST-1,ACL,10.3115/v1/d14-1181,N/A,https://aclanthology.org/D13-1170.pdf,Convolutional neural networks for sentence classification,,15,8128,sst-1
SST-2,ACL,10.3115/v1/d14-1181,N/A,https://aclanthology.org/D13-1170.pdf,Convolutional neural networks for sentence classification,,15,8128,sst-2
Subj,ACL,10.3115/v1/d14-1181,10.3115/1218955.1218990,https://aclanthology.org/P04-1035.pdf,Convolutional neural networks for sentence classification,,15,8128,subj
TREC,ACL,10.3115/v1/d14-1181,N/A,https://aclanthology.org/C02-1150.pdf,Convolutional neural networks for sentence classification,,15,8128,trec
CR,ACL,10.3115/v1/d14-1181,10.1145/1014052.1014073,https://www.cs.uic.edu/~liub/publications/kdd04-revSummary.pdf,Convolutional neural networks for sentence classification,,15,8128,cr
MPQA,ACL,10.3115/v1/d14-1181,10.1007/s10579-005-7880-9,https://www.cs.cornell.edu/home/cardie/papers/lre05withappendix.pdf,Convolutional neural networks for sentence classification,,15,8128,mpqa
Multi-NLI,ACL,10.48550/arXiv.1908.10084,10.18653/v1/N18-1101,https://aclanthology.org/N18-1101.pdf,Sentence-BERT: Sentence embeddings using siamese BERT-networks,,15,6220,multi-nli
SNLI,ACL,10.48550/arXiv.1908.10084,10.18653/v1/D15-1075,https://aclanthology.org/D15-1075.pdf,Sentence-BERT: Sentence embeddings using siamese BERT-networks,,15,6220,snli
STS2012,ACL,10.48550/arXiv.1908.10084,N/A,https://aclanthology.org/S12-1051.pdf,Sentence-BERT: Sentence embeddings using siamese BERT-networks,,15,6220,sts2012
STS2013,ACL,10.48550/arXiv.1908.10084,N/A,https://aclanthology.org/S13-1004.pdf,Sentence-BERT: Sentence embeddings using siamese BERT-networks,,15,6220,sts2013
STS2014,ACL,10.48550/arXiv.1908.10084,10.3115/v1/S14-2010,https://aclanthology.org/S14-2010.pdf,Sentence-BERT: Sentence embeddings using siamese BERT-networks,,15,6220,sts2014
STS2015,ACL,10.48550/arXiv.1908.10084,10.18653/v1/S15-2045,https://aclanthology.org/S15-2045.pdf,Sentence-BERT: Sentence embeddings using siamese BERT-networks,,15,6220,sts2015
STS2016,ACL,10.48550/arXiv.1908.10084,10.18653/v1/S16-1081,https://aclanthology.org/S16-1081.pdf,Sentence-BERT: Sentence embeddings using siamese BERT-networks,,15,6220,sts2016
STSb,ACL,10.48550/arXiv.1908.10084,10.48550/arXiv.1708.00055,https://arxiv.org/pdf/1708.00055,Sentence-BERT: Sentence embeddings using siamese BERT-networks,,15,6220,stsb
SICK,ACL,10.48550/arXiv.1908.10084,N/A,http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf,Sentence-BERT: Sentence embeddings using siamese BERT-networks,,15,6220,sick
AFS,ACL,10.48550/arXiv.1908.10084,10.18653/v1/W16-3636,https://aclanthology.org/W16-3636.pdf,Sentence-BERT: Sentence embeddings using siamese BERT-networks,,15,6220,afs
Wikipedia Sections,ACL,10.48550/arXiv.1908.10084,10.18653/v1/P18-2009,https://aclanthology.org/P18-2009.pdf,Sentence-BERT: Sentence embeddings using siamese BERT-networks,,15,6220,wikipedia sections
MR,ACL,10.48550/arXiv.1908.10084,10.3115/1219840.1219855,https://dl.acm.org/doi/pdf/10.3115/1219840.1219855,Sentence-BERT: Sentence embeddings using siamese BERT-networks,,15,6220,mr
CR,ACL,10.48550/arXiv.1908.10084,10.1145/1014052.1014073,https://www.cs.uic.edu/~liub/publications/kdd04-revSummary.pdf,Sentence-BERT: Sentence embeddings using siamese BERT-networks,,15,6220,cr
SUBJ,ACL,10.48550/arXiv.1908.10084,10.3115/1218955.1218990,https://aclanthology.org/P04-1035.pdf,Sentence-BERT: Sentence embeddings using siamese BERT-networks,,15,6220,subj
MPQA,ACL,10.48550/arXiv.1908.10084,10.1007/s10579-005-7880-9,https://www.cs.cornell.edu/home/cardie/papers/lre05withappendix.pdf,Sentence-BERT: Sentence embeddings using siamese BERT-networks,,15,6220,mpqa
SST-1,ACL,10.48550/arXiv.1908.10084,N/A,https://aclanthology.org/D13-1170.pdf,Sentence-BERT: Sentence embeddings using siamese BERT-networks,"doesn't mention -1, but we assume so",15,6220,sst-1
TREC,ACL,10.48550/arXiv.1908.10084,N/A,https://aclanthology.org/C02-1150.pdf,Sentence-BERT: Sentence embeddings using siamese BERT-networks,,15,6220,trec
MRPC,ACL,10.48550/arXiv.1908.10084,https://aclanthology.org/I05-5002/,https://aclanthology.org/I05-5002.pdf,Sentence-BERT: Sentence embeddings using siamese BERT-networks,,15,6220,mrpc
synthetic (pretraining),ACL,10.48550/arXiv.2004.04696,Not Found,Not Found,BLEURT: Learning robust metrics for text generation,,5,857,synthetic (pretraining)
WMT17Metrics,ACL,10.48550/arXiv.2004.04696,10.18653/v1/W17-4755,https://aclanthology.org/W17-4755.pdf,BLEURT: Learning robust metrics for text generation,evaluation,5,857,wmt17metrics
WMT18Metrics,ACL,10.48550/arXiv.2004.04696,10.18653/v1/W18-6450,https://aclanthology.org/W18-6450.pdf,BLEURT: Learning robust metrics for text generation,evaluation,5,857,wmt18metrics
WMT19Metrics,ACL,10.48550/arXiv.2004.04696,10.18653/v1/W19-5302,https://aclanthology.org/W19-5302.pdf,BLEURT: Learning robust metrics for text generation,evaluation,5,857,wmt19metrics
WebNLG 2017,ACL,10.48550/arXiv.2004.04696,10.18653/v1/W17-3518,https://aclanthology.org/W17-3518.pdf,BLEURT: Learning robust metrics for text generation,evaluation,5,857,webnlg 2017
Multi-NLI,ACL,10.48550/arXiv.2004.04696,10.18653/v1/N18-1101,https://aclanthology.org/N18-1101.pdf,BLEURT: Learning robust metrics for text generation,,5,857,multi-nli
SST-1,ACL,https://aclanthology.org/D13-1170/,N/A,https://aclanthology.org/D13-1170.pdf,Recursive deep models for semantic compositionality over a sentiment treebank,Dataset paper,15,6172,sst-1
Syntactic Test Set,ACL,https://aclanthology.org/N13-1090/,Not Found,Not Found,Linguistic regularities in continuous spaceword representations,Broken link to dataset generated by them,15,2673,syntactic test set
SemEval2012-2,ACL,https://aclanthology.org/N13-1090/,N/A,https://aclanthology.org/S12-1047.pdf,Linguistic regularities in continuous spaceword representations,,15,2673,semeval2012-2
IMDB reviews,ACL,https://aclanthology.org/P11-1015/,N/A,https://aclanthology.org/P11-1015,Learning word vectors for sentiment analysis,,15,4040,imdb reviews
Subj,ACL,https://aclanthology.org/P11-1015/,10.3115/1218955.1218990,https://aclanthology.org/P04-1035.pdf,Learning word vectors for sentiment analysis,,15,4040,subj
Movie Review,ACL,https://aclanthology.org/P11-1015/,10.3115/1218955.1218990,https://aclanthology.org/P04-1035.pdf,Learning word vectors for sentiment analysis,,15,4040,movie review
KITTI,CVPR,10.1109/CVPR.2012.6248074,10.1109/CVPR.2012.6248074,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6248074,Are we ready for autonomous driving? the KITTI vision benchmark suite,intrdoduced by the paper,15,11838,kitti
Sports-1M,CVPR,10.1109/CVPR.2014.223,10.1109/CVPR.2014.223,https://paperswithcode.com/paper/large-scale-video-classification-with-1,Large-scale video classification with convolutional neural networks,,15,5661,sports-1m
UCF101,CVPR,10.1109/CVPR.2014.223,10.48550/arXiv.1212.0402,https://arxiv.org/pdf/1212.0402,Large-scale video classification with convolutional neural networks,,15,5661,ucf101
PASCAL VOC 2007,CVPR,10.1109/CVPR.2014.81,10.1007/s11263-009-0275-4,https://link.springer.com/article/10.1007/s11263-009-0275-4#preview,Rich feature hierarchies for accurate object detection and semantic segmentation,,15,26779,pascal voc 2007
PASCAL VOC 2010,CVPR,10.1109/CVPR.2014.81,10.1007/s11263-009-0275-4,https://ieeexplore.ieee.org/document/6909514,Rich feature hierarchies for accurate object detection and semantic segmentation,,,,
PASCAL VOC 2011,CVPR,10.1109/CVPR.2014.81,N/A,http://host.robots.ox.ac.uk/pascal/VOC/voc2011/index.html,Rich feature hierarchies for accurate object detection and semantic segmentation,,,,
PASCAL VOC 2012,CVPR,10.1109/CVPR.2014.81,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,Rich feature hierarchies for accurate object detection and semantic segmentation,,,,
ImageNet 2012,CVPR,10.1109/CVPR.2014.81,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,Rich feature hierarchies for accurate object detection and semantic segmentation,,15,26779,imagenet 2012
ILSVRC 2014,CVPR,10.1109/CVPR.2015.7298594,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,Going deeper with convolutions,,15,40152,ilsvrc 2014
LFW,CVPR,10.1109/CVPR.2015.7298682,http://www.tamaraberg.com/papers/lfw.pdf,http://www.tamaraberg.com/papers/lfw.pdf,FaceNet: A unified embedding for face recognition and clustering,,15,11725,lfw
YouTube Faces DB,CVPR,10.1109/CVPR.2015.7298682,10.1109/CVPR.2011.5995566,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5995566,FaceNet: A unified embedding for face recognition and clustering,,15,11725,youtube faces db
Personal Photos,CVPR,10.1109/CVPR.2015.7298682,10.1109/CVPR.2015.7298682,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=7298682,FaceNet: A unified embedding for face recognition and clustering,"made by authors, not publicly released",15,11725,personal photos
CMU PIE,CVPR,10.1109/CVPR.2015.7298682,10.1109/AFGR.2002.1004130,https://www.cs.cmu.edu/~simonb/pie_db/pami.pdf,FaceNet: A unified embedding for face recognition and clustering,"used for ilustration, not for training",15,11725,cmu pie
Hold-out test set,CVPR,10.1109/CVPR.2015.7298682,10.1109/CVPR.2015.7298682,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=7298682,FaceNet: A unified embedding for face recognition and clustering,"made by authors, not publicly released",15,11725,hold-out test set
Face Thumbnails,CVPR,10.1109/CVPR.2015.7298682,10.1109/CVPR.2015.7298682,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=7298682,FaceNet: A unified embedding for face recognition and clustering,"made by authors, for training, private data",15,11725,face thumbnails
PASCAL VOC 2011,CVPR,10.1109/CVPR.2015.7298965,N/A,http://host.robots.ox.ac.uk/pascal/VOC/voc2011/index.html,Fully convolutional networks for semantic segmentation,,15,25035,pascal voc 2011
PASCAL VOC 2012,CVPR,10.1109/CVPR.2015.7298965,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,Fully convolutional networks for semantic segmentation,,,,
NYUDv2,CVPR,10.1109/CVPR.2015.7298965,10.1007/978-3-642-33715-4_54,https://link.springer.com/chapter/10.1007/978-3-642-33715-4_54,Fully convolutional networks for semantic segmentation,,15,25035,nyudv2
SIFT Flow,CVPR,10.1109/CVPR.2015.7298965,10.1109/TPAMI.2010.147,https://people.csail.mit.edu/celiu/SIFTflow/SIFTflow.pdf,Fully convolutional networks for semantic segmentation,,15,25035,sift flow
Set5,CVPR,10.1109/CVPR.2016.182,10.5244/C.26.135,https://www.researchgate.net/publication/260351242_Low-Complexity_Single_Image_Super-Resolution_Based_on_Nonnegative_Neighbor_Embedding,Accurate image super-resolution using very deep convolutional networks,,15,6707,set5
Set14,CVPR,10.1109/CVPR.2016.182,10.1007/978-3-642-27413-8_47,https://link.springer.com/chapter/10.1007/978-3-642-27413-8_47,Accurate image super-resolution using very deep convolutional networks,,15,6707,set14
Urban100,CVPR,10.1109/CVPR.2016.182,10.1109/CVPR.2015.7299156,https://paperswithcode.com/paper/single-image-super-resolution-from,Accurate image super-resolution using very deep convolutional networks,,15,6707,urban100
BSD100,CVPR,10.1109/CVPR.2016.182,10.1109/ICCV.2001.937655,https://ieeexplore.ieee.org/document/937655,Accurate image super-resolution using very deep convolutional networks,they call it B100,15,6707,bsd100
BSDS300,CVPR,10.1109/CVPR.2016.182,10.1109/ICCV.2001.937655,https://ieeexplore.ieee.org/document/937655,Accurate image super-resolution using very deep convolutional networks,it represents the 200 in 291 images,15,6707,bsds300
Set5,CVPR,10.1109/CVPR.2016.207,10.5244/C.26.135,https://www.researchgate.net/publication/260351242_Low-Complexity_Single_Image_Super-Resolution_Based_on_Nonnegative_Neighbor_Embedding,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,,15,6115,set5
Set14,CVPR,10.1109/CVPR.2016.207,10.1007/978-3-642-27413-8_47,https://link.springer.com/chapter/10.1007/978-3-642-27413-8_47,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,,15,6115,set14
BSDS300,CVPR,10.1109/CVPR.2016.207,10.1109/ICCV.2001.937655,https://ieeexplore.ieee.org/document/937655,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,,15,6115,bsds300
BSDS500,CVPR,10.1109/CVPR.2016.207,10.1109/TPAMI.2010.161,https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/papers/amfm_pami2010.pdf,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,,15,6115,bsds500
SuperTexture,CVPR,10.1109/CVPR.2016.207,10.1111/cgf.12544,https://varcity.ethz.ch/paper/eg2015_dai_jor.pdf,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,,15,6115,supertexture
ImageNet,CVPR,10.1109/CVPR.2016.207,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,50.000 random picked,15,6115,imagenet
Xiph,CVPR,10.1109/CVPR.2016.207,N/A,https://media.xiph.org/video/derf/,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,,15,6115,xiph
Ultra Video Group,CVPR,10.1109/CVPR.2016.207,N/A,https://ultravideo.fi/,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,,15,6115,ultra video group
91 dataset,CVPR,10.1109/CVPR.2016.207,10.1109/CVPR.2008.4587647,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4587647,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,"following the refference layers , this seem to be the dataset used",15,6115,91 dataset
ImageNet 2012,CVPR,10.1109/CVPR.2016.308,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,Rethinking the Inception Architecture for Computer Vision,,15,24488,imagenet 2012
ILSVRC 2014,CVPR,10.1109/CVPR.2016.319,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,Learning Deep Features for Discriminative Localization,,15,8736,ilsvrc 2014
CUB-200-2011 Birds,CVPR,10.1109/CVPR.2016.319,N/A,https://authors.library.caltech.edu/records/cvm3y-5hh21,Learning Deep Features for Discriminative Localization,,15,8736,cub-200-2011 birds
SUN397,CVPR,10.1109/CVPR.2016.319,10.1109/CVPR.2010.5539970,https://faculty.cc.gatech.edu/~hays/papers/sun.pdf,Learning Deep Features for Discriminative Localization,,15,8736,sun397
MIT Indoor67,CVPR,10.1109/CVPR.2016.319,10.1109/CVPRW.2009.5206537 ,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206537,Learning Deep Features for Discriminative Localization,,15,8736,mit indoor67
Scene15,CVPR,10.1109/CVPR.2016.319,N/A,https://www.kaggle.com/datasets/zaiyankhan/15scene-dataset,Learning Deep Features for Discriminative Localization,wrong citation,15,8736,scene15
SUN Attribute,CVPR,10.1109/CVPR.2016.319,10.1109/CVPR.2012.6247998,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/6247998,Learning Deep Features for Discriminative Localization,,15,8736,sun attribute
Caltech101,CVPR,10.1109/CVPR.2016.319,10.22002/D1.20086,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/1384978,Learning Deep Features for Discriminative Localization,,15,8736,caltech101
Caltech256,CVPR,10.1109/CVPR.2016.319,10.22002/D1.20087,https://www.researchgate.net/publication/30766223_Caltech-256_Object_Category_Dataset,Learning Deep Features for Discriminative Localization,,15,8736,caltech256
Stanford 40 Actions,CVPR,10.1109/CVPR.2016.319,10.1109/ICCV.2011.6126386,https://ieeexplore.ieee.org/document/6126386,Learning Deep Features for Discriminative Localization,,15,8736,stanford 40 actions
Cityscapes,CVPR,10.1109/CVPR.2016.350,10.1109/CVPR.2016.350,https://ieeexplore.ieee.org/document/7780719,The Cityscapes Dataset for Semantic Urban Scene Understanding,intrdoduced by the paper,15,9950,cityscapes
CIFAR-10,CVPR,10.1109/CVPR.2016.90,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Deep residual learning for image recognition,,15,178632,cifar-10
COCO,CVPR,10.1109/CVPR.2016.90,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Deep residual learning for image recognition,,15,178632,coco
ImageNet 2012,CVPR,10.1109/CVPR.2016.90,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,Deep residual learning for image recognition,,15,178632,imagenet 2012
PASCAL VOC 2007,CVPR,10.1109/CVPR.2016.90,10.1007/s11263-009-0275-4,https://link.springer.com/article/10.1007/s11263-009-0275-4#preview,Deep residual learning for image recognition,,15,178632,pascal voc 2007
PASCAL VOC 2012,CVPR,10.1109/CVPR.2016.90,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,Deep residual learning for image recognition,,,,
ImageNet,CVPR,10.1109/CVPR.2016.91,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,"You only look once: Unified, real-time object detection",,15,38749,imagenet
PASCAL VOC 2007,CVPR,10.1109/CVPR.2016.91,10.1007/s11263-009-0275-4,https://link.springer.com/article/10.1007/s11263-009-0275-4#preview,"You only look once: Unified, real-time object detection",,15,38749,pascal voc 2007
PASCAL VOC 2012,CVPR,10.1109/CVPR.2016.91,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,"You only look once: Unified, real-time object detection",,,,
Picasso,CVPR,10.1109/CVPR.2016.91,10.48550/arXiv.1409.6235,https://shiry.ttic.edu/publications/Picasso_ECCV_2014.pdf,"You only look once: Unified, real-time object detection",just for comparison at the end,15,38749,picasso
People-Art,CVPR,10.1109/CVPR.2016.91,10.48550/arXiv.1505.00110,https://arxiv.org/pdf/1505.00110,"You only look once: Unified, real-time object detection",just for comparison at the end,15,38749,people-art
BAPPS,CVPR,10.1109/CVPR.2018.00068,10.1109/CVPR.2018.00068,https://arxiv.org/pdf/1801.03924,The Unreasonable Effectiveness of Deep Features as a Perceptual Metric,they introduced the dataset,15,8710,bapps
ImageNet,CVPR,10.1109/CVPR.2018.00068,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,The Unreasonable Effectiveness of Deep Features as a Perceptual Metric,they cite the Imagenet-1k,15,8710,imagenet
DIV2K,CVPR,10.1109/CVPR.2018.00068,10.1109/CVPRW.2017.150,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/8014884,The Unreasonable Effectiveness of Deep Features as a Perceptual Metric,,15,8710,div2k
Middlebury,CVPR,10.1109/CVPR.2018.00068,https://vision.middlebury.edu/stereo/data/,https://vision.middlebury.edu/stereo/taxonomy-IJCV.pdf,The Unreasonable Effectiveness of Deep Features as a Perceptual Metric,"named Davis Middlebury, but the citation indicates the Middlebury dataset",15,8710,middlebury
Video Deblurring,CVPR,10.1109/CVPR.2018.00068,10.1109/CVPR.2017.33,https://openaccess.thecvf.com/content_cvpr_2017/papers/Su_Deep_Video_Deblurring_CVPR_2017_paper.pdf,The Unreasonable Effectiveness of Deep Features as a Perceptual Metric,,15,8710,video deblurring
ImageNet 2012,CVPR,10.1109/CVPR.2018.00474,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,MobileNetV2: Inverted Residuals and Linear Bottlenecks,,15,19775,imagenet 2012
COCO,CVPR,10.1109/CVPR.2018.00474,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,MobileNetV2: Inverted Residuals and Linear Bottlenecks,,15,19775,coco
PASCAL VOC 2012,CVPR,10.1109/CVPR.2018.00474,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,MobileNetV2: Inverted Residuals and Linear Bottlenecks,,15,19775,pascal voc 2012
COCO,CVPR,10.1109/CVPR.2018.00644,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Cascade R-CNN: Delving into High Quality Object Detection,,15,5558,coco
PASCAL VOC 2007,CVPR,10.1109/CVPR.2018.00644,10.1007/s11263-009-0275-4,https://link.springer.com/article/10.1007/s11263-009-0275-4#preview,Cascade R-CNN: Delving into High Quality Object Detection,,15,5558,pascal voc 2007
PASCAL VOC 2012,CVPR,10.1109/CVPR.2018.00644,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,Cascade R-CNN: Delving into High Quality Object Detection,,,,
ImageNet 2012,CVPR,10.1109/CVPR.2018.00716,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,,15,6926,imagenet 2012
COCO,CVPR,10.1109/CVPR.2018.00716,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,,15,6926,coco
Places,CVPR,10.1109/CVPR.2018.00745,10.1109/TPAMI.2017.2723009,http://places2.csail.mit.edu/PAMI_places.pdf,Squeeze-and-Excitation Networks,"they are using a subset, Places365",15,26785,places
ImageNet,CVPR,10.1109/CVPR.2018.00745,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Squeeze-and-Excitation Networks,,15,26785,imagenet
Kinetics,CVPR,10.1109/CVPR.2018.00813,10.48550/arXiv.1705.06950,https://arxiv.org/abs/1705.06950,Non-local Neural Networks,,15,9659,kinetics
Charades,CVPR,10.1109/CVPR.2018.00813,10.1007/978-3-319-46448-0_31,https://www.researchgate.net/publication/301875759_Hollywood_in_Homes_Crowdsourcing_Data_Collection_for_Activity_Understanding,Non-local Neural Networks,,15,9659,charades
COCO,CVPR,10.1109/CVPR.2018.00813,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Non-local Neural Networks,,15,9659,coco
ImageNet,CVPR,10.1109/CVPR.2018.00813,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Non-local Neural Networks,,15,9659,imagenet
COCO,CVPR,10.1109/CVPR.2018.00913,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Path Aggregation Network for Instance Segmentation,,15,7389,coco
Cityscapes,CVPR,10.1109/CVPR.2018.00913,10.1109/CVPR.2016.350,https://ieeexplore.ieee.org/document/7780719,Path Aggregation Network for Instance Segmentation,,15,7389,cityscapes
MVD,CVPR,10.1109/CVPR.2018.00913,10.1109/ICCV.2017.534,https://openaccess.thecvf.com/content_ICCV_2017/papers/Neuhold_The_Mapillary_Vistas_ICCV_2017_paper.pdf,Path Aggregation Network for Instance Segmentation,,15,7389,mvd
FFHQ,CVPR,10.1109/CVPR.2019.00453,10.48550/arXiv.1812.04948,https://openaccess.thecvf.com/content_CVPR_2019/papers/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.pdf,A style-based generator architecture for generative adversarial networks,introduced by the paper,15,7066,ffhq
CelebAHQ,CVPR,10.1109/CVPR.2019.00453,10.48550/arXiv.1710.10196,https://openreview.net/pdf?id=Hk99zCeAb,A style-based generator architecture for generative adversarial networks,,15,7066,celebahq
CIFAR-10,CVPR,10.1109/CVPR42600.2020.00165,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,GhostNet: More features from cheap operations,,5,3420,cifar-10
COCO,CVPR,10.1109/CVPR42600.2020.00165,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,GhostNet: More features from cheap operations,,5,3420,coco
Imagenet 2012,CVPR,10.1109/CVPR42600.2020.00165,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,GhostNet: More features from cheap operations,,5,3420,imagenet 2012
COCO,CVPR,10.1109/CVPR42600.2020.00165,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,GhostNet: More features from cheap operations,"it mentiones that more details about the training of the model are in the Appendix, but the paper doesn't have any Appendix",5,3420,coco
LOL,CVPR,10.1109/CVPR42600.2020.00185,10.48550/arXiv.1808.04560,https://arxiv.org/abs/1808.04560,Zero-reference deep curve estimation for low-light image enhancement,,5,1630,lol
MIT-Adobe FiveK,CVPR,10.1109/CVPR42600.2020.00185,10.1109/CVPR.2011.5995332,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=5995332,Zero-reference deep curve estimation for low-light image enhancement,,5,1630,mit-adobe fivek
SICE,CVPR,10.1109/CVPR42600.2020.00185,10.1109/TIP.2018.2794218,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/8259342,Zero-reference deep curve estimation for low-light image enhancement,"for some actions only Part1 is used, for others only Part2 and for others both Part1 and Part2",5,1630,sice
DARK FACE,CVPR,10.1109/CVPR42600.2020.00185,10.1109/TIP.2020.2981922,https://www.researchgate.net/publication/340238487_Advancing_Image_Understanding_in_Poor_Visibility_Environments_A_Collective_Benchmark_Study,Zero-reference deep curve estimation for low-light image enhancement,,5,1630,dark face
WiderFace,CVPR,10.1109/CVPR42600.2020.00185,10.1109/CVPR.2016.596,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/7780965,Zero-reference deep curve estimation for low-light image enhancement,,5,1630,widerface
NPE,CVPR,10.1109/CVPR42600.2020.00185,10.1109/TIP.2013.2261309,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/6512558,Zero-reference deep curve estimation for low-light image enhancement,benchmark,5,1630,npe
LIME,CVPR,10.1109/CVPR42600.2020.00185,10.1109/TIP.2016.2639450,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/7782813,Zero-reference deep curve estimation for low-light image enhancement,benchmark,5,1630,lime
MEF,CVPR,10.1109/CVPR42600.2020.00185,10.1109/TIP.2015.2442920,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/7120119,Zero-reference deep curve estimation for low-light image enhancement,benchmark,5,1630,mef
DICM,CVPR,10.1109/CVPR42600.2020.00185,10.1109/ICIP.2012.6467022,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/6467022,Zero-reference deep curve estimation for low-light image enhancement,benchmark,5,1630,dicm
VV,CVPR,10.1109/CVPR42600.2020.00185,N/A,https://sites.google.com/site/vonikakis/datasets?authuser=0,Zero-reference deep curve estimation for low-light image enhancement,"benchmark; it references a link to the owner of the dataset, however on the link, multiplke datasets can be identified",5,1630,vv
Waymo,CVPR,10.1109/CVPR42600.2020.00252,10.1109/CVPR42600.2020.00252,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9156973,Scalability in perception for autonomous driving: Waymo open dataset,,5,2058,waymo
BDD100K,CVPR,10.1109/CVPR42600.2020.00271,10.1109/CVPR42600.2020.00271,https://arxiv.org/pdf/1805.04687,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"introduced by the paper, but the reference link in the paper is not working",5,1604,bdd100k
Oxford and Paris,CVPR,10.1109/CVPR42600.2020.00499,10.48550/arXiv.1803.11285,https://arxiv.org/pdf/1803.11285,SuperGlue: Learning Feature Matching with Graph Neural Networks,used as underlying images to generate pairs,5,1887,oxford and paris
ScanNET,CVPR,10.1109/CVPR42600.2020.00499,10.48550/arXiv.1702.04405,https://arxiv.org/pdf/1702.04405,SuperGlue: Learning Feature Matching with Graph Neural Networks,,5,1887,scannet
PhotoTourism,CVPR,10.1109/CVPR42600.2020.00499,10.1145/1141911.1141964,https://phototour.cs.washington.edu/Photo_Tourism.pdf,SuperGlue: Learning Feature Matching with Graph Neural Networks,"cited the dataset for which PhotoTourism is a subset: YFCC100M, and a challange that is part of",5,1887,phototourism
Mega Depth,CVPR,10.1109/CVPR42600.2020.00499,10.1109/CVPR.2018.00218,https://www.cs.cornell.edu/projects/megadepth/,SuperGlue: Learning Feature Matching with Graph Neural Networks,,5,1887,mega depth
FFHQ,CVPR,10.1109/CVPR42600.2020.00813,10.48550/arXiv.1812.04948,https://openaccess.thecvf.com/content_CVPR_2019/papers/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.pdf,Analyzing and improving the image quality of stylegan,no reference,5,4218,ffhq
LSUN,CVPR,10.1109/CVPR42600.2020.00813,10.48550/arXiv.1506.03365,https://www.researchgate.net/publication/278048515_LSUN_Construction_of_a_Large-scale_Image_Dataset_using_Deep_Learning_with_Humans_in_the_Loop,Analyzing and improving the image quality of stylegan,"no reference, used CAR, CAT, CHURCH, HORSE subset",5,4218,lsun
ImageNet-1K,CVPR,10.1109/CVPR42600.2020.00975,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,Momentum Contrast for Unsupervised Visual Representation Learning,"in paper it uses ImageNet-1M, but it is the same as 1K (they also mention it). the citation is alos for imagenet",15,9623,imagenet-1k
Instagram-1B,CVPR,10.1109/CVPR42600.2020.00975,10.48550/arXiv.1805.00932,https://openaccess.thecvf.com/content_ECCV_2018/papers/Dhruv_Mahajan_Exploring_the_Limits_ECCV_2018_paper.pdf,Momentum Contrast for Unsupervised Visual Representation Learning,,15,9623,instagram-1b
PASCAL VOC 2007,CVPR,10.1109/CVPR42600.2020.00975,10.1007/s11263-009-0275-4,https://link.springer.com/article/10.1007/s11263-009-0275-4#preview,Momentum Contrast for Unsupervised Visual Representation Learning,,15,9623,pascal voc 2007
COCO,CVPR,10.1109/CVPR42600.2020.00975,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Momentum Contrast for Unsupervised Visual Representation Learning,,15,9623,coco
LVIS,CVPR,10.1109/CVPR42600.2020.00975,10.1109/CVPR.2019.00550,https://ieeexplore.ieee.org/document/8954457,Momentum Contrast for Unsupervised Visual Representation Learning,,15,9623,lvis
Cityscapes,CVPR,10.1109/CVPR42600.2020.00975,10.1109/CVPR.2016.350,https://ieeexplore.ieee.org/document/7780719,Momentum Contrast for Unsupervised Visual Representation Learning,,15,9623,cityscapes
iNaturalist,CVPR,10.1109/CVPR42600.2020.00975,10.1109/CVPR.2018.00914,https://ieeexplore.ieee.org/document/8579012,Momentum Contrast for Unsupervised Visual Representation Learning,,15,9623,inaturalist
COCO,CVPR,10.1109/CVPR42600.2020.00978,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection,,5,1651,coco
KITTI,CVPR,10.1109/CVPR42600.2020.01054,10.1109/CVPR.2012.6248074,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6248074,PV-RCNN: Point-voxel feature set abstraction for 3D object detection,,5,1666,kitti
Waymo,CVPR,10.1109/CVPR42600.2020.01054,10.1109/CVPR42600.2020.00252,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9156973,PV-RCNN: Point-voxel feature set abstraction for 3D object detection,,5,1666,waymo
Imagenet 2012,CVPR,10.1109/CVPR42600.2020.01070,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,Self-training with noisy student improves imagenet classification,no reference,5,1694,imagenet 2012
JFT-300M,CVPR,10.1109/CVPR42600.2020.01070,10.48550/arXiv.1707.02968,https://arxiv.org/pdf/1707.02968v2,Self-training with noisy student improves imagenet classification,"two references, but none of them is to the dataset paper",5,1694,jft-300m
ImageNet-A,CVPR,10.1109/CVPR42600.2020.01070,10.48550/arXiv.1907.07174,https://arxiv.org/pdf/1907.07174,Self-training with noisy student improves imagenet classification,,5,1694,imagenet-a
ImageNet-C,CVPR,10.1109/CVPR42600.2020.01070,10.48550/arXiv.1903.12261,https://arxiv.org/pdf/1903.12261,Self-training with noisy student improves imagenet classification,,5,1694,imagenet-c
ImageNet-P,CVPR,10.1109/CVPR42600.2020.01070,10.48550/arXiv.1903.12261,https://arxiv.org/pdf/1903.12261,Self-training with noisy student improves imagenet classification,,5,1694,imagenet-p
COCO,CVPR,10.1109/CVPR42600.2020.01079,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,EfficientDet: Scalable and efficient object detection,,15,5952,coco
PASCAL VOC 2012,CVPR,10.1109/CVPR42600.2020.01079,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,EfficientDet: Scalable and efficient object detection,for semantic segmentation with EfficientDet,15,5952,pascal voc 2012
Imagenet-1K,CVPR,10.1109/CVPR42600.2020.01155,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,ECA-Net: Efficient channel attention for deep convolutional neural networks,citing ImageNet,15,5703,imagenet-1k
COCO,CVPR,10.1109/CVPR42600.2020.01155,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,ECA-Net: Efficient channel attention for deep convolutional neural networks,,15,5703,coco
nuScenes,CVPR,10.1109/CVPR42600.2020.01164,10.1109/CVPR42600.2020.01164,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=9156412,Nuscenes: A multimodal dataset for autonomous driving,introduced by the paper,5,3672,nuscenes
ADE20K,CVPR,10.1109/CVPR46437.2021.00681,10.1007/s11263-018-1140-0,https://www.researchgate.net/publication/306357649_Semantic_Understanding_of_Scenes_Through_the_ADE20K_Dataset,Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers,,5,2657,ade20k
Cityscapes,CVPR,10.1109/CVPR46437.2021.00681,10.1109/CVPR.2016.350,https://ieeexplore.ieee.org/document/7780719,Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers,,5,2657,cityscapes
PASCAL Context,CVPR,10.1109/CVPR46437.2021.00681,10.1109/CVPR.2014.119,https://cs.stanford.edu/~roozbeh/pascal-context/mottaghi_et_al_cvpr14.pdf,Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers,,5,2657,pascal context
ImageNet,CVPR,10.1109/CVPR46437.2021.01268,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Taming transformers for high-resolution image synthesis,and a subset called RIN,5,1566,imagenet
COCO-Stuff,CVPR,10.1109/CVPR46437.2021.01268,10.48550/arXiv.1612.03716,https://openaccess.thecvf.com/content_cvpr_2018/papers/Caesar_COCO-Stuff_Thing_and_CVPR_2018_paper.pdf,Taming transformers for high-resolution image synthesis,,5,1566,coco-stuff
ADE20K,CVPR,10.1109/CVPR46437.2021.01268,10.1007/s11263-018-1140-0,https://www.researchgate.net/publication/306357649_Semantic_Understanding_of_Scenes_Through_the_ADE20K_Dataset,Taming transformers for high-resolution image synthesis,,5,1566,ade20k
DeepFashion,CVPR,10.1109/CVPR46437.2021.01268,10.1109/CVPR.2016.124,https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf,Taming transformers for high-resolution image synthesis,,5,1566,deepfashion
LSUN-CT,CVPR,10.1109/CVPR46437.2021.01268,10.48550/arXiv.1506.03365,https://www.researchgate.net/publication/278048515_LSUN_Construction_of_a_Large-scale_Image_Dataset_using_Deep_Learning_with_Humans_in_the_Loop,Taming transformers for high-resolution image synthesis,,5,1566,lsun-ct
FFHQ,CVPR,10.1109/CVPR46437.2021.01268,10.48550/arXiv.1812.04948,https://openaccess.thecvf.com/content_CVPR_2019/papers/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.pdf,Taming transformers for high-resolution image synthesis,,5,1566,ffhq
CelebAHQ,CVPR,10.1109/CVPR46437.2021.01268,10.48550/arXiv.1710.10196,https://openreview.net/pdf?id=Hk99zCeAb,Taming transformers for high-resolution image synthesis,,5,1566,celebahq
S-FLCKR,CVPR,10.1109/CVPR46437.2021.01268,10.1109/CVPR46437.2021.01268,https://arxiv.org/pdf/2012.09841,Taming transformers for high-resolution image synthesis,"conditioned on semnatic layouts and obtained their custom dataset, based on Flickr",5,1566,s-flckr
CIFAR-10,CVPR,10.1109/CVPR46437.2021.01268,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Taming transformers for high-resolution image synthesis,,5,1566,cifar-10
PASCAL VOC 2007,CVPR,10.1109/CVPR46437.2021.01350,10.1007/s11263-009-0275-4,https://link.springer.com/article/10.1007/s11263-009-0275-4#preview,Coordinate attention for efficient mobile network design,,5,4225,pascal voc 2007
PASCAL VOC 2012,CVPR,10.1109/CVPR46437.2021.01350,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,Coordinate attention for efficient mobile network design,,5,4225,pascal voc 2012
COCO,CVPR,10.1109/CVPR46437.2021.01350,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Coordinate attention for efficient mobile network design,,5,4225,coco
ImageNet,CVPR,10.1109/CVPR46437.2021.01350,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Coordinate attention for efficient mobile network design,,5,4225,imagenet
Cityscapes,CVPR,10.1109/CVPR46437.2021.01350,10.1109/CVPR.2016.350,https://ieeexplore.ieee.org/document/7780719,Coordinate attention for efficient mobile network design,,5,4225,cityscapes
ImageNet,CVPR,10.1109/CVPR46437.2021.01352,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,RepVgg: Making VGG-style ConvNets Great Again,,5,1754,imagenet
Cityscapes,CVPR,10.1109/CVPR46437.2021.01352,10.1109/CVPR.2016.350,https://ieeexplore.ieee.org/document/7780719,RepVgg: Making VGG-style ConvNets Great Again,,5,1754,cityscapes
ImageNet 2012,CVPR,10.1109/CVPR46437.2021.01549,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,Exploring simple Siamese representation learning,,5,2816,imagenet 2012
Pascal VOC 2007,CVPR,10.1109/CVPR46437.2021.01549,10.1007/s11263-009-0275-4,https://link.springer.com/article/10.1007/s11263-009-0275-4#preview,Exploring simple Siamese representation learning,,5,2816,pascal voc 2007
Pascal VOC 2012,CVPR,10.1109/CVPR46437.2021.01549,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,Exploring simple Siamese representation learning,,5,2816,pascal voc 2012
COCO,CVPR,10.1109/CVPR46437.2021.01549,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Exploring simple Siamese representation learning,,5,2816,coco
SIDD,CVPR,10.1109/CVPR52688.2022.00564,10.1109/CVPR.2018.00182,https://openaccess.thecvf.com/content_cvpr_2018/papers/Abdelhamed_A_High-Quality_Denoising_CVPR_2018_paper.pdf,Restormer: Efficient Transformer for High-Resolution Image Restoration,image denosining,5,1931,sidd
DND,CVPR,10.1109/CVPR52688.2022.00564,10.1109/CVPR.2017.294,https://noise.visinf.tu-darmstadt.de/,Restormer: Efficient Transformer for High-Resolution Image Restoration,image denosining,5,1931,dnd
Test100,CVPR,10.1109/CVPR52688.2022.00564,10.1109/TCSVT.2019.2920407,https://arxiv.org/pdf/1701.05957,Restormer: Efficient Transformer for High-Resolution Image Restoration,image deraining,5,1931,test100
Rain100H,CVPR,10.1109/CVPR52688.2022.00564,10.48550/arXiv.1609.07769,https://arxiv.org/pdf/1609.07769,Restormer: Efficient Transformer for High-Resolution Image Restoration,image deraining,5,1931,rain100h
Rain100L,CVPR,10.1109/CVPR52688.2022.00564,10.48550/arXiv.1609.07769,https://arxiv.org/pdf/1609.07769,Restormer: Efficient Transformer for High-Resolution Image Restoration,image deraining,5,1931,rain100l
Test2800,CVPR,10.1109/CVPR52688.2022.00564,10.1109/CVPR.2017.186,https://openaccess.thecvf.com/content_cvpr_2017/papers/Fu_Removing_Rain_From_CVPR_2017_paper.pdf,Restormer: Efficient Transformer for High-Resolution Image Restoration,image deraining,5,1931,test2800
Test1200,CVPR,10.1109/CVPR52688.2022.00564,10.48550/arXiv.1802.07412,https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Density-Aware_Single_Image_CVPR_2018_paper.pdf,Restormer: Efficient Transformer for High-Resolution Image Restoration,image deraining,5,1931,test1200
GoPro,CVPR,10.1109/CVPR52688.2022.00564,10.1109/CVPR.2017.35,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=8099518,Restormer: Efficient Transformer for High-Resolution Image Restoration,image deblurring,5,1931,gopro
HIDE,CVPR,10.1109/CVPR52688.2022.00564,10.1109/ICCV.2019.00567,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?arnumber=9010839,Restormer: Efficient Transformer for High-Resolution Image Restoration,image deblurring,5,1931,hide
RealBlur-L,CVPR,10.1109/CVPR52688.2022.00564,N/A,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700188.pdf,Restormer: Efficient Transformer for High-Resolution Image Restoration,image deblurring,5,1931,realblur-l
RealBlur-J,CVPR,10.1109/CVPR52688.2022.00564,N/A,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700188.pdf,Restormer: Efficient Transformer for High-Resolution Image Restoration,image deblurring,5,1931,realblur-j
DPDD,CVPR,10.1109/CVPR52688.2022.00564,10.1007/978-3-030-58607-2_7,https://link.springer.com/chapter/10.1007/978-3-030-58607-2_7,Restormer: Efficient Transformer for High-Resolution Image Restoration,defocus deblurring,5,1931,dpdd
Set12,CVPR,10.1109/CVPR52688.2022.00564,10.1109/TIP.2017.2662206,https://www4.comp.polyu.edu.hk/~cslzhang/paper/DnCNN.pdf,Restormer: Efficient Transformer for High-Resolution Image Restoration,synthetic benchmark,5,1931,set12
BSD68,CVPR,10.1109/CVPR52688.2022.00564,10.1109/ICCV.2001.937655,https://ieeexplore.ieee.org/document/937655,Restormer: Efficient Transformer for High-Resolution Image Restoration,synthetic benchmark,5,1931,bsd68
Urban100,CVPR,10.1109/CVPR52688.2022.00564,10.1109/CVPR.2015.7299156,https://paperswithcode.com/paper/single-image-super-resolution-from,Restormer: Efficient Transformer for High-Resolution Image Restoration,synthetic benchmark,5,1931,urban100
Kodak24,CVPR,10.1109/CVPR52688.2022.00564,https://r0k.us/graphics/kodak/,https://r0k.us/graphics/kodak/,Restormer: Efficient Transformer for High-Resolution Image Restoration,synthetic benchmark,5,1931,kodak24
McMaster,CVPR,10.1109/CVPR52688.2022.00564,10.1117/1.3600632,https://researchrepository.wvu.edu/cgi/viewcontent.cgi?article=1087&context=faculty_publications,Restormer: Efficient Transformer for High-Resolution Image Restoration,synthetic benchmark,5,1931,mcmaster
ImageNet,CVPR,10.1109/CVPR52688.2022.01042,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,High-Resolution Image Synthesis with Latent Diffusion Models,,15,7632,imagenet
COCO,CVPR,10.1109/CVPR52688.2022.01042,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,High-Resolution Image Synthesis with Latent Diffusion Models,,15,7632,coco
CelebAHQ,CVPR,10.1109/CVPR52688.2022.01042,10.48550/arXiv.1710.10196,https://openreview.net/pdf?id=Hk99zCeAb,High-Resolution Image Synthesis with Latent Diffusion Models,,15,7632,celebahq
FFHQ,CVPR,10.1109/CVPR52688.2022.01042,10.48550/arXiv.1812.04948,https://openaccess.thecvf.com/content_CVPR_2019/papers/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.pdf,High-Resolution Image Synthesis with Latent Diffusion Models,,15,7632,ffhq
LSUN-Churces,CVPR,10.1109/CVPR52688.2022.01042,10.48550/arXiv.1506.03365,https://www.researchgate.net/publication/278048515_LSUN_Construction_of_a_Large-scale_Image_Dataset_using_Deep_Learning_with_Humans_in_the_Loop,High-Resolution Image Synthesis with Latent Diffusion Models,,15,7632,lsun-churces
LSUN-Bedrooms,CVPR,10.1109/CVPR52688.2022.01042,10.48550/arXiv.1506.03365,https://www.researchgate.net/publication/278048515_LSUN_Construction_of_a_Large-scale_Image_Dataset_using_Deep_Learning_with_Humans_in_the_Loop,High-Resolution Image Synthesis with Latent Diffusion Models,,15,7632,lsun-bedrooms
LAION-400M,CVPR,10.1109/CVPR52688.2022.01042,10.48550/arXiv.2111.02114,https://arxiv.org/pdf/2111.02114v1,High-Resolution Image Synthesis with Latent Diffusion Models,,15,7632,laion-400m
OpenImages,CVPR,10.1109/CVPR52688.2022.01042,10.48550/arXiv.1811.00982,https://arxiv.org/pdf/1811.00982,High-Resolution Image Synthesis with Latent Diffusion Models,"they don't mention what version they use in the paper, but the citation is for V4",15,7632,openimages
Places,CVPR,10.1109/CVPR52688.2022.01042,10.1109/TPAMI.2017.2723009,http://places2.csail.mit.edu/PAMI_places.pdf,High-Resolution Image Synthesis with Latent Diffusion Models,,15,7632,places
COCO,CVPR,10.1109/CVPR52688.2022.01167,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,A ConvNet for the 2020s,,5,4415,coco
Imagenet-1K,CVPR,10.1109/CVPR52688.2022.01167,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,A ConvNet for the 2020s,,5,4415,imagenet-1k
ADE20K,CVPR,10.1109/CVPR52688.2022.01167,10.1007/s11263-018-1140-0,https://www.researchgate.net/publication/306357649_Semantic_Understanding_of_Scenes_Through_the_ADE20K_Dataset,A ConvNet for the 2020s,,5,4415,ade20k
ImageNet,CVPR,10.1109/CVPR52688.2022.01167,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,A ConvNet for the 2020s,"they reference as 22K, but both are accepted",5,4415,imagenet
ImageNet-1K,CVPR,10.1109/CVPR52688.2022.01553,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,Masked Autoencoders Are Scalable Vision Learners,,5,4505,imagenet-1k
COCO,CVPR,10.1109/CVPR52688.2022.01553,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Masked Autoencoders Are Scalable Vision Learners,,5,4505,coco
ADE20K,CVPR,10.1109/CVPR52688.2022.01553,10.1007/s11263-018-1140-0,https://www.researchgate.net/publication/306357649_Semantic_Understanding_of_Scenes_Through_the_ADE20K_Dataset,Masked Autoencoders Are Scalable Vision Learners,,5,4505,ade20k
iNaturalist,CVPR,10.1109/CVPR52688.2022.01553,10.1109/CVPR.2018.00914,https://ieeexplore.ieee.org/document/8579012,Masked Autoencoders Are Scalable Vision Learners,2017-2019,5,4505,inaturalist
Places205,CVPR,10.1109/CVPR52688.2022.01553,N/A,http://places.csail.mit.edu/places_NIPS14.pdf,Masked Autoencoders Are Scalable Vision Learners,"cited the paper of Places, not the subset",5,4505,places205
Places365,CVPR,10.1109/CVPR52688.2022.01553,10.48550/arXiv.1909.02410,https://arxiv.org/pdf/1909.02410v3,Masked Autoencoders Are Scalable Vision Learners,"cited the paper of Places, not the subset",5,4505,places365
DreamBooth,CVPR,10.1109/CVPR52729.2023.00037,10.1109/CVPR52729.2023.02155,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=10204880,Magic3D: High-Resolution Text-to-3D Content Creation,,2,494,dreambooth
DreamFusion,CVPR,10.1109/CVPR52729.2023.00037,10.48550/arXiv.2209.14988,https://arxiv.org/pdf/2209.14988,Magic3D: High-Resolution Text-to-3D Content Creation,,2,494,dreamfusion
Wild-TI2I,CVPR,10.1109/CVPR52729.2023.00191,10.1109/CVPR52729.2023.00191,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=10204217,Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation,introduced by the paper,2,305,wild-ti2i
ImageNet-R-TI2I,CVPR,10.1109/CVPR52729.2023.00191,10.1109/CVPR52729.2023.00191,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=10204217,Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation,introduced by the paper,2,305,imagenet-r-ti2i
ImageNet-R,CVPR,10.1109/CVPR52729.2023.00191,10.1109/ICCV48922.2021.00823,https://arxiv.org/pdf/2006.16241v3,Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation,used to generate their dataset,2,305,imagenet-r
CustomConcept101,CVPR,10.1109/CVPR52729.2023.00192,10.1109/CVPR52729.2023.00192,https://arxiv.org/abs/2212.04488,Multi-Concept Customization of Text-to-Image Diffusion,"benchmark introduced by the paper, they introduce the name of the dataset only in the arxiv version in the appendix",2,341,customconcept101
LAION-400M,CVPR,10.1109/CVPR52729.2023.00192,10.48550/arXiv.2111.02114,https://arxiv.org/pdf/2111.02114v1,Multi-Concept Customization of Text-to-Image Diffusion,for regularization,2,341,laion-400m
COCO,CVPR,10.1109/CVPR52729.2023.00192,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Multi-Concept Customization of Text-to-Image Diffusion,"for FID-based regularization, they cited https://dl.acm.org/doi/pdf/10.5555/3295222.3295408",2,341,coco
LAION-400M,CVPR,10.1109/CVPR52729.2023.00276,10.48550/arXiv.2111.02114,https://arxiv.org/pdf/2111.02114v1,Reproducible scaling laws for contrastive language-image learning,they use also a 80M subset and they would refer to it as LAION-80M,2,299,laion-400m
LAION-5B,CVPR,10.1109/CVPR52729.2023.00276,10.48550/arXiv.2210.08402,https://arxiv.org/pdf/2210.08402,Reproducible scaling laws for contrastive language-image learning,they use a English text subset and would refer to it as LAION-2B,2,299,laion-5b
ImageNet,CVPR,10.1109/CVPR52729.2023.00276,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Reproducible scaling laws for contrastive language-image learning,,2,299,imagenet
ImageNet,CVPR,10.1109/CVPR52729.2023.00276,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Reproducible scaling laws for contrastive language-image learning,they filter the ones with few examples (?! what does few examples mean) and call it ImageNet-12K,2,299,imagenet
ImageNet-V2,CVPR,10.1109/CVPR52729.2023.00276,10.48550/arXiv.1902.10811,https://arxiv.org/pdf/1902.10811,Reproducible scaling laws for contrastive language-image learning,,2,299,imagenet-v2
ImageNet-A,CVPR,10.1109/CVPR52729.2023.00276,10.48550/arXiv.1907.07174,https://arxiv.org/pdf/1907.07174,Reproducible scaling laws for contrastive language-image learning,,2,299,imagenet-a
ImageNet-R,CVPR,10.1109/CVPR52729.2023.00276,10.1109/ICCV48922.2021.00823,https://arxiv.org/pdf/2006.16241v3,Reproducible scaling laws for contrastive language-image learning,,2,299,imagenet-r
ImageNet-Sketch,CVPR,10.1109/CVPR52729.2023.00276,10.48550/arXiv.1905.13549,https://arxiv.org/pdf/1905.13549v2,Reproducible scaling laws for contrastive language-image learning,,2,299,imagenet-sketch
ObjectNet,CVPR,10.1109/CVPR52729.2023.00276,10.5555/3454287.3455135,https://dl.acm.org/doi/pdf/10.5555/3454287.3455135,Reproducible scaling laws for contrastive language-image learning,,2,299,objectnet
COCO,CVPR,10.1109/CVPR52729.2023.00276,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Reproducible scaling laws for contrastive language-image learning,,2,299,coco
Flickr30k,CVPR,10.1109/CVPR52729.2023.00276,10.1162/tacl_a_00166,https://aclanthology.org/Q14-1006.pdf,Reproducible scaling laws for contrastive language-image learning,,2,299,flickr30k
CIFAR-100,CVPR,10.1109/CVPR52729.2023.00276,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Reproducible scaling laws for contrastive language-image learning,,2,299,cifar-100
VTAB,CVPR,10.1109/CVPR52729.2023.00276,10.48550/arXiv.1910.04867,https://arxiv.org/pdf/1910.04867,Reproducible scaling laws for contrastive language-image learning,,2,299,vtab
StanfordCars,CVPR,10.1109/CVPR52729.2023.00276,10.1109/ICCVW.2013.77,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=6755945,Reproducible scaling laws for contrastive language-image learning,"the paper of the datasets introduces also a second dataset, none of them are called cars",2,299,stanfordcars
DTD,CVPR,10.1109/CVPR52729.2023.00276,10.1109/CVPR.2014.461,https://openaccess.thecvf.com/content_cvpr_2014/papers/Cimpoi_Describing_Textures_in_2014_CVPR_paper.pdf,Reproducible scaling laws for contrastive language-image learning,,2,299,dtd
EuroSAT,CVPR,10.1109/CVPR52729.2023.00276,10.1109/JSTARS.2019.2918242,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/8736785,Reproducible scaling laws for contrastive language-image learning,,2,299,eurosat
GTSRB,CVPR,10.1109/CVPR52729.2023.00276,10.1109/IJCNN.2011.6033395,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=6033395,Reproducible scaling laws for contrastive language-image learning,,2,299,gtsrb
MNIST,CVPR,10.1109/CVPR52729.2023.00276,10.1109/5.726791,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=726791,Reproducible scaling laws for contrastive language-image learning,broken link,2,299,mnist
RESISC45,CVPR,10.1109/CVPR52729.2023.00276,10.1109/JPROC.2017.2675998,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=7891544,Reproducible scaling laws for contrastive language-image learning,"broken link, good citation",2,299,resisc45
SUN397,CVPR,10.1109/CVPR52729.2023.00276,10.1109/CVPR.2010.5539970,https://faculty.cc.gatech.edu/~hays/papers/sun.pdf,Reproducible scaling laws for contrastive language-image learning,"broken link, good citation",2,299,sun397
SVHN,CVPR,10.1109/CVPR52729.2023.00276,N/A,http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf,Reproducible scaling laws for contrastive language-image learning,"broken link, good citation",2,299,svhn
MSRS,CVPR,10.1109/CVPR52729.2023.00572,10.1016/j.inffus.2022.03.007,https://www.sciencedirect.com/science/article/abs/pii/S156625352200032X,CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion,,2,383,msrs
Road Scene,CVPR,10.1109/CVPR52729.2023.00572,10.1609/aaai.v34i07.6936,https://www.researchgate.net/publication/342538023_FusionDN_A_Unified_Densely_Connected_Network_for_Image_Fusion,CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion,,2,383,road scene
TNO,CVPR,10.1109/CVPR52729.2023.00572,10.1117/1.OE.51.1.010901,https://www.researchgate.net/publication/224883433_Progress_in_color_night_vision,CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion,,2,383,tno
Harvard Medical,CVPR,10.1109/CVPR52729.2023.00572,N/A,https://hms.harvard.edu/taxonomy/term/86,CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion,broken link,2,383,harvard medical
M3FD,CVPR,10.1109/CVPR52729.2023.00572,10.48550/arXiv.2203.16220,https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Target-Aware_Dual_Adversarial_Learning_and_a_Multi-Scenario_Multi-Modality_Benchmark_To_CVPR_2022_paper.pdf,CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion,,2,383,m3fd
Imagic,CVPR,10.1109/CVPR52729.2023.00582,10.1109/CVPR52729.2023.00582,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=10203581,Imagic: Text-Based Real Image Editing with Diffusion Models,introduces a new benchmark,2,441,imagic
CIFAR-10,CVPR,10.1109/CVPR52729.2023.00596,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,SCConv: Spatial and Channel Reconstruction Convolution for Feature Redundancy,,2,379,cifar-10
CIFAR-100,CVPR,10.1109/CVPR52729.2023.00596,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,SCConv: Spatial and Channel Reconstruction Convolution for Feature Redundancy,,2,379,cifar-100
ImageNet-1K,CVPR,10.1109/CVPR52729.2023.00596,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,SCConv: Spatial and Channel Reconstruction Convolution for Feature Redundancy,"the paper cites a NeurIPS paper, but it should be introduced by the large ImageNet paper https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf",2,379,imagenet-1k
PASCAL VOC 2007,CVPR,10.1109/CVPR52729.2023.00596,10.1007/s11263-009-0275-4,https://link.springer.com/article/10.1007/s11263-009-0275-4#preview,SCConv: Spatial and Channel Reconstruction Convolution for Feature Redundancy,,2,379,pascal voc 2007
PASCAL VOC 2012,CVPR,10.1109/CVPR52729.2023.00596,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,SCConv: Spatial and Channel Reconstruction Convolution for Feature Redundancy,,,,
COCO,CVPR,10.1109/CVPR52729.2023.00596,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,SCConv: Spatial and Channel Reconstruction Convolution for Feature Redundancy,,2,379,coco
MOT16,CVPR,10.1109/CVPR52729.2023.00934,10.48550/arXiv.1603.00831,https://arxiv.org/pdf/1603.00831v2,Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking,,2,354,mot16
MOT20,CVPR,10.1109/CVPR52729.2023.00934,10.48550/arXiv.2003.09003,https://arxiv.org/pdf/2003.09003,Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking,,2,354,mot20
KITTI,CVPR,10.1109/CVPR52729.2023.00934,10.1109/CVPR.2012.6248074,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6248074,Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking,referenced another pape,2,354,kitti
DanceTrack,CVPR,10.1109/CVPR52729.2023.00934,10.48550/arXiv.2111.14690,https://arxiv.org/pdf/2111.14690,Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking,,2,354,dancetrack
CroHD,CVPR,10.1109/CVPR52729.2023.00934,10.48550/arXiv.2103.13516,https://arxiv.org/pdf/2103.13516v1,Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking,,2,354,crohd
ImageNet-1k,CVPR,10.1109/CVPR52729.2023.00995,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,BiFormer: Vision Transformer with Bi-Level Routing Attention,,2,656,imagenet-1k
COCO,CVPR,10.1109/CVPR52729.2023.00995,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,BiFormer: Vision Transformer with Bi-Level Routing Attention,2017,2,656,coco
ADE20K,CVPR,10.1109/CVPR52729.2023.00995,10.1007/s11263-018-1140-0,https://www.researchgate.net/publication/306357649_Semantic_Understanding_of_Scenes_Through_the_ADE20K_Dataset,BiFormer: Vision Transformer with Bi-Level Routing Attention,,2,656,ade20k
ImageNet-1K,CVPR,10.1109/CVPR52729.2023.01157,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,"Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks",,2,1156,imagenet-1k
COCO,CVPR,10.1109/CVPR52729.2023.01157,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,"Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks",,2,1156,coco
ImageNet-1K,CVPR,10.1109/CVPR52729.2023.01385,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions,pretraining,2,513,imagenet-1k
ImageNet,CVPR,10.1109/CVPR52729.2023.01385,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions,pretraining,2,513,imagenet
Laion-400M,CVPR,10.1109/CVPR52729.2023.01385,10.48550/arXiv.2111.02114,https://arxiv.org/pdf/2111.02114v1,InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions,these 3 were used to create a joint dataset via M3I pretraining,2,513,laion-400m
YFCC-15M,CVPR,10.1109/CVPR52729.2023.01385,10.48550/arXiv.1503.01817,https://arxiv.org/pdf/1503.01817,InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions,these 3 were used to create a joint dataset via M3I pretraining,2,513,yfcc-15m
CC12M,CVPR,10.1109/CVPR52729.2023.01385,10.48550/arXiv.2102.08981,https://arxiv.org/pdf/2102.08981v2,InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions,these 3 were used to create a joint dataset via M3I pretraining,2,513,cc12m
ADE20K,CVPR,10.1109/CVPR52729.2023.01385,10.1007/s11263-018-1140-0,https://www.researchgate.net/publication/306357649_Semantic_Understanding_of_Scenes_Through_the_ADE20K_Dataset,InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions,semantic segm,2,513,ade20k
Objects365,CVPR,10.1109/CVPR52729.2023.01385,10.1109/ICCV.2019.00852,https://openaccess.thecvf.com/content_ICCV_2019/papers/Shao_Objects365_A_Large-Scale_High-Quality_Dataset_for_Object_Detection_ICCV_2019_paper.pdf,InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions,semantic segm,2,513,objects365
ImageNet-1K,CVPR,10.1109/CVPR52729.2023.01386,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention,,2,301,imagenet-1k
COCO,CVPR,10.1109/CVPR52729.2023.01386,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention,,2,301,coco
CIFAR-10,CVPR,10.1109/CVPR52729.2023.01386,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention,,2,301,cifar-10
CIFAR-100,CVPR,10.1109/CVPR52729.2023.01386,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention,,2,301,cifar-100
Flowers,CVPR,10.1109/CVPR52729.2023.01386,10.1109/CVPR.2006.42,https://www.robots.ox.ac.uk/~men/papers/nilsback_cvpr06.pdf,EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention,,2,301,flowers
StanfordCars,CVPR,10.1109/CVPR52729.2023.01386,10.1109/ICCVW.2013.77,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=6755945,EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention,,2,301,stanfordcars
Oxford-IIIT Pets,CVPR,10.1109/CVPR52729.2023.01386,10.1109/CVPR.2012.6248092,https://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf,EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention,,2,301,oxford-iiit pets
ImageNet-Real,CVPR,10.1109/CVPR52729.2023.01386,10.48550/arXiv.2006.07159,https://arxiv.org/abs/2006.07159,EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention,,2,301,imagenet-real
AudioSet,CVPR,10.1109/CVPR52729.2023.01457,10.1109/ICASSP.2017.7952261,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=7952261,ImageBind One Embedding Space to Bind Them All,audio-only,2,398,audioset
ESC,CVPR,10.1109/CVPR52729.2023.01457,10.1145/2733373.2806390,https://www.karolpiczak.com/papers/Piczak2015-ESC-Dataset.pdf,ImageBind One Embedding Space to Bind Them All,,2,398,esc
Clotho,CVPR,10.1109/CVPR52729.2023.01457,10.1109/ICASSP40776.2020.9052990,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=9052990,ImageBind One Embedding Space to Bind Them All,random reference,2,398,clotho
AudioCaps,CVPR,10.1109/CVPR52729.2023.01457,10.18653/v1/N19-1011,https://aclanthology.org/N19-1011.pdf,ImageBind One Embedding Space to Bind Them All,,2,398,audiocaps
VGGSound,CVPR,10.1109/CVPR52729.2023.01457,10.48550/arXiv.2004.14368,https://arxiv.org/pdf/2004.14368,ImageBind One Embedding Space to Bind Them All,,2,398,vggsound
SUN RGB-D,CVPR,10.1109/CVPR52729.2023.01457,10.1109/CVPR.2015.7298655,https://ieeexplore.ieee.org/document/7298655,ImageBind One Embedding Space to Bind Them All,,2,398,sun rgb-d
NYUDv2,CVPR,10.1109/CVPR52729.2023.01457,10.1007/978-3-642-33715-4_54,https://link.springer.com/chapter/10.1007/978-3-642-33715-4_54,ImageBind One Embedding Space to Bind Them All,,2,398,nyudv2
LLVIP,CVPR,10.1109/CVPR52729.2023.01457,10.48550/arXiv.2108.10831,https://arxiv.org/pdf/2108.10831,ImageBind One Embedding Space to Bind Them All,,2,398,llvip
Ego4D,CVPR,10.1109/CVPR52729.2023.01457,10.48550/arXiv.2110.07058,https://arxiv.org/pdf/2110.07058,ImageBind One Embedding Space to Bind Them All,,2,398,ego4d
LAION-400M,CVPR,10.1109/CVPR52729.2023.01457,10.48550/arXiv.2111.02114,https://arxiv.org/pdf/2111.02114v1,ImageBind One Embedding Space to Bind Them All,openClip trained on,2,398,laion-400m
ImageNet-1K,CVPR,10.1109/CVPR52729.2023.01548,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders,pretraining,2,615,imagenet-1k
ImageNet,CVPR,10.1109/CVPR52729.2023.01548,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders,fine-tuning,2,615,imagenet
ADE20K,CVPR,10.1109/CVPR52729.2023.01548,10.1007/s11263-018-1140-0,https://www.researchgate.net/publication/306357649_Semantic_Understanding_of_Scenes_Through_the_ADE20K_Dataset,ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders,semantic segmentation,2,615,ade20k
COCO,CVPR,10.1109/CVPR52729.2023.01548,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders,instance segmentation,2,615,coco
nuScenes,CVPR,10.1109/CVPR52729.2023.01712,10.1109/CVPR42600.2020.01164,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=9156412,Planning-oriented Autonomous Driving,,2,341,nuscenes
InstructPix2Pix,CVPR,10.1109/CVPR52729.2023.01764,10.1109/CVPR52729.2023.01764,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=10204579,InstructPix2Pix: Learning to Follow Image Editing Instructions,introduced by the paper,2,704,instructpix2pix
LAION-Aesthetic,CVPR,10.1109/CVPR52729.2023.01764,https://github.com/LAION-AI/laion-datasets/blob/main/laion-aesthetic.md,https://github.com/LAION-AI/laion-datasets/blob/main/laion-aesthetic.md,InstructPix2Pix: Learning to Follow Image Editing Instructions,v2 6.5+,2,704,laion-aesthetic
ImageNet,CVPR,10.1109/CVPR52729.2023.01832,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,MaPLe: Multi-modal Prompt Learning,,2,396,imagenet
Caltech101,CVPR,10.1109/CVPR52729.2023.01832,10.22002/D1.20086,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/1384978,MaPLe: Multi-modal Prompt Learning,,2,396,caltech101
Oxford-IIIT Pets,CVPR,10.1109/CVPR52729.2023.01832,10.1109/CVPR.2012.6248092,https://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf,MaPLe: Multi-modal Prompt Learning,reffered as OxforPets,2,396,oxford-iiit pets
StanfordCars,CVPR,10.1109/CVPR52729.2023.01832,10.1109/ICCVW.2013.77,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=6755945,MaPLe: Multi-modal Prompt Learning,,2,396,stanfordcars
Oxford Flowers 102,CVPR,10.1109/CVPR52729.2023.01832,10.1109/ICVGIP.2008.47,https://ieeexplore.ieee.org/document/4756141,MaPLe: Multi-modal Prompt Learning,,2,396,oxford flowers 102
Food-101,CVPR,10.1109/CVPR52729.2023.01832,10.1007/978-3-319-10599-4_29,https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/static/bossard_eccv14_food-101.pdf,MaPLe: Multi-modal Prompt Learning,,2,396,food-101
FGVC Aircraft,CVPR,10.1109/CVPR52729.2023.01832,10.48550/arXiv.1306.5151,https://arxiv.org/abs/1306.5151,MaPLe: Multi-modal Prompt Learning,,2,396,fgvc aircraft
SUN397,CVPR,10.1109/CVPR52729.2023.01832,10.1109/CVPR.2010.5539970,https://faculty.cc.gatech.edu/~hays/papers/sun.pdf,MaPLe: Multi-modal Prompt Learning,,2,396,sun397
UCF101,CVPR,10.1109/CVPR52729.2023.01832,10.48550/arXiv.1212.0402,https://arxiv.org/pdf/1212.0402,MaPLe: Multi-modal Prompt Learning,,2,396,ucf101
DTD,CVPR,10.1109/CVPR52729.2023.01832,10.1109/CVPR.2014.461,https://openaccess.thecvf.com/content_cvpr_2014/papers/Cimpoi_Describing_Textures_in_2014_CVPR_paper.pdf,MaPLe: Multi-modal Prompt Learning,,2,396,dtd
EuroSAT,CVPR,10.1109/CVPR52729.2023.01832,10.1109/JSTARS.2019.2918242,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/8736785,MaPLe: Multi-modal Prompt Learning,,2,396,eurosat
ImageNet-V2,CVPR,10.1109/CVPR52729.2023.01832,10.48550/arXiv.1902.10811,https://arxiv.org/pdf/1902.10811,MaPLe: Multi-modal Prompt Learning,,2,396,imagenet-v2
ImageNet-Sketch,CVPR,10.1109/CVPR52729.2023.01832,10.48550/arXiv.1905.13549,https://arxiv.org/pdf/1905.13549v2,MaPLe: Multi-modal Prompt Learning,,2,396,imagenet-sketch
ImageNet-A,CVPR,10.1109/CVPR52729.2023.01832,10.48550/arXiv.1907.07174,https://arxiv.org/pdf/1907.07174,MaPLe: Multi-modal Prompt Learning,,2,396,imagenet-a
ImageNet-R,CVPR,10.1109/CVPR52729.2023.01832,10.1109/ICCV48922.2021.00823,https://arxiv.org/pdf/2006.16241v3,MaPLe: Multi-modal Prompt Learning,,2,396,imagenet-r
CC12M,CVPR,10.1109/CVPR52729.2023.01855,10.48550/arXiv.2102.08981,https://arxiv.org/pdf/2102.08981v2,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,used dataset without captions; pretraining,2,321,cc12m
CC3M,CVPR,10.1109/CVPR52729.2023.01855,N/A,https://github.com/rom1504/img2dataset/blob/main/dataset_examples/cc3m.md,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,used dataset without captions; pretraining,2,321,cc3m
COCO,CVPR,10.1109/CVPR52729.2023.01855,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,pretraining,2,321,coco
ADE20K,CVPR,10.1109/CVPR52729.2023.01855,10.1007/s11263-018-1140-0,https://www.researchgate.net/publication/306357649_Semantic_Understanding_of_Scenes_Through_the_ADE20K_Dataset,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,"pretraining, and semantic segmentation",2,321,ade20k
ImageNet,CVPR,10.1109/CVPR52729.2023.01855,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,pretraining,2,321,imagenet
Objects365,CVPR,10.1109/CVPR52729.2023.01855,10.1109/ICCV.2019.00852,https://openaccess.thecvf.com/content_ICCV_2019/papers/Shao_Objects365_A_Large-Scale_High-Quality_Dataset_for_Object_Detection_ICCV_2019_paper.pdf,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,pretraining,2,321,objects365
ImageNet-V2,CVPR,10.1109/CVPR52729.2023.01855,10.48550/arXiv.1902.10811,https://arxiv.org/pdf/1902.10811,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,image classification,2,321,imagenet-v2
ImageNet-Real,CVPR,10.1109/CVPR52729.2023.01855,10.48550/arXiv.2006.07159,https://arxiv.org/abs/2006.07159,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,image classification,2,321,imagenet-real
ImageNet-A,CVPR,10.1109/CVPR52729.2023.01855,10.48550/arXiv.1907.07174,https://arxiv.org/pdf/1907.07174,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,"image classification, (adversarial)",2,321,imagenet-a
ImageNet-R,CVPR,10.1109/CVPR52729.2023.01855,10.1109/ICCV48922.2021.00823,https://arxiv.org/pdf/2006.16241v3,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,"image classification, (rendition)",2,321,imagenet-r
ImageNet-Sketch,CVPR,10.1109/CVPR52729.2023.01855,10.48550/arXiv.1905.13549,https://arxiv.org/pdf/1905.13549v2,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,image classification,2,321,imagenet-sketch
ImageNet-1K,CVPR,10.1109/CVPR52729.2023.01855,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,fine-tuned,2,321,imagenet-1k
Kinetics-400,CVPR,10.1109/CVPR52729.2023.01855,10.48550/arXiv.1705.06950,https://arxiv.org/pdf/1705.06950v1,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,"they merge them, creating K-722",2,321,kinetics-400
Kinetics-600,CVPR,10.1109/CVPR52729.2023.01855,10.57702/4hj86f6e,https://arxiv.org/pdf/1808.01340v1,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,"they merge them, creating K-722",2,321,kinetics-600
Kinetics 700,CVPR,10.1109/CVPR52729.2023.01855,10.48550/arXiv.1907.06987,https://arxiv.org/pdf/1907.06987v2,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,"they merge them, creating K-722",2,321,kinetics 700
LVIS,CVPR,10.1109/CVPR52729.2023.01855,10.1109/CVPR.2019.00550,https://ieeexplore.ieee.org/document/8954457,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,V1.0,2,321,lvis
COCO-Stuff,CVPR,10.1109/CVPR52729.2023.01855,10.48550/arXiv.1612.03716,https://openaccess.thecvf.com/content_cvpr_2018/papers/Caesar_COCO-Stuff_Thing_and_CVPR_2018_paper.pdf,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,,2,321,coco-stuff
CIFAR-10,CVPR,10.1109/CVPR52729.2023.01855,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,,2,321,cifar-10
CIFAR-100,CVPR,10.1109/CVPR52729.2023.01855,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,,2,321,cifar-100
ObjectNet,CVPR,10.1109/CVPR52729.2023.01855,10.5555/3454287.3455135,https://dl.acm.org/doi/pdf/10.5555/3454287.3455135,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,,2,321,objectnet
UCF101,CVPR,10.1109/CVPR52729.2023.01855,10.48550/arXiv.1212.0402,https://arxiv.org/pdf/1212.0402,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,,2,321,ucf101
Cityscapes,CVPR,10.1109/CVPR52729.2023.01871,10.1109/CVPR.2016.350,https://ieeexplore.ieee.org/document/7780719,PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers,,2,339,cityscapes
CamVid,CVPR,10.1109/CVPR52729.2023.01871,10.1016/j.patrec.2008.04.005,https://www.sciencedirect.com/science/article/abs/pii/S0167865508001220,PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers,,2,339,camvid
PASCAL Context,CVPR,10.1109/CVPR52729.2023.01871,10.1109/CVPR.2014.119,https://cs.stanford.edu/~roozbeh/pascal-context/mottaghi_et_al_cvpr14.pdf,PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers,,2,339,pascal context
ImageNet,CVPR,10.1109/CVPR52729.2023.01871,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers,they seem to be using Imagenet 2012,2,339,imagenet
DIV2K,CVPR,10.1109/CVPR52729.2023.02142,10.1109/CVPRW.2017.150,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/8014884,Activating More Pixels in Image Super-Resolution Transformer,referenced to the wrong paper,2,520,div2k
Flickr2K,CVPR,10.1109/CVPR52729.2023.02142,N/A,https://www.kaggle.com/datasets/daehoyang/flickr2k,Activating More Pixels in Image Super-Resolution Transformer,,2,520,flickr2k
ImageNet,CVPR,10.1109/CVPR52729.2023.02142,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Activating More Pixels in Image Super-Resolution Transformer,,2,520,imagenet
Set5,CVPR,10.1109/CVPR52729.2023.02142,10.5244/C.26.135,https://www.researchgate.net/publication/260351242_Low-Complexity_Single_Image_Super-Resolution_Based_on_Nonnegative_Neighbor_Embedding,Activating More Pixels in Image Super-Resolution Transformer,,2,520,set5
Set14,CVPR,10.1109/CVPR52729.2023.02142,10.1007/978-3-642-27413-8_47,https://link.springer.com/chapter/10.1007/978-3-642-27413-8_47,Activating More Pixels in Image Super-Resolution Transformer,,2,520,set14
BSD100,CVPR,10.1109/CVPR52729.2023.02142,10.1109/ICCV.2001.937655,https://ieeexplore.ieee.org/document/937655,Activating More Pixels in Image Super-Resolution Transformer,referenced the wrong paper,2,520,bsd100
Urban100,CVPR,10.1109/CVPR52729.2023.02142,10.1109/CVPR.2015.7299156,https://paperswithcode.com/paper/single-image-super-resolution-from,Activating More Pixels in Image Super-Resolution Transformer,,2,520,urban100
Manga109,CVPR,10.1109/CVPR52729.2023.02142,10.48550/arXiv.1510.04389,https://arxiv.org/pdf/1510.04389v1,Activating More Pixels in Image Super-Resolution Transformer,,2,520,manga109
DreamBooth,CVPR,10.1109/CVPR52729.2023.02155,10.1109/CVPR52729.2023.02155,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=10204880,DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation,introduced by the paper,2,1071,dreambooth
RDS,CVPR,10.1109/CVPR52729.2023.02161,10.1109/CVPR52729.2023.02161,https://arxiv.org/pdf/2304.08818,Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models,introduced by the paper,2,381,rds
WebVID-10M,CVPR,10.1109/CVPR52729.2023.02161,10.48550/arXiv.2104.00650,https://arxiv.org/pdf/2104.00650v2,Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models,,2,381,webvid-10m
UCF101,CVPR,10.1109/CVPR52729.2023.02161,10.48550/arXiv.1212.0402,https://arxiv.org/pdf/1212.0402,Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models,,2,381,ucf101
MSR-VTT,CVPR,10.1109/CVPR52729.2023.02161,10.1109/CVPR.2016.571,https://openaccess.thecvf.com/content_cvpr_2016/papers/Xu_MSR-VTT_A_Large_CVPR_2016_paper.pdf,Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models,,2,381,msr-vtt
MountainBike,CVPR,10.1109/CVPR52729.2023.02161,10.48550/arXiv.2206.03429,https://arxiv.org/pdf/2206.03429,Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models,,2,381,mountainbike
Objects365,CVPR,10.1109/CVPR52733.2024.01605,10.1109/ICCV.2019.00852,https://openaccess.thecvf.com/content_ICCV_2019/papers/Shao_Objects365_A_Large-Scale_High-Quality_Dataset_for_Object_Detection_ICCV_2019_paper.pdf,DETRs Beat YOLOs on Real-time Object Detection,pretraining,2,588,objects365
COCO,CVPR,10.1109/CVPR52733.2024.01605,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,DETRs Beat YOLOs on Real-time Object Detection,,2,588,coco
ImageNet 2012,CVPR,10.1109/CVPRW50498.2020.00203,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,CSPNet: A new backbone that can enhance learning capability of CNN,,5,3147,imagenet 2012
COCO,CVPR,10.1109/CVPRW50498.2020.00203,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,CSPNet: A new backbone that can enhance learning capability of CNN,,5,3147,coco
CIFAR-10,CVPR,10.1109/CVPRW50498.2020.00359,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Randaugment: Practical automated data augmentation with a reduced search space,,5,1983,cifar-10
CIFAR-100,CVPR,10.1109/CVPRW50498.2020.00359,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Randaugment: Practical automated data augmentation with a reduced search space,,5,1983,cifar-100
SVHN,CVPR,10.1109/CVPRW50498.2020.00359,N/A,http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf,Randaugment: Practical automated data augmentation with a reduced search space,,5,1983,svhn
ImageNet,CVPR,10.1109/CVPRW50498.2020.00359,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Randaugment: Practical automated data augmentation with a reduced search space,,5,1983,imagenet
COCO,CVPR,10.1109/CVPRW50498.2020.00359,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Randaugment: Practical automated data augmentation with a reduced search space,,5,1983,coco
CIFAR-10,NeurIPS,10.48550/arXiv.1206.2944,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Practical Bayesian optimization of machine learning algorithms,,15,"5,701",cifar-10
MNIST,NeurIPS,10.48550/arXiv.1206.2944,10.1109/5.726791,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=726791,Practical Bayesian optimization of machine learning algorithms,not referenced,15,"5,701",mnist
N/A WIki articles,NeurIPS,10.48550/arXiv.1206.2944,10.48550/arXiv.1206.2944,https://arxiv.org/pdf/1206.2944,Practical Bayesian optimization of machine learning algorithms,"""random set"" (of articles)",15,"5,701",n/a wiki articles
COCO,NeurIPS,10.48550/arXiv.1206.2944,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Practical Bayesian optimization of machine learning algorithms,"used, not referenced. this paper trains a bunch for experiments but they never mention what they train on",15,"5,701",coco
Google News,NeurIPS,10.48550/arXiv.1310.4546,N/A,N/A,Distributed representations of words and phrases and their compositionality,internal dataset,15,"25,301",google news
Word Analogy Task,NeurIPS,10.48550/arXiv.1310.4546,10.48550/arXiv.1301.3781,https://arxiv.org/pdf/1301.3781,Distributed representations of words and phrases and their compositionality,,15,"25,301",word analogy task
ImageNet 2012,NeurIPS,10.48550/arXiv.1406.2199,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,Two-stream convolutional networks for action recognition in videos,"pretraining, cites the ILSVRC 2010, but they mention using ILSVRC 2012",15,"6,287",imagenet 2012
UCF101,NeurIPS,10.48550/arXiv.1406.2199,10.48550/arXiv.1212.0402,https://arxiv.org/pdf/1212.0402,Two-stream convolutional networks for action recognition in videos,,15,"6,287",ucf101
HMDB51,NeurIPS,10.48550/arXiv.1406.2199,10.1109/ICCV.2011.6126543,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=6126543,Two-stream convolutional networks for action recognition in videos,,15,"6,287",hmdb51
MNIST,NeurIPS,10.48550/arXiv.1406.2661,10.1109/5.726791,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=726791,Generative adversarial nets,,15,"48,297",mnist
TFD,NeurIPS,10.48550/arXiv.1406.2661,N/A,N/A,Generative adversarial nets,,15,"48,297",tfd
CIFAR-10,NeurIPS,10.48550/arXiv.1406.2661,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Generative adversarial nets,,15,"48,297",cifar-10
WMT14,NeurIPS,10.48550/arXiv.1409.3215,10.3115/v1/W14-3302,https://aclanthology.org/W14-3302.pdf,Sequence to sequence learning with neural networks,broken reference to actual subset of wmt14,15,"15,234",wmt14
ImageNet 2012,NeurIPS,10.48550/arXiv.1411.1792,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,How transferable are features in deep neural networks?,"used to create multiple datasets, by doing multiple A/B splits",15,"6,227",imagenet 2012
PASCAL VOC 2007,NeurIPS,10.48550/arXiv.1506.01497,10.1007/s11263-009-0275-4,https://link.springer.com/article/10.1007/s11263-009-0275-4#preview,Faster R-CNN: Towards real-time object detection with region proposal networks,,15,"33,127",pascal voc 2007
PASCAL VOC 2012,NeurIPS,10.48550/arXiv.1506.01497,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,Faster R-CNN: Towards real-time object detection with region proposal networks,,15,"33,127",pascal voc 2012
COCO,NeurIPS,10.48550/arXiv.1506.01497,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Faster R-CNN: Towards real-time object detection with region proposal networks,,15,"33,127",coco
ImageNet 2012,NeurIPS,10.48550/arXiv.1506.01497,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,Faster R-CNN: Towards real-time object detection with region proposal networks,,15,"33,127",imagenet 2012
MNIST,NeurIPS,10.48550/arXiv.1506.02025,10.1109/5.726791,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=726791,Spatial transformer networks,no reference,15,"6,099",mnist
CUB-200-2011 Birds,NeurIPS,10.48550/arXiv.1506.02025,N/A,https://authors.library.caltech.edu/records/cvm3y-5hh21,Spatial transformer networks,,15,"6,099",cub-200-2011 birds
SVHN,NeurIPS,10.48550/arXiv.1506.02025,N/A,http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf,Spatial transformer networks,,15,"6,099",svhn
3D MNIST,NeurIPS,10.48550/arXiv.1506.02025,10.48550/arXiv.1506.02025,https://arxiv.org/pdf/1506.02025,Spatial transformer networks,introduced and used in the paper,15,"6,099",3d mnist
Distorted MNIST,NeurIPS,10.48550/arXiv.1506.02025,10.48550/arXiv.1506.02025,https://arxiv.org/pdf/1506.02025,Spatial transformer networks,introduced and used in the paper,15,"6,099",distorted mnist
MNIST,NeurIPS,10.48550/arXiv.1606.03498,10.1109/5.726791,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=726791,Improved techniques for training GANs,"semi-supervised experiments + sample generation experiments, no reference",15,"6,114",mnist
CIFAR-10,NeurIPS,10.48550/arXiv.1606.03498,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Improved techniques for training GANs,"semi-supervised experiments + sample generation experiments, no reference",15,"6,114",cifar-10
SVHN,NeurIPS,10.48550/arXiv.1606.03498,N/A,http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf,Improved techniques for training GANs,"semi-supervised experiments + sample generation experiments, no reference",15,"6,114",svhn
ImageNet 2012,NeurIPS,10.48550/arXiv.1606.03498,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,Improved techniques for training GANs,"semi-supervised experiments, no reference",15,"6,114",imagenet 2012
Omniglot,NeurIPS,10.48550/arXiv.1606.04080,10.48550/arXiv.1902.03477,https://arxiv.org/pdf/1902.03477v2,Matching networks for one shot learning,,15,"5,721",omniglot
Imagenet 2012,NeurIPS,10.48550/arXiv.1606.04080,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,Matching networks for one shot learning,,15,"5,721",imagenet 2012
PTB,NeurIPS,10.48550/arXiv.1606.04080,N/A,https://aclanthology.org/J93-2004/,Matching networks for one shot learning,,15,"5,721",ptb
miniImageNet,NeurIPS,10.48550/arXiv.1606.04080,10.48550/arXiv.1606.04080,https://arxiv.org/pdf/1606.04080v2,Matching networks for one shot learning,"made by authors, details in paper",15,"5,721",miniimagenet
MNIST,NeurIPS,10.48550/arXiv.1606.09375,10.1109/5.726791,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=726791,Convolutional neural networks on graphs with fast localized spectral filtering,,15,"6,665",mnist
20NEWS,NeurIPS,10.48550/arXiv.1606.09375,10.1016/B978-1-55860-377-6.50048-7,https://paperswithcode.com/dataset/news20,Convolutional neural networks on graphs with fast localized spectral filtering,"by following the citations, this seems to be the dataset used",15,"6,665",20news
Omniglot,NeurIPS,10.48550/arXiv.1703.05175,10.48550/arXiv.1902.03477,https://arxiv.org/pdf/1902.03477v2,Prototypical networks for few-shot learning,reference to the wrong paper,15,"6,172",omniglot
miniImageNet,NeurIPS,10.48550/arXiv.1703.05175,10.48550/arXiv.1606.04080,https://arxiv.org/pdf/1606.04080v2,Prototypical networks for few-shot learning,,15,"6,172",miniimagenet
CUB-200-2011 Birds,NeurIPS,10.48550/arXiv.1703.05175,N/A,https://authors.library.caltech.edu/records/cvm3y-5hh21,Prototypical networks for few-shot learning,,15,"6,172",cub-200-2011 birds
LSUN,NeurIPS,10.48550/arXiv.1704.00028,10.48550/arXiv.1506.03365,https://www.researchgate.net/publication/278048515_LSUN_Construction_of_a_Large-scale_Image_Dataset_using_Deep_Learning_with_Humans_in_the_Loop,Improved training of wasserstein GANs,,15,"7,291",lsun
CIFAR-10,NeurIPS,10.48550/arXiv.1704.00028,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Improved training of wasserstein GANs,,15,"7,291",cifar-10
1B Word,NeurIPS,10.48550/arXiv.1704.00028,10.48550/arXiv.1312.3005,https://arxiv.org/pdf/1312.3005,Improved training of wasserstein GANs,,15,"7,291",1b word
MNIST,NeurIPS,10.48550/arXiv.1704.00028,10.1109/5.726791,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=726791,Improved training of wasserstein GANs,,15,"7,291",mnist
MNIST,NeurIPS,10.48550/arXiv.1705.07874,10.1109/5.726791,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=726791,A unified approach to interpreting model predictions,,15,"16,200",mnist
Web of Science,NeurIPS,10.48550/arXiv.1706.02216,N/A,N/A,Inductive representation learning on large graphs,,15,"11,673",web of science
Reddit,NeurIPS,10.48550/arXiv.1706.02216,10.1145/3178876.3186141,https://snap.stanford.edu/data/soc-RedditHyperlinks.html,Inductive representation learning on large graphs,,15,"11,673",reddit
PPI,NeurIPS,10.48550/arXiv.1706.02216,https://snap.stanford.edu/graphsage/,https://snap.stanford.edu/graphsage/,Inductive representation learning on large graphs,,15,"11,673",ppi
MNIST,NeurIPS,10.48550/arXiv.1706.02413,10.1109/5.726791,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=726791,PointNet++: Deep hierarchical feature learning on point sets in a metric space,,15,"8,244",mnist
ModelNet40,NeurIPS,10.48550/arXiv.1706.02413,10.48550/arXiv.1406.5670,https://arxiv.org/pdf/1406.5670,PointNet++: Deep hierarchical feature learning on point sets in a metric space,,15,"8,244",modelnet40
SHREC15,NeurIPS,10.48550/arXiv.1706.02413,10.2312/3dor.20151064,https://www.icst.pku.edu.cn/zlian/representa/3d15/index.htm,PointNet++: Deep hierarchical feature learning on point sets in a metric space,,15,"8,244",shrec15
ScanNet,NeurIPS,10.48550/arXiv.1706.02413,10.48550/arXiv.1702.04405,https://arxiv.org/pdf/1702.04405,PointNet++: Deep hierarchical feature learning on point sets in a metric space,,15,"8,244",scannet
WMT14,NeurIPS,10.48550/arXiv.1706.03762,10.3115/v1/W14-3302,https://aclanthology.org/W14-3302.pdf,Attention is all you need,"English <-> German, English <-> French",15,"83,482",wmt14
PTB,NeurIPS,10.48550/arXiv.1706.03762,N/A,https://aclanthology.org/J93-2004/,Attention is all you need,only the WSJ portion,15,"83,482",ptb
BerkeleyParser,NeurIPS,10.48550/arXiv.1706.03762,10.48550/arXiv.1412.7449,https://arxiv.org/pdf/1412.7449,Attention is all you need,,15,"83,482",berkeleyparser
CelebA,NeurIPS,10.48550/arXiv.1706.08500,10.48550/arXiv.1411.7766,https://arxiv.org/pdf/1411.7766,GANs trained by a two time-scale update rule converge to a local Nash equilibrium,,15,"8,643",celeba
Cifar-10,NeurIPS,10.48550/arXiv.1706.08500,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,GANs trained by a two time-scale update rule converge to a local Nash equilibrium,,15,"8,643",cifar-10
SVHN,NeurIPS,10.48550/arXiv.1706.08500,N/A,http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf,GANs trained by a two time-scale update rule converge to a local Nash equilibrium,,15,"8,643",svhn
LSUN,NeurIPS,10.48550/arXiv.1706.08500,10.48550/arXiv.1506.03365,https://www.researchgate.net/publication/278048515_LSUN_Construction_of_a_Large-scale_Image_Dataset_using_Deep_Learning_with_Humans_in_the_Loop,GANs trained by a two time-scale update rule converge to a local Nash equilibrium,,15,"8,643",lsun
1B Word,NeurIPS,10.48550/arXiv.1706.08500,10.48550/arXiv.1312.3005,https://arxiv.org/pdf/1312.3005,GANs trained by a two time-scale update rule converge to a local Nash equilibrium,,15,"8,643",1b word
CIFAR-10,NeurIPS,10.48550/arXiv.1904.12848,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Unsupervised data augmentation for consistency training,,5,"1,384",cifar-10
SVHN,NeurIPS,10.48550/arXiv.1904.12848,N/A,http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf,Unsupervised data augmentation for consistency training,,5,"1,384",svhn
DB Pedia,NeurIPS,10.48550/arXiv.1904.12848,10.48550/arXiv.1509.01626,https://arxiv.org/pdf/1509.01626,Unsupervised data augmentation for consistency training,,5,"1,384",db pedia
Amazon-2,NeurIPS,10.48550/arXiv.1904.12848,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Unsupervised data augmentation for consistency training,,5,"1,384",amazon-2
Amazon-5,NeurIPS,10.48550/arXiv.1904.12848,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Unsupervised data augmentation for consistency training,,5,"1,384",amazon-5
Yelp-2,NeurIPS,10.48550/arXiv.1904.12848,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Unsupervised data augmentation for consistency training,,5,"1,384",yelp-2
Yelp-5,NeurIPS,10.48550/arXiv.1904.12848,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,Unsupervised data augmentation for consistency training,,5,"1,384",yelp-5
IMDb-,NeurIPS,10.48550/arXiv.1904.12848,N/A,https://aclanthology.org/P11-1015.pdf,Unsupervised data augmentation for consistency training,,5,"1,384",imdb-
ImageNet,NeurIPS,10.48550/arXiv.1904.12848,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Unsupervised data augmentation for consistency training,,5,"1,384",imagenet
CIFAR-10,NeurIPS,10.48550/arXiv.2001.07685,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,FixMatch: Simplifying semi-supervised learning with consistency and confidence,"sometimes they use it as is, but they also modify it for other experiments",5,"2,275",cifar-10
CIFAR-100,NeurIPS,10.48550/arXiv.2001.07685,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,FixMatch: Simplifying semi-supervised learning with consistency and confidence,,5,"2,275",cifar-100
SVHN,NeurIPS,10.48550/arXiv.2001.07685,N/A,http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf,FixMatch: Simplifying semi-supervised learning with consistency and confidence,,5,"2,275",svhn
STL-10,NeurIPS,10.48550/arXiv.2001.07685,N/A,http://proceedings.mlr.press/v15/coates11a/coates11a.pdf,FixMatch: Simplifying semi-supervised learning with consistency and confidence,,5,"2,275",stl-10
ImageNet,NeurIPS,10.48550/arXiv.2001.07685,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,FixMatch: Simplifying semi-supervised learning with consistency and confidence,,5,"2,275",imagenet
ImageNet,NeurIPS,10.48550/arXiv.2004.11362,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Supervised contrastive learning,,5,"3,228",imagenet
ImageNet-C,NeurIPS,10.48550/arXiv.2004.11362,10.48550/arXiv.1903.12261,https://arxiv.org/pdf/1903.12261,Supervised contrastive learning,,5,"3,228",imagenet-c
CIFAR-10,NeurIPS,10.48550/arXiv.2004.11362,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Supervised contrastive learning,,5,"3,228",cifar-10
CIFAR-100,NeurIPS,10.48550/arXiv.2004.11362,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Supervised contrastive learning,,5,"3,228",cifar-100
Food-101,NeurIPS,10.48550/arXiv.2004.11362,10.1007/978-3-319-10599-4_29,https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/static/bossard_eccv14_food-101.pdf,Supervised contrastive learning,,5,"3,228",food-101
BirdSnap,NeurIPS,10.48550/arXiv.2004.11362,10.1109/CVPR.2014.259,https://openaccess.thecvf.com/content_cvpr_2014/papers/Berg_Birdsnap_Large-scale_Fine-grained_2014_CVPR_paper.pdf,Supervised contrastive learning,,5,"3,228",birdsnap
SUN397,NeurIPS,10.48550/arXiv.2004.11362,10.1109/CVPR.2010.5539970,https://faculty.cc.gatech.edu/~hays/papers/sun.pdf,Supervised contrastive learning,,5,"3,228",sun397
StanfordCars,NeurIPS,10.48550/arXiv.2004.11362,10.1109/ICCVW.2013.77,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=6755945,Supervised contrastive learning,,5,"3,228",stanfordcars
FGVC Aircraft,NeurIPS,10.48550/arXiv.2004.11362,10.48550/arXiv.1306.5151,https://arxiv.org/abs/1306.5151,Supervised contrastive learning,,5,"3,228",fgvc aircraft
PASCAL VOC 2007,NeurIPS,10.48550/arXiv.2004.11362,10.1007/s11263-009-0275-4,https://link.springer.com/article/10.1007/s11263-009-0275-4#preview,Supervised contrastive learning,,5,"3,228",pascal voc 2007
DTD,NeurIPS,10.48550/arXiv.2004.11362,10.1109/CVPR.2014.461,https://openaccess.thecvf.com/content_cvpr_2014/papers/Cimpoi_Describing_Textures_in_2014_CVPR_paper.pdf,Supervised contrastive learning,,5,"3,228",dtd
Oxford-IIIT Pets,NeurIPS,10.48550/arXiv.2004.11362,10.1109/CVPR.2012.6248092,https://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf,Supervised contrastive learning,,5,"3,228",oxford-iiit pets
Caltech101,NeurIPS,10.48550/arXiv.2004.11362,10.22002/D1.20086,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/1384978,Supervised contrastive learning,,5,"3,228",caltech101
Oxford Flowers 102,NeurIPS,10.48550/arXiv.2004.11362,10.1109/ICVGIP.2008.47,https://ieeexplore.ieee.org/document/4756141,Supervised contrastive learning,,5,"3,228",oxford flowers 102
ogbn-products,NeurIPS,10.48550/arXiv.2005.00687,10.48550/arXiv.2005.00687,https://arxiv.org/pdf/2005.00687,Open graph benchmark: Datasets for machine learning on graphs,released by paper (benchmark paper),5,"1,255",ogbn-products
ogbn-proteins,NeurIPS,10.48550/arXiv.2005.00687,10.48550/arXiv.2005.00688,https://arxiv.org/pdf/2005.00687,Open graph benchmark: Datasets for machine learning on graphs,released by paper (benchmark paper),5,"1,255",ogbn-proteins
ogbn-arxiv,NeurIPS,10.48550/arXiv.2005.00687,10.48550/arXiv.2005.00689,https://arxiv.org/pdf/2005.00687,Open graph benchmark: Datasets for machine learning on graphs,released by paper (benchmark paper),5,"1,255",ogbn-arxiv
ogbn-papers100M,NeurIPS,10.48550/arXiv.2005.00687,10.48550/arXiv.2005.00690,https://arxiv.org/pdf/2005.00687,Open graph benchmark: Datasets for machine learning on graphs,released by paper (benchmark paper),5,"1,255",ogbn-papers100m
ogbn-mag,NeurIPS,10.48550/arXiv.2005.00687,10.48550/arXiv.2005.00691,https://arxiv.org/pdf/2005.00687,Open graph benchmark: Datasets for machine learning on graphs,released by paper (benchmark paper),5,"1,255",ogbn-mag
ogbl-ppa,NeurIPS,10.48550/arXiv.2005.00687,10.48550/arXiv.2005.00692,https://arxiv.org/pdf/2005.00687,Open graph benchmark: Datasets for machine learning on graphs,released by paper (benchmark paper),5,"1,255",ogbl-ppa
ogbl-collab,NeurIPS,10.48550/arXiv.2005.00687,10.48550/arXiv.2005.00693,https://arxiv.org/pdf/2005.00687,Open graph benchmark: Datasets for machine learning on graphs,released by paper (benchmark paper),5,"1,255",ogbl-collab
ogbl-ddi,NeurIPS,10.48550/arXiv.2005.00687,10.48550/arXiv.2005.00694,https://arxiv.org/pdf/2005.00687,Open graph benchmark: Datasets for machine learning on graphs,released by paper (benchmark paper),5,"1,255",ogbl-ddi
ogbl-citation2,NeurIPS,10.48550/arXiv.2005.00687,10.48550/arXiv.2005.00695,https://arxiv.org/pdf/2005.00687,Open graph benchmark: Datasets for machine learning on graphs,released by paper (benchmark paper),5,"1,255",ogbl-citation2
ogbl-wikikg2,NeurIPS,10.48550/arXiv.2005.00687,10.48550/arXiv.2005.00696,https://arxiv.org/pdf/2005.00687,Open graph benchmark: Datasets for machine learning on graphs,released by paper (benchmark paper),5,"1,255",ogbl-wikikg2
ogbl-biokg,NeurIPS,10.48550/arXiv.2005.00687,10.48550/arXiv.2005.00697,https://arxiv.org/pdf/2005.00687,Open graph benchmark: Datasets for machine learning on graphs,released by paper (benchmark paper),5,"1,255",ogbl-biokg
ogbg-molhiv,NeurIPS,10.48550/arXiv.2005.00687,10.48550/arXiv.2005.00698,https://arxiv.org/pdf/2005.00687,Open graph benchmark: Datasets for machine learning on graphs,released by paper (benchmark paper),5,"1,255",ogbg-molhiv
ogbg-molpcba,NeurIPS,10.48550/arXiv.2005.00687,10.48550/arXiv.2005.00699,https://arxiv.org/pdf/2005.00687,Open graph benchmark: Datasets for machine learning on graphs,released by paper (benchmark paper),5,"1,255",ogbg-molpcba
ogbg-ppa,NeurIPS,10.48550/arXiv.2005.00687,10.48550/arXiv.2005.00700,https://arxiv.org/pdf/2005.00687,Open graph benchmark: Datasets for machine learning on graphs,released by paper (benchmark paper),5,"1,255",ogbg-ppa
ogbg-code2,NeurIPS,10.48550/arXiv.2005.00687,10.48550/arXiv.2005.00701,https://arxiv.org/pdf/2005.00687,Open graph benchmark: Datasets for machine learning on graphs,released by paper (benchmark paper),5,"1,255",ogbg-code2
NQ,NeurIPS,10.48550/arXiv.2005.11401,10.1162/tacl_a_00276,https://aclanthology.org/Q19-1026.pdf,Retrieval-augmented generation for knowledge-intensive NLP tasks,,5,"2,587",nq
TriviaQA,NeurIPS,10.48550/arXiv.2005.11401,10.18653/v1/P17-1147,https://aclanthology.org/P17-1147.pdf,Retrieval-augmented generation for knowledge-intensive NLP tasks,,5,"2,587",triviaqa
WQ,NeurIPS,10.48550/arXiv.2005.11401,N/A,https://aclanthology.org/D13-1160.pdf,Retrieval-augmented generation for knowledge-intensive NLP tasks,,5,"2,587",wq
CuratedTREC,NeurIPS,10.48550/arXiv.2005.11401,10.1007/978-3-319-24027-5_20,http://ailao.eu/yodaqa/yodaqa-clef2015.pdf,Retrieval-augmented generation for knowledge-intensive NLP tasks,,5,"2,587",curatedtrec
MS MARCO,NeurIPS,10.48550/arXiv.2005.11401,10.48550/arXiv.1611.09268,https://arxiv.org/pdf/1611.09268,Retrieval-augmented generation for knowledge-intensive NLP tasks,,5,"2,587",ms marco
SearchQA,NeurIPS,10.48550/arXiv.2005.11401,10.48550/arXiv.1704.05179,https://arxiv.org/pdf/1704.05179v3,Retrieval-augmented generation for knowledge-intensive NLP tasks,,5,"2,587",searchqa
FEVER,NeurIPS,10.48550/arXiv.2005.11401,10.18653/v1/N18-1074,https://aclanthology.org/N18-1074.pdf,Retrieval-augmented generation for knowledge-intensive NLP tasks,"they mention fever-2 and fever-3 but the references are kind of confusing, i will check later",5,"2,587",fever
Wikipedia December 2018,NeurIPS,10.48550/arXiv.2005.11401,10.18653/v1/P19-1612,https://aclanthology.org/P19-1612.pdf,Retrieval-augmented generation for knowledge-intensive NLP tasks,,5,"2,587",wikipedia december 2018
Common Crawl,NeurIPS,10.48550/arXiv.2005.14165,10.48550/arXiv.1910.10683,https://arxiv.org/pdf/1910.10683,Language models are few-shot learners,"training. modified (filtering + augmentation) ""41 shards of monthly CommonCrawl covering 2016 to 2019"" but the final dataset is privately owned by OpenAI (see paper for more info)",15,"19,613",common crawl
WebText2,NeurIPS,10.48550/arXiv.2005.14165,10.48550/arXiv.2001.08361,https://arxiv.org/pdf/2001.08361,Language models are few-shot learners,training. privately owned by OpenAI,15,"19,613",webtext2
Books1,NeurIPS,10.48550/arXiv.2005.14165,10.48550/arXiv.2005.14165,https://arxiv.org/pdf/2005.14165,Language models are few-shot learners,training. privately owned by OpenAI,15,"19,613",books1
Books2,NeurIPS,10.48550/arXiv.2005.14165,10.48550/arXiv.2005.14165,https://arxiv.org/pdf/2005.14165,Language models are few-shot learners,training. privately owned by OpenAI,15,"19,613",books2
English Wikipedia,NeurIPS,10.48550/arXiv.2005.14165,N/A,N/A,Language models are few-shot learners,training (3B tokens),15,"19,613",english wikipedia
PTB old,NeurIPS,10.48550/arXiv.2005.14165,10.3115/1075812.1075835,https://aclanthology.org/H94-1020.pdf,Language models are few-shot learners,"eval, some modifications to remove stuff that overlapped the training set",15,"19,613",ptb old
LAMBADA,NeurIPS,10.48550/arXiv.2005.14165,10.18653/v1/P16-1144,https://aclanthology.org/P16-1144.pdf,Language models are few-shot learners,eval,15,"19,613",lambada
NQ,NeurIPS,10.48550/arXiv.2005.14165,10.1162/tacl_a_00276,https://aclanthology.org/Q19-1026.pdf,Language models are few-shot learners,eval ,15,"19,613",nq
WQ,NeurIPS,10.48550/arXiv.2005.14165,N/A,https://aclanthology.org/D13-1160.pdf,Language models are few-shot learners,eval,15,"19,613",wq
TriviaQA,NeurIPS,10.48550/arXiv.2005.14165,10.18653/v1/P17-1147,https://aclanthology.org/P17-1147.pdf,Language models are few-shot learners,eval,15,"19,613",triviaqa
HellaSwag,NeurIPS,10.48550/arXiv.2005.14165,10.18653/v1/P19-1472,https://aclanthology.org/P19-1472.pdf,Language models are few-shot learners,eval,15,"19,613",hellaswag
StoryCloze,NeurIPS,10.48550/arXiv.2005.14165,10.48550/arXiv.1604.01696,https://arxiv.org/pdf/1604.01696,Language models are few-shot learners,eval,15,"19,613",storycloze
WMT14,NeurIPS,10.48550/arXiv.2005.14165,10.3115/v1/W14-3302,https://aclanthology.org/W14-3302.pdf,Language models are few-shot learners,eval (French <-> English),15,"19,613",wmt14
WMT16,NeurIPS,10.48550/arXiv.2005.14165,10.18653/v1/W16-2301,https://aclanthology.org/W16-2301.pdf,Language models are few-shot learners,"eval (German <-> English, Romanian <-> English)",15,"19,613",wmt16
WinoGrad,NeurIPS,10.48550/arXiv.2005.14165,N/A,https://cdn.aaai.org/ocs/4492/4492-21843-1-PB.pdf,Language models are few-shot learners,eval,15,"19,613",winograd
WinoGrande,NeurIPS,10.48550/arXiv.2005.14165,10.1145/3474381,https://arxiv.org/pdf/1907.10641,Language models are few-shot learners,eval,15,"19,613",winogrande
PIQA,NeurIPS,10.48550/arXiv.2005.14165,10.1609/aaai.v34i05.6239,https://ojs.aaai.org/index.php/AAAI/article/view/6239,Language models are few-shot learners,eval,15,"19,613",piqa
ARC-challenge,NeurIPS,10.48550/arXiv.2005.14165,10.48550/arXiv.1803.05457,https://arxiv.org/pdf/1803.05457,Language models are few-shot learners,eval,15,"19,613",arc-challenge
ARC-easy,NeurIPS,10.48550/arXiv.2005.14165,10.48550/arXiv.1803.05457,https://arxiv.org/pdf/1803.05457,Language models are few-shot learners,eval,15,"19,613",arc-easy
OpenBookQA,NeurIPS,10.48550/arXiv.2005.14165,10.18653/v1/D18-1260,https://aclanthology.org/D18-1260.pdf,Language models are few-shot learners,eval,15,"19,613",openbookqa
CoQA,NeurIPS,10.48550/arXiv.2005.14165,10.1162/tacl_a_00266,https://aclanthology.org/Q19-1016.pdf,Language models are few-shot learners,eval,15,"19,613",coqa
QuAC,NeurIPS,10.48550/arXiv.2005.14165,10.18653/v1/D18-1241,https://aclanthology.org/D18-1241.pdf,Language models are few-shot learners,eval,15,"19,613",quac
DROP,NeurIPS,10.48550/arXiv.2005.14165,10.18653/v1/N19-1246,https://aclanthology.org/N19-1246.pdf,Language models are few-shot learners,eval,15,"19,613",drop
SQuADv2,NeurIPS,10.48550/arXiv.2005.14165,10.48550/arXiv.1806.03822,https://arxiv.org/pdf/1806.03822,Language models are few-shot learners,eval,15,"19,613",squadv2
RACE,NeurIPS,10.48550/arXiv.2005.14165,10.18653/v1/D17-1082,https://aclanthology.org/D17-1082.pdf,Language models are few-shot learners,eval,15,"19,613",race
SuperGLUE,NeurIPS,10.48550/arXiv.2005.14165,10.48550/arXiv.1905.00537,https://papers.neurips.cc/paper_files/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf,Language models are few-shot learners,eval. all of it,15,"19,613",superglue
ANLI,NeurIPS,10.48550/arXiv.2005.14165,10.18653/v1/2020.acl-main.441,https://aclanthology.org/2020.acl-main.441.pdf,Language models are few-shot learners,eval,15,"19,613",anli
N/A arithmetic datasets,NeurIPS,10.48550/arXiv.2005.14165,10.48550/arXiv.2005.14165,https://arxiv.org/pdf/2005.14165,Language models are few-shot learners,"eval. they said they will release them, not sure if publicly available atm or not. 2D+, 2D-, 3D+, 3D-, 4D+, 4D-, 5D+, 5D-, 2Dx, 1DC",15,"19,613",n/a arithmetic datasets
N/A word scrambling datasets,NeurIPS,10.48550/arXiv.2005.14165,10.48550/arXiv.2005.14165,https://arxiv.org/pdf/2005.14165,Language models are few-shot learners,"eval. they said they will release them, not sure if publicly available atm or not. CL, A1, A2, RI, RW",15,"19,613",n/a word scrambling datasets
N/A SAT analogy problems,NeurIPS,10.48550/arXiv.2005.14165,10.48550/arXiv.2005.14165,https://arxiv.org/pdf/2005.14165,Language models are few-shot learners,"eval. they said they will release them, not sure if publicly available atm or not.",15,"19,613",n/a sat analogy problems
newser articles,NeurIPS,10.48550/arXiv.2005.14165,10.48550/arXiv.2005.14165,https://arxiv.org/pdf/2005.14165,Language models are few-shot learners,"used for an experiment, see paper for details, ss 3.9.4",15,"19,613",newser articles
Winogender,NeurIPS,10.48550/arXiv.2005.14165,10.18653/v1/N18-2002,https://aclanthology.org/N18-2002.pdf,Language models are few-shot learners,eval ,15,"19,613",winogender
N/A gender bias dataset,NeurIPS,10.48550/arXiv.2005.14165,10.48550/arXiv.2005.14165,https://arxiv.org/pdf/2005.14165,Language models are few-shot learners,"eval, made by authors, synthetic",15,"19,613",n/a gender bias dataset
N/A racial bias dataset,NeurIPS,10.48550/arXiv.2005.14165,10.48550/arXiv.2005.14165,https://arxiv.org/pdf/2005.14165,Language models are few-shot learners,"eval, made by authors, synthetic",15,"19,613",n/a racial bias dataset
SentiWordNet 3.0,NeurIPS,10.48550/arXiv.2005.14165,N/A,http://www.lrec-conf.org/proceedings/lrec2010/pdf/769_Paper.pdf,Language models are few-shot learners,evaluation (for racial bias),15,"19,613",sentiwordnet 3.0
N/A religious bias dataset,NeurIPS,10.48550/arXiv.2005.14165,10.48550/arXiv.2005.14165,https://arxiv.org/pdf/2005.14165,Language models are few-shot learners,"eval, made by authors, synthetic",15,"19,613",n/a religious bias dataset
ImageNet 2012,NeurIPS,10.48550/arXiv.2006.07733,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,Bootstrap your own latent: a new approach to self-supervised learning,training + eval,5,"3,942",imagenet 2012
Places365-Standard,NeurIPS,10.48550/arXiv.2006.07733,10.1109/TPAMI.2017.2723009,http://places2.csail.mit.edu/PAMI_places.pdf,Bootstrap your own latent: a new approach to self-supervised learning,pretraining,5,"3,942",places365-standard
Food-101,NeurIPS,10.48550/arXiv.2006.07733,10.1007/978-3-319-10599-4_29,https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/static/bossard_eccv14_food-101.pdf,Bootstrap your own latent: a new approach to self-supervised learning,,5,"3,942",food-101
CIFAR-10,NeurIPS,10.48550/arXiv.2006.07733,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Bootstrap your own latent: a new approach to self-supervised learning,,5,"3,942",cifar-10
CIFAR-100,NeurIPS,10.48550/arXiv.2006.07733,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Bootstrap your own latent: a new approach to self-supervised learning,,5,"3,942",cifar-100
BirdSnap,NeurIPS,10.48550/arXiv.2006.07733,10.1109/CVPR.2014.259,https://openaccess.thecvf.com/content_cvpr_2014/papers/Berg_Birdsnap_Large-scale_Fine-grained_2014_CVPR_paper.pdf,Bootstrap your own latent: a new approach to self-supervised learning,,5,"3,942",birdsnap
SUN397,NeurIPS,10.48550/arXiv.2006.07733,10.1109/CVPR.2010.5539970,https://faculty.cc.gatech.edu/~hays/papers/sun.pdf,Bootstrap your own latent: a new approach to self-supervised learning,,5,"3,942",sun397
StanfordCars,NeurIPS,10.48550/arXiv.2006.07733,10.1109/ICCVW.2013.77,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=6755945,Bootstrap your own latent: a new approach to self-supervised learning,,5,"3,942",stanfordcars
FGVC Aircraft,NeurIPS,10.48550/arXiv.2006.07733,10.48550/arXiv.1306.5151,https://arxiv.org/abs/1306.5151,Bootstrap your own latent: a new approach to self-supervised learning,,5,"3,942",fgvc aircraft
PASCAL VOC 2007,NeurIPS,10.48550/arXiv.2006.07733,10.1007/s11263-009-0275-4,https://link.springer.com/article/10.1007/s11263-009-0275-4#preview,Bootstrap your own latent: a new approach to self-supervised learning,,5,"3,942",pascal voc 2007
PASCAL VOC 2012,NeurIPS,10.48550/arXiv.2006.07733,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,Bootstrap your own latent: a new approach to self-supervised learning,,5,"3,942",pascal voc 2012
DTD,NeurIPS,10.48550/arXiv.2006.07733,10.1109/CVPR.2014.461,https://openaccess.thecvf.com/content_cvpr_2014/papers/Cimpoi_Describing_Textures_in_2014_CVPR_paper.pdf,Bootstrap your own latent: a new approach to self-supervised learning,,5,"3,942",dtd
Oxford-IIIT Pets,NeurIPS,10.48550/arXiv.2006.07733,10.1109/CVPR.2012.6248092,https://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf,Bootstrap your own latent: a new approach to self-supervised learning,,5,"3,942",oxford-iiit pets
Caltech101,NeurIPS,10.48550/arXiv.2006.07733,10.22002/D1.20086,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/1384978,Bootstrap your own latent: a new approach to self-supervised learning,,5,"3,942",caltech101
Oxford Flowers 102,NeurIPS,10.48550/arXiv.2006.07733,10.1109/ICVGIP.2008.47,https://ieeexplore.ieee.org/document/4756141,Bootstrap your own latent: a new approach to self-supervised learning,,5,"3,942",oxford flowers 102
NYUDv2,NeurIPS,10.48550/arXiv.2006.07733,10.1007/978-3-642-33715-4_54,https://link.springer.com/chapter/10.1007/978-3-642-33715-4_54,Bootstrap your own latent: a new approach to self-supervised learning,used but not referenced,5,"3,942",nyudv2
CelebA,NeurIPS,10.48550/arXiv.2006.09661,10.48550/arXiv.1411.7766,https://arxiv.org/pdf/1411.7766,Implicit neural representations with periodic activation functions,,5,"1,511",celeba
BSDS500,NeurIPS,10.48550/arXiv.2006.09661,10.1109/TPAMI.2010.161,https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/papers/amfm_pami2010.pdf,Implicit neural representations with periodic activation functions,,5,"1,511",bsds500
N/A synthetic datasets,NeurIPS,10.48550/arXiv.2006.09661,10.48550/arXiv.2006.09661,https://arxiv.org/pdf/2006.09661,Implicit neural representations with periodic activation functions,made by authors for some experiments,5,"1,511",n/a synthetic datasets
ImageNet,NeurIPS,10.48550/arXiv.2006.09882,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Unsupervised learning of visual features by contrasting cluster assignments,"no reference, so idk which one",5,"2,314",imagenet
Places205,NeurIPS,10.48550/arXiv.2006.09882,N/A,http://places.csail.mit.edu/places_NIPS14.pdf,Unsupervised learning of visual features by contrasting cluster assignments,,5,"2,314",places205
PASCAL VOC 2007,NeurIPS,10.48550/arXiv.2006.09882,10.1007/s11263-009-0275-4,https://link.springer.com/article/10.1007/s11263-009-0275-4#preview,Unsupervised learning of visual features by contrasting cluster assignments,,5,"2,314",pascal voc 2007
iNaturalist,NeurIPS,10.48550/arXiv.2006.09882,10.1109/CVPR.2018.00914,https://ieeexplore.ieee.org/document/8579012,Unsupervised learning of visual features by contrasting cluster assignments,,5,"2,314",inaturalist
PASCAL VOC 2012,NeurIPS,10.48550/arXiv.2006.09882,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,Unsupervised learning of visual features by contrasting cluster assignments,,5,"2,314",pascal voc 2012
COCO,NeurIPS,10.48550/arXiv.2006.09882,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Unsupervised learning of visual features by contrasting cluster assignments,,5,"2,314",coco
Instagram-1B,NeurIPS,10.48550/arXiv.2006.09882,10.48550/arXiv.1805.00932,https://openaccess.thecvf.com/content_ECCV_2018/papers/Dhruv_Mahajan_Exploring_the_Limits_ECCV_2018_paper.pdf,Unsupervised learning of visual features by contrasting cluster assignments,"not sure if this is actually the one, paper mentions ""1 billion random public Instagram pictures"" with no reference",5,"2,314",instagram-1b
DIV2K,NeurIPS,10.48550/arXiv.2006.10739,10.1109/CVPRW.2017.150,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/8014884,Fourier features let networks learn high frequency functions in low dimensional domains,"random samples from it, renamed ""Natural""",5,"1,338",div2k
Text,NeurIPS,10.48550/arXiv.2006.10739,10.48550/arXiv.2006.10739,https://arxiv.org/pdf/2006.10739,Fourier features let networks learn high frequency functions in low dimensional domains,made by authors,5,"1,338",text
"""randomized Shepp-Logan phantoms""",NeurIPS,10.48550/arXiv.2006.10739,10.1109/TNS.1974.6499235,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6499235,Fourier features let networks learn high frequency functions in low dimensional domains,"not a dataset, authors randomly modify the image to get a ""dataset""",5,"1,338","""randomized shepp-logan phantoms"""
ATLAS,NeurIPS,10.48550/arXiv.2006.10739,10.1038/sdata.2018.11,https://www.nature.com/articles/sdata201811,Fourier features let networks learn high frequency functions in low dimensional domains,,5,"1,338",atlas
Lego,NeurIPS,10.48550/arXiv.2006.10739,10.48550/arXiv.2003.08934,https://arxiv.org/pdf/2003.08934,Fourier features let networks learn high frequency functions in low dimensional domains,,5,"1,338",lego
CIFAR-10,NeurIPS,10.48550/arXiv.2006.11239,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Denoising diffusion probabilistic models,,15,"9,246",cifar-10
LSUN,NeurIPS,10.48550/arXiv.2006.11239,10.48550/arXiv.1506.03365,https://www.researchgate.net/publication/278048515_LSUN_Construction_of_a_Large-scale_Image_Dataset_using_Deep_Learning_with_Humans_in_the_Loop,Denoising diffusion probabilistic models,"bedroom, church, cat",15,"9,246",lsun
CelebAHQ,NeurIPS,10.48550/arXiv.2006.11239,10.48550/arXiv.1710.10196,https://openreview.net/pdf?id=Hk99zCeAb,Denoising diffusion probabilistic models,,15,"9,246",celebahq
LibriSpeech,NeurIPS,10.48550/arXiv.2006.11477,10.1109/ICASSP.2015.7178964,https://www.danielpovey.com/files/2015_icassp_librispeech.pdf,wav2vec 2.0: A framework for self-supervised learning of speech representations,,5,"3,605",librispeech
LibriVox,NeurIPS,10.48550/arXiv.2006.11477,?,https://librivox.org/,wav2vec 2.0: A framework for self-supervised learning of speech representations,,5,"3,605",librivox
TIMIT,NeurIPS,10.48550/arXiv.2006.11477,10.35111/17gk-bn40,https://nvlpubs.nist.gov/nistpubs/Legacy/IR/nistir4930.pdf,wav2vec 2.0: A framework for self-supervised learning of speech representations,,5,"3,605",timit
Libri-light,NeurIPS,10.48550/arXiv.2006.11477,?,?,wav2vec 2.0: A framework for self-supervised learning of speech representations,,5,"3,605",libri-light
LJ Speech,NeurIPS,10.48550/arXiv.2010.05646,?,https://keithito.com/LJ-Speech-Dataset/,HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis,,5,"1,273",lj speech
VCTK,NeurIPS,10.48550/arXiv.2010.05646,10.7488/ds/2645,https://datashare.ed.ac.uk/handle/10283/3443,HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis,,5,"1,273",vctk
N/A synthetic radio,NeurIPS,10.48550/arXiv.2010.05646,10.48550/arXiv.2010.05646,https://arxiv.org/pdf/2010.05646,HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis,made by authors,5,"1,273",n/a synthetic radio
MNIST,NeurIPS,10.48550/arXiv.2010.13902,10.1109/5.726791,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=726791,Graph contrastive learning with augmentations,,5,"1,645",mnist
CIFAR-10,NeurIPS,10.48550/arXiv.2010.13902,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Graph contrastive learning with augmentations,,5,"1,645",cifar-10
NCI1,NeurIPS,10.48550/arXiv.2010.13902,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Graph contrastive learning with augmentations,,5,"1,645",nci1
PROTEINS,NeurIPS,10.48550/arXiv.2010.13902,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Graph contrastive learning with augmentations,,5,"1,645",proteins
DD,NeurIPS,10.48550/arXiv.2010.13902,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Graph contrastive learning with augmentations,,5,"1,645",dd
COLLAB,NeurIPS,10.48550/arXiv.2010.13902,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Graph contrastive learning with augmentations,,5,"1,645",collab
RDT-B,NeurIPS,10.48550/arXiv.2010.13902,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Graph contrastive learning with augmentations,,5,"1,645",rdt-b
GITHUB,NeurIPS,10.48550/arXiv.2010.13902,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Graph contrastive learning with augmentations,,5,"1,645",github
MUTAG,NeurIPS,10.48550/arXiv.2010.13902,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Graph contrastive learning with augmentations,,5,"1,645",mutag
IMDB-BINARY,NeurIPS,10.48550/arXiv.2010.13902,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Graph contrastive learning with augmentations,,5,"1,645",imdb-binary
RDT-M5K,NeurIPS,10.48550/arXiv.2010.13902,10.48550/arXiv.2007.08663,https://arxiv.org/pdf/2007.08663,Graph contrastive learning with augmentations,,5,"1,645",rdt-m5k
PPI,NeurIPS,10.48550/arXiv.2010.13902,https://snap.stanford.edu/graphsage/,https://snap.stanford.edu/graphsage/,Graph contrastive learning with augmentations,,5,"1,645",ppi
BBBP,NeurIPS,10.48550/arXiv.2010.13902,10.1039/C7SC02664A,https://arxiv.org/pdf/1703.00564,Graph contrastive learning with augmentations,,5,"1,645",bbbp
Tox-21,NeurIPS,10.48550/arXiv.2010.13902,10.1039/C7SC02664A,https://arxiv.org/pdf/1703.00564,Graph contrastive learning with augmentations,,5,"1,645",tox-21
ToxCast,NeurIPS,10.48550/arXiv.2010.13902,10.1039/C7SC02664A,https://arxiv.org/pdf/1703.00564,Graph contrastive learning with augmentations,,5,"1,645",toxcast
SIDER,NeurIPS,10.48550/arXiv.2010.13902,10.1039/C7SC02664A,https://arxiv.org/pdf/1703.00564,Graph contrastive learning with augmentations,,5,"1,645",sider
ClinTox,NeurIPS,10.48550/arXiv.2010.13902,10.1039/C7SC02664A,https://arxiv.org/pdf/1703.00564,Graph contrastive learning with augmentations,,5,"1,645",clintox
MUV,NeurIPS,10.48550/arXiv.2010.13902,10.1039/C7SC02664A,https://arxiv.org/pdf/1703.00564,Graph contrastive learning with augmentations,,5,"1,645",muv
HIV,NeurIPS,10.48550/arXiv.2010.13902,10.1039/C7SC02664A,https://arxiv.org/pdf/1703.00564,Graph contrastive learning with augmentations,,5,"1,645",hiv
BACE,NeurIPS,10.48550/arXiv.2010.13902,10.1039/C7SC02664A,https://arxiv.org/pdf/1703.00564,Graph contrastive learning with augmentations,,5,"1,645",bace
LSUN,NeurIPS,10.48550/arXiv.2105.05233,10.48550/arXiv.1506.03365,https://www.researchgate.net/publication/278048515_LSUN_Construction_of_a_Large-scale_Image_Dataset_using_Deep_Learning_with_Humans_in_the_Loop,Diffusion Models Beat GANs on Image Synthesis,"training; classes used: bedroom, horse, cat",5,"4,045",lsun
ImageNet,NeurIPS,10.48550/arXiv.2105.05233,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Diffusion Models Beat GANs on Image Synthesis,training,5,"4,045",imagenet
ImageNet-1K,NeurIPS,10.48550/arXiv.2105.15203,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,pre-training encoder,5,"3,896",imagenet-1k
Cityscapes,NeurIPS,10.48550/arXiv.2105.15203,10.1109/CVPR.2016.350,https://ieeexplore.ieee.org/document/7780719,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,"training, eval",5,"3,896",cityscapes
ADE20K,NeurIPS,10.48550/arXiv.2105.15203,10.1007/s11263-018-1140-0,https://www.researchgate.net/publication/306357649_Semantic_Understanding_of_Scenes_Through_the_ADE20K_Dataset,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,"training, eval",5,"3,896",ade20k
COCO-Stuff,NeurIPS,10.48550/arXiv.2105.15203,10.48550/arXiv.1612.03716,https://openaccess.thecvf.com/content_cvpr_2018/papers/Caesar_COCO-Stuff_Thing_and_CVPR_2018_paper.pdf,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,"training, eval",5,"3,896",coco-stuff
CityScapes-C,NeurIPS,10.48550/arXiv.2105.15203,10.48550/arXiv.2105.15203,https://arxiv.org/pdf/2105.15203,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,"evaluation, made by authors",5,"3,896",cityscapes-c
ImageNet 2012,NeurIPS,10.48550/arXiv.2105.15203,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,"trainig + eval, references the original ImageNet but mentions ILRSVC 2012",5,"3,896",imagenet 2012
ImageNet,NeurIPS,10.48550/arXiv.2105.15203,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,training,5,"3,896",imagenet
JFT-300M,NeurIPS,10.48550/arXiv.2105.15203,10.48550/arXiv.1707.02968,https://arxiv.org/pdf/1707.02968v2,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,training,5,"3,896",jft-300m
JFT-3B,NeurIPS,10.48550/arXiv.2105.15203,10.48550/arXiv.2106.04560,https://arxiv.org/pdf/2106.04560,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,training for scale analysis,5,"3,896",jft-3b
ImageNet-Real,NeurIPS,10.48550/arXiv.2105.15203,10.48550/arXiv.2006.07159,https://arxiv.org/abs/2006.07159,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,evaluation,5,"3,896",imagenet-real
CIFAR-10,NeurIPS,10.48550/arXiv.2105.15203,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,evaluation,5,"3,896",cifar-10
CIFAR-100,NeurIPS,10.48550/arXiv.2105.15203,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,evaluation,5,"3,896",cifar-100
Oxford-IIIT Pets,NeurIPS,10.48550/arXiv.2105.15203,10.1109/CVPR.2012.6248092,https://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,evaluation,5,"3,896",oxford-iiit pets
Oxford Flowers 102,NeurIPS,10.48550/arXiv.2105.15203,10.1109/ICVGIP.2008.47,https://ieeexplore.ieee.org/document/4756141,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,evaluation,5,"3,896",oxford flowers 102
VTAB-1K,NeurIPS,10.48550/arXiv.2105.15203,10.48550/arXiv.1910.04867,https://arxiv.org/pdf/1910.04867,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,evaluation,5,"3,896",vtab-1k
ETT,NeurIPS,10.48550/arXiv.2106.13008,10.1609/aaai.v35i12.17325,https://ojs.aaai.org/index.php/AAAI/article/view/17325,Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting,"h1, h2, m1, m2",5,"1,629",ett
ECL / Electricity,NeurIPS,10.48550/arXiv.2106.13008,10.24432/C58C86,https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014,Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting,,5,"1,629",ecl / electricity
Exchange-Rate,NeurIPS,10.48550/arXiv.2106.13008,10.48550/arXiv.1703.07015,https://arxiv.org/pdf/1703.07015,Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting,,5,"1,629",exchange-rate
Traffic,NeurIPS,10.48550/arXiv.2106.13008,N/A,N/A,Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting,,5,"1,629",traffic
Jena Weather,NeurIPS,10.48550/arXiv.2106.13008,N/A,https://www.bgc-jena.mpg.de/wetter/,Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting,,5,"1,629",jena weather
ILI,NeurIPS,10.48550/arXiv.2106.13008,N/A,https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html,Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting,,5,"1,629",ili
Conceptual Captions,NeurIPS,10.48550/arXiv.2107.07651,N/A,https://aclanthology.org/P18-1238.pdf,Align before Fuse: Vision and Language Representation Learning with Momentum Distillation,training,5,"1,328",conceptual captions
SBU,NeurIPS,10.48550/arXiv.2107.07651,N/A,https://papers.nips.cc/paper_files/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf,Align before Fuse: Vision and Language Representation Learning with Momentum Distillation,training,5,"1,328",sbu
COCO,NeurIPS,10.48550/arXiv.2107.07651,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Align before Fuse: Vision and Language Representation Learning with Momentum Distillation,training + evaluation,5,"1,328",coco
Visual Genome,NeurIPS,10.48550/arXiv.2107.07651,10.1007/s11263-016-0981-7,https://www.researchgate.net/publication/301844872_Visual_Genome_Connecting_Language_and_Vision_Using_Crowdsourced_Dense_Image_Annotations,Align before Fuse: Vision and Language Representation Learning with Momentum Distillation,training,5,"1,328",visual genome
CC12M,NeurIPS,10.48550/arXiv.2107.07651,10.48550/arXiv.2102.08981,https://arxiv.org/pdf/2102.08981v2,Align before Fuse: Vision and Language Representation Learning with Momentum Distillation,training for scale analysis,5,"1,328",cc12m
Flickr30k,NeurIPS,10.48550/arXiv.2107.07651,10.1162/tacl_a_00166,https://aclanthology.org/Q14-1006.pdf,Align before Fuse: Vision and Language Representation Learning with Momentum Distillation,evaluation,5,"1,328",flickr30k
SNLI-VE,NeurIPS,10.48550/arXiv.2107.07651,10.48550/arXiv.1901.06706,https://arxiv.org/pdf/1901.06706,Align before Fuse: Vision and Language Representation Learning with Momentum Distillation,evaluation,5,"1,328",snli-ve
VQA 2.0,NeurIPS,10.48550/arXiv.2107.07651,10.1109/CVPR.2017.670,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8100153&tag=1,Align before Fuse: Vision and Language Representation Learning with Momentum Distillation,evaluation,5,"1,328",vqa 2.0
NLVR2,NeurIPS,10.48550/arXiv.2107.07651,10.18653/v1/P19-1644,https://aclanthology.org/P19-1644v2.pdf,Align before Fuse: Vision and Language Representation Learning with Momentum Distillation,evaluation,5,"1,328",nlvr2
Ref-COCO+,NeurIPS,10.48550/arXiv.2107.07651,10.48550/arXiv.1608.00272,https://arxiv.org/pdf/1608.00272,Align before Fuse: Vision and Language Representation Learning with Momentum Distillation,evaluation,5,"1,328",ref-coco+
GSM8K,NeurIPS,10.48550/arXiv.2201.11903,10.48550/arXiv.2110.14168,https://arxiv.org/pdf/2110.14168v2,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,,5,"4,314",gsm8k
SVAMP,NeurIPS,10.48550/arXiv.2201.11903,10.18653/v1/2021.naacl-main.168,https://aclanthology.org/2021.naacl-main.168.pdf,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,,5,"4,314",svamp
ASDiv,NeurIPS,10.48550/arXiv.2201.11903,10.18653/v1/2020.acl-main.92,https://aclanthology.org/2020.acl-main.92.pdf,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,,5,"4,314",asdiv
AQuA,NeurIPS,10.48550/arXiv.2201.11903,10.18653/v1/P17-1015,https://aclanthology.org/P17-1015.pdf,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,,5,"4,314",aqua
MAWPS,NeurIPS,10.48550/arXiv.2201.11903,10.18653/v1/N16-1136,https://aclanthology.org/N16-1136.pdf,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,,5,"4,314",mawps
CommonSenseQA,NeurIPS,10.48550/arXiv.2201.11903,10.18653/v1/N19-1421,https://aclanthology.org/N19-1421.pdf,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,,5,"4,314",commonsenseqa
StrategyQA,NeurIPS,10.48550/arXiv.2201.11903,10.1162/tacl_a_00370,https://aclanthology.org/2021.tacl-1.21.pdf,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,,5,"4,314",strategyqa
Date Understanding,NeurIPS,10.48550/arXiv.2201.11903,10.48550/arXiv.2206.04615,https://arxiv.org/pdf/2206.04615,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,,5,"4,314",date understanding
Sports Understanding,NeurIPS,10.48550/arXiv.2201.11903,10.48550/arXiv.2206.04615,https://arxiv.org/pdf/2206.04615,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,,5,"4,314",sports understanding
SayCan,NeurIPS,10.48550/arXiv.2201.11903,10.48550/arXiv.2204.01691,https://arxiv.org/pdf/2204.01691,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,,5,"4,314",saycan
Last Letter,NeurIPS,10.48550/arXiv.2201.11903,10.48550/arXiv.2201.11903,https://arxiv.org/pdf/2201.11903,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,dataset introduced by same paper,5,"4,314",last letter
Coin Flip,NeurIPS,10.48550/arXiv.2201.11903,10.48550/arXiv.2201.11903,https://arxiv.org/pdf/2201.11903,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,dataset introduced by same paper,5,"4,314",coin flip
"N/A, OpenAI API prompt dataset",NeurIPS,10.48550/arXiv.2203.02155,10.48550/arXiv.2203.02155,https://arxiv.org/pdf/2203.02155,Training language models to follow instructions with human feedback,training; dataset introduced by the paper,5,"4,795","n/a, openai api prompt dataset"
RealToxicityPrompts,NeurIPS,10.48550/arXiv.2203.02155,10.48550/arXiv.2009.11462,https://arxiv.org/pdf/2009.11462,Training language models to follow instructions with human feedback,evaluation,5,"4,795",realtoxicityprompts
Winogender,NeurIPS,10.48550/arXiv.2203.02155,10.18653/v1/N18-2002,https://aclanthology.org/N18-2002.pdf,Training language models to follow instructions with human feedback,evaluation,5,"4,795",winogender
CrowS-Pairs,NeurIPS,10.48550/arXiv.2203.02155,10.18653/v1/2020.emnlp-main.154,https://aclanthology.org/2020.emnlp-main.154.pdf,Training language models to follow instructions with human feedback,evaluation,5,"4,795",crows-pairs
SQuADv2,NeurIPS,10.48550/arXiv.2203.02155,10.48550/arXiv.1806.03822,https://arxiv.org/pdf/1806.03822,Training language models to follow instructions with human feedback,evaluation,5,"4,795",squadv2
DROP,NeurIPS,10.48550/arXiv.2203.02155,10.18653/v1/N19-1246,https://aclanthology.org/N19-1246.pdf,Training language models to follow instructions with human feedback,evaluation,5,"4,795",drop
HellaSwag,NeurIPS,10.48550/arXiv.2203.02155,10.18653/v1/P19-1472,https://aclanthology.org/P19-1472.pdf,Training language models to follow instructions with human feedback,evaluation,5,"4,795",hellaswag
WMT15,NeurIPS,10.48550/arXiv.2203.02155,10.18653/v1/W15-3001,https://aclanthology.org/W15-3001.pdf,Training language models to follow instructions with human feedback,evaluation; French-English translation,5,"4,795",wmt15
FLAN,NeurIPS,10.48550/arXiv.2203.02155,10.48550/arXiv.2109.01652,https://arxiv.org/pdf/2109.01652,Training language models to follow instructions with human feedback,evaluation,5,"4,795",flan
T0++,NeurIPS,10.48550/arXiv.2203.02155,10.48550/arXiv.2110.08207,https://arxiv.org/pdf/2110.08207,Training language models to follow instructions with human feedback,evaluation,5,"4,795",t0++
TruthfulQA,NeurIPS,10.48550/arXiv.2203.02155,10.48550/arXiv.2109.07958,https://arxiv.org/pdf/2109.07958,Training language models to follow instructions with human feedback,evaluation,5,"4,795",truthfulqa
QuAC,NeurIPS,10.48550/arXiv.2203.02155,10.18653/v1/D18-1241,https://aclanthology.org/D18-1241.pdf,Training language models to follow instructions with human feedback,evaluation,5,"4,795",quac
RTE,NeurIPS,10.48550/arXiv.2203.02155,10.48550/arXiv.1905.00537,https://arxiv.org/pdf/1905.00537,Training language models to follow instructions with human feedback,evaluation,5,"4,795",rte
Winograd,NeurIPS,10.48550/arXiv.2203.02155,N/A,https://cdn.aaai.org/ocs/4492/4492-21843-1-PB.pdf,Training language models to follow instructions with human feedback,"evaluation, my paper uses another reference https://arxiv.org/pdf/1905.00537 i'm not sure if they are the same dataset",5,"4,795",winograd
Webis-TLDR-17,NeurIPS,10.48550/arXiv.2203.02155,10.18653/v1/W17-4508,https://aclanthology.org/W17-4508.pdf,Training language models to follow instructions with human feedback,evaluation,5,"4,795",webis-tldr-17
CNN/DM modified,NeurIPS,10.48550/arXiv.2203.02155,10.18653/v1/K16-1028,https://aclanthology.org/K16-1028.pdf,Training language models to follow instructions with human feedback,evaluation,5,"4,795",cnn/dm modified
M3W,NeurIPS,10.48550/arXiv.2204.14198,10.48550/arXiv.2204.14198,https://arxiv.org/pdf/2204.14198,Flamingo: a Visual Language Model for Few-Shot Learning,training; dataset released in this paper,5,"1,486",m3w
ALIGN,NeurIPS,10.48550/arXiv.2204.14198,10.48550/arXiv.2102.05918,https://arxiv.org/pdf/2102.05918,Flamingo: a Visual Language Model for Few-Shot Learning,training,5,"1,486",align
LTIP,NeurIPS,10.48550/arXiv.2204.14198,10.48550/arXiv.2204.14198,https://arxiv.org/pdf/2204.14198,Flamingo: a Visual Language Model for Few-Shot Learning,training; dataset released in this paper,5,"1,486",ltip
VTP,NeurIPS,10.48550/arXiv.2204.14198,10.48550/arXiv.2204.14198,https://arxiv.org/pdf/2204.14198,Flamingo: a Visual Language Model for Few-Shot Learning,training; dataset released in this paper,5,"1,486",vtp
ImageNet-1K,NeurIPS,10.48550/arXiv.2204.14198,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,Flamingo: a Visual Language Model for Few-Shot Learning,"dev, eval",5,"1,486",imagenet-1k
COCO Captions,NeurIPS,10.48550/arXiv.2204.14198,10.48550/arXiv.1504.00325,https://arxiv.org/pdf/1504.00325,Flamingo: a Visual Language Model for Few-Shot Learning,"dev, eval. COCO was mentioned, but the reference points to COCO captions",5,"1,486",coco captions
VQA 2.0,NeurIPS,10.48550/arXiv.2204.14198,10.1109/CVPR.2017.670,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8100153&tag=1,Flamingo: a Visual Language Model for Few-Shot Learning,"dev, eval. VQAv2 is menitoned, but the reference paper is the original VQA",5,"1,486",vqa 2.0
OKVQA,NeurIPS,10.48550/arXiv.2204.14198,10.48550/arXiv.1906.00067,https://arxiv.org/pdf/1906.00067,Flamingo: a Visual Language Model for Few-Shot Learning,"dev, eval",5,"1,486",okvqa
Flickr30k,NeurIPS,10.48550/arXiv.2204.14198,10.1162/tacl_a_00166,https://aclanthology.org/Q14-1006.pdf,Flamingo: a Visual Language Model for Few-Shot Learning,evaluation,5,"1,486",flickr30k
VizWiz,NeurIPS,10.48550/arXiv.2204.14198,10.48550/arXiv.1802.08218,https://arxiv.org/pdf/1802.08218,Flamingo: a Visual Language Model for Few-Shot Learning,evaluation,5,"1,486",vizwiz
TextVQA,NeurIPS,10.48550/arXiv.2204.14198,10.48550/arXiv.1904.08920,https://arxiv.org/pdf/1904.08920,Flamingo: a Visual Language Model for Few-Shot Learning,evaluation,5,"1,486",textvqa
VisDial,NeurIPS,10.48550/arXiv.2204.14198,10.48550/arXiv.1611.08669,https://arxiv.org/pdf/1611.08669,Flamingo: a Visual Language Model for Few-Shot Learning,evaluation,5,"1,486",visdial
HatefulMemes,NeurIPS,10.48550/arXiv.2204.14198,10.48550/arXiv.2005.04790,https://arxiv.org/pdf/2005.04790,Flamingo: a Visual Language Model for Few-Shot Learning,evaluation,5,"1,486",hatefulmemes
Kinetics 700 2020,NeurIPS,10.48550/arXiv.2204.14198,10.48550/arXiv.2010.10864,https://arxiv.org/pdf/2010.10864,Flamingo: a Visual Language Model for Few-Shot Learning,dev,5,"1,486",kinetics 700 2020
VATEX,NeurIPS,10.48550/arXiv.2204.14198,10.1109/ICCV.2019.00468,https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_VaTeX_A_Large-Scale_High-Quality_Multilingual_Dataset_for_Video-and-Language_Research_ICCV_2019_paper.pdf,Flamingo: a Visual Language Model for Few-Shot Learning,"dev, eval",5,"1,486",vatex
MSVD-QA,NeurIPS,10.48550/arXiv.2204.14198,10.1145/3123266.3123427,http://staff.ustc.edu.cn/~hexn/papers/mm17-videoQA.pdf,Flamingo: a Visual Language Model for Few-Shot Learning,"dev, eval",5,"1,486",msvd-qa
YouCook2,NeurIPS,10.48550/arXiv.2204.14198,10.1609/aaai.v32i1.12342,https://cdn.aaai.org/ojs/12342/12342-13-15870-1-2-20201228.pdf,Flamingo: a Visual Language Model for Few-Shot Learning,evaluation,5,"1,486",youcook2
MSRVTT-QA,NeurIPS,10.48550/arXiv.2204.14198,10.1145/3123266.3123428,http://staff.ustc.edu.cn/~hexn/papers/mm17-videoQA.pdf,Flamingo: a Visual Language Model for Few-Shot Learning,evaluation,5,"1,486",msrvtt-qa
iVQA,NeurIPS,10.48550/arXiv.2204.14198,10.48550/arXiv.2012.00451,https://arxiv.org/pdf/2012.00451,Flamingo: a Visual Language Model for Few-Shot Learning,evaluation,5,"1,486",ivqa
RareAct,NeurIPS,10.48550/arXiv.2204.14198,10.48550/arXiv.2008.01018,https://arxiv.org/pdf/2008.01018,Flamingo: a Visual Language Model for Few-Shot Learning,evaluation,5,"1,486",rareact
NextQA,NeurIPS,10.48550/arXiv.2204.14198,10.1109/CVPR46437.2021.00965,https://arxiv.org/pdf/2105.08276,Flamingo: a Visual Language Model for Few-Shot Learning,evaluation,5,"1,486",nextqa
STAR,NeurIPS,10.48550/arXiv.2204.14198,10.48550/arXiv.2405.09711,https://arxiv.org/pdf/2405.09711,Flamingo: a Visual Language Model for Few-Shot Learning,evaluation,5,"1,486",star
COCO,NeurIPS,10.48550/arXiv.2205.11487,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,evaluation,5,"2,207",coco
DrawBench,NeurIPS,10.48550/arXiv.2205.11487,10.48550/arXiv.2205.11487,https://arxiv.org/pdf/2205.11487,Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,evaluation; dataset created by authors,5,"2,207",drawbench
N/A internal datasets,NeurIPS,10.48550/arXiv.2205.11487,10.48550/arXiv.2205.11488,https://arxiv.org/pdf/2205.11487,Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,"training, nothing else mentioned about these datasets",5,"2,207",n/a internal datasets
LAION-400M,NeurIPS,10.48550/arXiv.2205.11487,10.48550/arXiv.2111.02114,https://arxiv.org/pdf/2111.02114v1,Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,training,5,"2,207",laion-400m
SingleEq,NeurIPS,10.48550/arXiv.2205.11916,10.1162/tacl_a_00160,https://aclanthology.org/Q15-1042.pdf,Large Language Models are Zero-Shot Reasoners,evaluation,5,"1,569",singleeq
AddSub,NeurIPS,10.48550/arXiv.2205.11916,N/A,https://github.com/AGI-Edgerunners/LLM-Adapters/tree/main/dataset/AddSub,Large Language Models are Zero-Shot Reasoners,evaluation,5,"1,569",addsub
MultiArith,NeurIPS,10.48550/arXiv.2205.11916,10.18653/v1/D15-1202,https://aclanthology.org/D15-1202.pdf,Large Language Models are Zero-Shot Reasoners,evaluation,5,"1,569",multiarith
AQuA,NeurIPS,10.48550/arXiv.2205.11916,10.18653/v1/P17-1015,https://aclanthology.org/P17-1015.pdf,Large Language Models are Zero-Shot Reasoners,evaluation,5,"1,569",aqua
GSM8K,NeurIPS,10.48550/arXiv.2205.11916,10.48550/arXiv.2110.14168,https://arxiv.org/pdf/2110.14168v2,Large Language Models are Zero-Shot Reasoners,evaluation,5,"1,569",gsm8k
SVAMP,NeurIPS,10.48550/arXiv.2205.11916,10.18653/v1/2021.naacl-main.168,https://aclanthology.org/2021.naacl-main.168.pdf,Large Language Models are Zero-Shot Reasoners,evaluation,5,"1,569",svamp
CommonSenseQA,NeurIPS,10.48550/arXiv.2205.11916,10.18653/v1/N19-1421,https://aclanthology.org/N19-1421.pdf,Large Language Models are Zero-Shot Reasoners,evaluation,5,"1,569",commonsenseqa
StrategyQA,NeurIPS,10.48550/arXiv.2205.11916,10.1162/tacl_a_00370,https://aclanthology.org/2021.tacl-1.21.pdf,Large Language Models are Zero-Shot Reasoners,evaluation,5,"1,569",strategyqa
Last Letter,NeurIPS,10.48550/arXiv.2205.11916,10.48550/arXiv.2201.11903,https://arxiv.org/pdf/2201.11903,Large Language Models are Zero-Shot Reasoners,evaluation,5,"1,569",last letter
Coin Flip,NeurIPS,10.48550/arXiv.2205.11916,10.48550/arXiv.2201.11903,https://arxiv.org/pdf/2201.11903,Large Language Models are Zero-Shot Reasoners,evaluation,5,"1,569",coin flip
Date Understanding,NeurIPS,10.48550/arXiv.2205.11916,10.48550/arXiv.2206.04615,https://arxiv.org/pdf/2206.04615,Large Language Models are Zero-Shot Reasoners,evaluation,5,"1,569",date understanding
Tracking Shuffled Objects,NeurIPS,10.48550/arXiv.2205.11916,10.48550/arXiv.2206.04615,https://arxiv.org/pdf/2206.04615,Large Language Models are Zero-Shot Reasoners,evaluation,5,"1,569",tracking shuffled objects
CCNet,NeurIPS,10.48550/arXiv.2302.04761,10.48550/arXiv.1911.00359,https://arxiv.org/pdf/1911.00359,Toolformer: Language Models Can Teach Themselves to Use Tools,"training - they use a subset of this and re-annotate, so the end result is a diff dataset. evaluation - another subset",2,212,ccnet
SQuAD,NeurIPS,10.48550/arXiv.2302.04761,10.18653/v1/D16-1264,https://arxiv.org/pdf/1606.05250,Toolformer: Language Models Can Teach Themselves to Use Tools,evaluation,2,212,squad
Google-RE,NeurIPS,10.48550/arXiv.2302.04761,N/A,https://code.google.com/archive/p/relation-extraction-corpus/,Toolformer: Language Models Can Teach Themselves to Use Tools,evaluation,2,212,google-re
T-REx,NeurIPS,10.48550/arXiv.2302.04761,N/A,https://aclanthology.org/L18-1544.pdf,Toolformer: Language Models Can Teach Themselves to Use Tools,evaluation,2,212,t-rex
ASDiv,NeurIPS,10.48550/arXiv.2302.04761,10.18653/v1/2020.acl-main.92,https://aclanthology.org/2020.acl-main.92.pdf,Toolformer: Language Models Can Teach Themselves to Use Tools,evaluation,2,212,asdiv
SVAMP,NeurIPS,10.48550/arXiv.2302.04761,10.18653/v1/2021.naacl-main.168,https://aclanthology.org/2021.naacl-main.168.pdf,Toolformer: Language Models Can Teach Themselves to Use Tools,evaluation,2,212,svamp
MAWPS,NeurIPS,10.48550/arXiv.2302.04761,10.18653/v1/N16-1136,https://aclanthology.org/N16-1136.pdf,Toolformer: Language Models Can Teach Themselves to Use Tools,evaluation,2,212,mawps
WQ,NeurIPS,10.48550/arXiv.2302.04761,N/A,https://aclanthology.org/D13-1160.pdf,Toolformer: Language Models Can Teach Themselves to Use Tools,evaluation,2,212,wq
NQ,NeurIPS,10.48550/arXiv.2302.04761,10.1162/tacl_a_00276,https://aclanthology.org/Q19-1026.pdf,Toolformer: Language Models Can Teach Themselves to Use Tools,evaluation,2,212,nq
TriviaQA,NeurIPS,10.48550/arXiv.2302.04761,10.18653/v1/P17-1147,https://aclanthology.org/P17-1147.pdf,Toolformer: Language Models Can Teach Themselves to Use Tools,evaluation,2,212,triviaqa
MLQA,NeurIPS,10.48550/arXiv.2302.04761,10.18653/v1/P19-1484,https://aclanthology.org/P19-1484.pdf,Toolformer: Language Models Can Teach Themselves to Use Tools,evaluation,2,212,mlqa
TempLAMA,NeurIPS,10.48550/arXiv.2302.04761,10.1162/tacl_a_00459,https://aclanthology.org/2022.tacl-1.15.pdf,Toolformer: Language Models Can Teach Themselves to Use Tools,evaluation,2,212,templama
DATESET,NeurIPS,10.48550/arXiv.2302.04761,10.48550/arXiv.2302.04761,https://arxiv.org/pdf/2302.04761,Toolformer: Language Models Can Teach Themselves to Use Tools,evaluation. dataset created by authors,2,212,dateset
WikiText-103,NeurIPS,10.48550/arXiv.2302.04761,10.48550/arXiv.1609.07843,https://arxiv.org/pdf/1609.07843,Toolformer: Language Models Can Teach Themselves to Use Tools,evaluation,2,212,wikitext-103
ETT,NeurIPS,10.48550/arXiv.2302.11939,10.1609/aaai.v35i12.17325,https://ojs.aaai.org/index.php/AAAI/article/view/17325,One Fits All: Power General Time Series Analysis by Pretrained LM,"imputation, long term forecasts. subsets: ETTh1, ETTh2, ETTm1, ETTm2",2,99,ett
ECL / Electricity,NeurIPS,10.48550/arXiv.2302.11939,10.24432/C58C86,https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014,One Fits All: Power General Time Series Analysis by Pretrained LM,imputation,2,99,ecl / electricity
Jena Weather,NeurIPS,10.48550/arXiv.2302.11939,N/A,https://www.bgc-jena.mpg.de/wetter/,One Fits All: Power General Time Series Analysis by Pretrained LM,"imputation, lomg term forecasts",2,99,jena weather
UEA,NeurIPS,10.48550/arXiv.2302.11939,10.48550/arXiv.1811.00075,https://arxiv.org/pdf/1811.00075,One Fits All: Power General Time Series Analysis by Pretrained LM,"time series classification. only 10 / 128 datasets are used, but only 4 are mentioned",2,99,uea
SMD,NeurIPS,10.48550/arXiv.2302.11939,10.1145/3292500.3330672,https://netman.aiops.org/wp-content/uploads/2019/08/OmniAnomaly_camera-ready.pdf,One Fits All: Power General Time Series Analysis by Pretrained LM,time series anomaly detection,2,99,smd
MSL,NeurIPS,10.48550/arXiv.2302.11939,10.1145/3219819.3219845,https://arxiv.org/pdf/1802.04431,One Fits All: Power General Time Series Analysis by Pretrained LM,time series anomaly detection,2,99,msl
SMAP,NeurIPS,10.48550/arXiv.2302.11939,10.1145/3219819.3219845,https://arxiv.org/pdf/1802.04431,One Fits All: Power General Time Series Analysis by Pretrained LM,time series anomaly detection,2,99,smap
SWaT,NeurIPS,10.48550/arXiv.2302.11939,10.1109/CySWater.2016.7469060,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/7469060,One Fits All: Power General Time Series Analysis by Pretrained LM,time series anomaly detection,2,99,swat
PSM,NeurIPS,10.48550/arXiv.2302.11939,10.1145/3447548.3467174,?,One Fits All: Power General Time Series Analysis by Pretrained LM,time series anomaly detection,2,99,psm
Traffic,NeurIPS,10.48550/arXiv.2302.11939,N/A,N/A,One Fits All: Power General Time Series Analysis by Pretrained LM,long term forecasts,2,99,traffic
ECL / Electricity,NeurIPS,10.48550/arXiv.2302.11939,10.24432/C58C86,https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014,One Fits All: Power General Time Series Analysis by Pretrained LM,long term forecasts,2,99,ecl / electricity
ILI,NeurIPS,10.48550/arXiv.2302.11939,N/A,https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html,One Fits All: Power General Time Series Analysis by Pretrained LM,long term forecasts,2,99,ili
M4,NeurIPS,10.48550/arXiv.2302.11939,10.1016/j.ijforecast.2018.06.001,https://www.researchgate.net/publication/325901666_The_M4_Competition_Results_findings_conclusion_and_way_forward,One Fits All: Power General Time Series Analysis by Pretrained LM,short term forecasts,2,99,m4
HotpotQA,NeurIPS,10.48550/arXiv.2303.11366,10.18653/v1/D18-1259,https://aclanthology.org/D18-1259.pdf,Reflexion: Language Agents with Verbal Reinforcement Learning,,2,288,hotpotqa
AlfWorld,NeurIPS,10.48550/arXiv.2303.11366,10.48550/arXiv.2010.03768,https://arxiv.org/pdf/2010.03768,Reflexion: Language Agents with Verbal Reinforcement Learning,,2,288,alfworld
HumanEval,NeurIPS,10.48550/arXiv.2303.11366,10.48550/arXiv.2107.03374,https://arxiv.org/pdf/2107.03374,Reflexion: Language Agents with Verbal Reinforcement Learning,,2,288,humaneval
MBPP,NeurIPS,10.48550/arXiv.2303.11366,10.48550/arXiv.2108.07732,https://arxiv.org/pdf/2108.07732,Reflexion: Language Agents with Verbal Reinforcement Learning,,2,288,mbpp
LeetCodeHardGym,NeurIPS,10.48550/arXiv.2303.11366,10.48550/arXiv.2303.11366,https://papers.nips.cc/paper_files/paper/2023/file/1b44b878bb782e6954cd888628510e90-Paper-Conference.pdf,Reflexion: Language Agents with Verbal Reinforcement Learning,dataset released in this paper,2,288,leetcodehardgym
"N/A, game of 24 dataset",NeurIPS,10.48550/arXiv.2303.11366,10.48550/arXiv.2305.10601,https://papers.nips.cc/paper_files/paper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf,Reflexion: Language Agents with Verbal Reinforcement Learning,made by authors,2,288,"n/a, game of 24 dataset"
"N/A, creative writing dataset",NeurIPS,10.48550/arXiv.2303.11366,10.48550/arXiv.2305.10602,https://papers.nips.cc/paper_files/paper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf,Reflexion: Language Agents with Verbal Reinforcement Learning,made by authors,2,288,"n/a, creative writing dataset"
"N/A, mini crossword dataset",NeurIPS,10.48550/arXiv.2303.11366,10.48550/arXiv.2305.10603,https://papers.nips.cc/paper_files/paper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf,Reflexion: Language Agents with Verbal Reinforcement Learning,made by authors,2,288,"n/a, mini crossword dataset"
GSM8K,NeurIPS,10.48550/arXiv.2303.11366,10.48550/arXiv.2110.14168,https://arxiv.org/pdf/2110.14168v2,Reflexion: Language Agents with Verbal Reinforcement Learning,they use it without referencing it,2,288,gsm8k
StrategyQA,NeurIPS,10.48550/arXiv.2303.11366,10.1162/tacl_a_00370,https://aclanthology.org/2021.tacl-1.21.pdf,Reflexion: Language Agents with Verbal Reinforcement Learning,they use it without referencing it,2,288,strategyqa
N/A 3,NeurIPS,10.48550/arXiv.2303.17580,10.48550/arXiv.2303.17580,https://papers.nips.cc/paper_files/paper/2023/file/77c33e6a367922d003ff102ffb92b658-Paper-Conference.pdf,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"made by authors, not named, gpt-annotated",2,141,n/a 3
N/A 4,NeurIPS,10.48550/arXiv.2303.17580,10.48550/arXiv.2303.17580,https://papers.nips.cc/paper_files/paper/2023/file/77c33e6a367922d003ff102ffb92b658-Paper-Conference.pdf,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"made by authors, not named, human-annotated",2,141,n/a 4
GSM8K,NeurIPS,10.48550/arXiv.2303.17651,10.48550/arXiv.2110.14168,https://arxiv.org/pdf/2110.14168v2,SELF-REFINE: Iterative Refinement with Self-Feedback,,2,214,gsm8k
FED,NeurIPS,10.48550/arXiv.2303.17651,10.18653/v1/2020.sigdial-1.28,https://arxiv.org/pdf/2006.12719,SELF-REFINE: Iterative Refinement with Self-Feedback,,2,214,fed
DB Pedia,NeurIPS,10.48550/arXiv.2303.17651,10.48550/arXiv.1509.01626,https://arxiv.org/pdf/1509.01626,SELF-REFINE: Iterative Refinement with Self-Feedback,,2,214,db pedia
Amazon-2,NeurIPS,10.48550/arXiv.2303.17651,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,SELF-REFINE: Iterative Refinement with Self-Feedback,,2,214,amazon-2
Amazon-5,NeurIPS,10.48550/arXiv.2303.17651,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,SELF-REFINE: Iterative Refinement with Self-Feedback,,2,214,amazon-5
Yelp-2,NeurIPS,10.48550/arXiv.2303.17651,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,SELF-REFINE: Iterative Refinement with Self-Feedback,,2,214,yelp-2
Yelp-5,NeurIPS,10.48550/arXiv.2303.17651,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,SELF-REFINE: Iterative Refinement with Self-Feedback,,2,214,yelp-5
AG's News,NeurIPS,10.48550/arXiv.2303.17651,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,SELF-REFINE: Iterative Refinement with Self-Feedback,,2,214,ag's news
Sogou,NeurIPS,10.48550/arXiv.2303.17651,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,SELF-REFINE: Iterative Refinement with Self-Feedback,,2,214,sogou
Yahoo answers,NeurIPS,10.48550/arXiv.2303.17651,10.48550/arXiv.1509.01626,https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,SELF-REFINE: Iterative Refinement with Self-Feedback,,2,214,yahoo answers
PIE,NeurIPS,10.48550/arXiv.2303.17651,10.48550/arXiv.2302.07867,https://arxiv.org/pdf/2302.07867,SELF-REFINE: Iterative Refinement with Self-Feedback,,2,214,pie
CodeNet,NeurIPS,10.48550/arXiv.2303.17651,10.48550/arXiv.2105.12655,https://arxiv.org/pdf/2105.12655,SELF-REFINE: Iterative Refinement with Self-Feedback,,2,214,codenet
acronyms,NeurIPS,10.48550/arXiv.2303.17651,N/A,https://github.com/problemsniper/Crawl-Wiki-For-Acronyms/blob/master/AcronymsFile.csv,SELF-REFINE: Iterative Refinement with Self-Feedback,"they filter it, check appendix q",2,214,acronyms
CommonGen-Hard,NeurIPS,10.48550/arXiv.2303.17651,10.48550/arXiv.2303.17651,https://arxiv.org/pdf/2303.17651,SELF-REFINE: Iterative Refinement with Self-Feedback,"a more compex version of CommonGen, check paper for details",2,214,commongen-hard
COCO,NeurIPS,10.48550/arXiv.2304.06718,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Segment Everything Everywhere All at Once,"referred to as COCO2017, same reference",2,124,coco
Ref-COCOg,NeurIPS,10.48550/arXiv.2304.06718,10.48550/arXiv.1608.00272,https://arxiv.org/pdf/1608.00272,Segment Everything Everywhere All at Once,,2,124,ref-cocog
Ref-COCO+,NeurIPS,10.48550/arXiv.2304.06718,10.48550/arXiv.1608.00272,https://arxiv.org/pdf/1608.00272,Segment Everything Everywhere All at Once,,2,124,ref-coco+
Ref-COCO,NeurIPS,10.48550/arXiv.2304.06718,10.48550/arXiv.1608.00272,https://arxiv.org/pdf/1608.00272,Segment Everything Everywhere All at Once,,2,124,ref-coco
LVIS,NeurIPS,10.48550/arXiv.2304.06718,10.1109/CVPR.2019.00550,https://ieeexplore.ieee.org/document/8954457,Segment Everything Everywhere All at Once,"not used as is - ""We replace the COCO mask with an overlap IoU larger than 0.7 with LVIS mask during training""",2,124,lvis
DAVIS,NeurIPS,10.48550/arXiv.2304.06718,10.1109/CVPR.2016.85,https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Perazzi_A_Benchmark_Dataset_CVPR_2016_paper.pdf,Segment Everything Everywhere All at Once,"DAVIS2017, DAVIS2016-Interactive",2,124,davis
Youtube-VOS,NeurIPS,10.48550/arXiv.2304.06718,10.48550/arXiv.1809.03327,https://arxiv.org/pdf/1809.03327,Segment Everything Everywhere All at Once,,2,124,youtube-vos
DAVIS2017,NeurIPS,10.48550/arXiv.2304.06718,10.48550/arXiv.1704.00675,https://arxiv.org/pdf/1704.00675,Segment Everything Everywhere All at Once,,2,124,davis2017
DAVIS2016,NeurIPS,10.48550/arXiv.2304.06718,10.48550/arXiv.1704.00675,https://arxiv.org/pdf/1704.00675,Segment Everything Everywhere All at Once,,2,124,davis2016
YouCook2,NeurIPS,10.48550/arXiv.2304.06718,10.1609/aaai.v32i1.12342,https://cdn.aaai.org/ojs/12342/12342-13-15870-1-2-20201228.pdf,Segment Everything Everywhere All at Once,,2,124,youcook2
OpenImages,NeurIPS,10.48550/arXiv.2304.06718,10.48550/arXiv.1811.00982,https://arxiv.org/pdf/1811.00982,Segment Everything Everywhere All at Once,"used, not referenced",2,124,openimages
PASCAL VOC,NeurIPS,10.48550/arXiv.2304.06718,10.1007/s11263-009-0275-4,https://link.springer.com/article/10.1007/s11263-009-0275-4#preview,Segment Everything Everywhere All at Once,"used, not referenced",2,124,pascal voc
OpenAssistant Conversations,NeurIPS,10.48550/arXiv.2304.07327,10.48550/arXiv.2304.07327,https://papers.nips.cc/paper_files/paper/2023/file/949f0f8f32267d297c2d4e3ee10a2e7e-Paper-Datasets_and_Benchmarks.pdf,OpenAssistant Conversations - Democratizing Large Language Model Alignment,Dataset is released in this paper,2,86,openassistant conversations
ScienceQA,NeurIPS,10.48550/arXiv.2304.08485,10.48550/arXiv.2209.09513,https://arxiv.org/pdf/2209.09513,Visual Instruction Tuning,,2,740,scienceqa
COCO,NeurIPS,10.48550/arXiv.2304.08485,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Visual Instruction Tuning,,2,740,coco
CC-595K,NeurIPS,10.48550/arXiv.2304.08485,10.48550/arXiv.2304.08485,https://arxiv.org/pdf/2304.08485,Visual Instruction Tuning,subset of CC3M made by authors,2,740,cc-595k
LLaVA-Instruct-158K,NeurIPS,10.48550/arXiv.2304.08485,10.48550/arXiv.2304.08485,https://arxiv.org/pdf/2304.08485,Visual Instruction Tuning,dataset released in this paper,2,740,llava-instruct-158k
LLAVA-Bench,NeurIPS,10.48550/arXiv.2304.08485,10.48550/arXiv.2304.08485,https://arxiv.org/pdf/2304.08485,Visual Instruction Tuning,dataset released in this paper,2,740,llava-bench
HumanEval,NeurIPS,10.48550/arXiv.2305.01210,10.48550/arXiv.2107.03374,https://arxiv.org/pdf/2107.03374,Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation,,2,113,humaneval
HumanEval+,NeurIPS,10.48550/arXiv.2305.01210,10.48550/arXiv.2305.01210,https://arxiv.org/pdf/2305.01210,Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation,made by authors from HumanEval,2,113,humaneval+
HumanEval+-Mini,NeurIPS,10.48550/arXiv.2305.01210,10.48550/arXiv.2305.01210,https://arxiv.org/pdf/2305.01210,Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation,made by authors from HumanEval,2,113,humaneval+-mini
COCO Captions,NeurIPS,10.48550/arXiv.2305.06500,10.48550/arXiv.1504.00325,https://arxiv.org/pdf/1504.00325,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,training,2,213,coco captions
Web CapFilt,NeurIPS,10.48550/arXiv.2305.06500,10.48550/arXiv.2305.06500,https://papers.nips.cc/paper_files/paper/2023/file/9a6a435e75419a836fe47ab6793623e6-Paper-Conference.pdf,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,training,2,213,web capfilt
NoCaps,NeurIPS,10.48550/arXiv.2305.06500,10.1109/ICCV.2019.00904,https://openaccess.thecvf.com/content_ICCV_2019/papers/Agrawal_nocaps_novel_object_captioning_at_scale_ICCV_2019_paper.pdf,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,evaluation,2,213,nocaps
Flickr30k,NeurIPS,10.48550/arXiv.2305.06500,10.1162/tacl_a_00166,https://aclanthology.org/Q14-1006.pdf,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,evaluation,2,213,flickr30k
TextCaps,NeurIPS,10.48550/arXiv.2305.06500,10.1007/978-3-030-58536-5_44,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470732.pdf,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,training,2,213,textcaps
VQA 2.0,NeurIPS,10.48550/arXiv.2305.06500,10.1109/CVPR.2017.670,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8100153&tag=1,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,training,2,213,vqa 2.0
VizWiz,NeurIPS,10.48550/arXiv.2305.06500,10.48550/arXiv.1802.08218,https://arxiv.org/pdf/1802.08218,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,evaluation,2,213,vizwiz
GQA,NeurIPS,10.48550/arXiv.2305.06500,10.1109/CVPR.2019.00686,https://arxiv.org/pdf/1902.09506,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,evaluation,2,213,gqa
VSR,NeurIPS,10.48550/arXiv.2305.06500,10.1162/tacl_a_00566,https://aclanthology.org/2023.tacl-1.37.pdf,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,evaluation,2,213,vsr
IconQA,NeurIPS,10.48550/arXiv.2305.06500,10.48550/arXiv.2110.13214,https://arxiv.org/pdf/2110.13214,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,evaluation,2,213,iconqa
OKVQA,NeurIPS,10.48550/arXiv.2305.06500,10.48550/arXiv.1906.00067,https://arxiv.org/pdf/1906.00067,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,training,2,213,okvqa
A-OKVQA,NeurIPS,10.48550/arXiv.2305.06500,10.1007/978-3-031-20074-8_9,https://link.springer.com/chapter/10.1007/978-3-031-20074-8_9,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,training,2,213,a-okvqa
ScienceQA,NeurIPS,10.48550/arXiv.2305.06500,10.48550/arXiv.2209.09513,https://arxiv.org/pdf/2209.09513,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,evaluation,2,213,scienceqa
VisDial,NeurIPS,10.48550/arXiv.2305.06500,10.48550/arXiv.1611.08669,https://arxiv.org/pdf/1611.08669,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,evaluation,2,213,visdial
OCR-VQA,NeurIPS,10.48550/arXiv.2305.06500,10.1109/ICDAR.2019.00156,?,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,training,2,213,ocr-vqa
TextVQA,NeurIPS,10.48550/arXiv.2305.06500,10.48550/arXiv.1904.08920,https://arxiv.org/pdf/1904.08920,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,evaluation,2,213,textvqa
HatefulMemes,NeurIPS,10.48550/arXiv.2305.06500,10.48550/arXiv.2005.04790,https://arxiv.org/pdf/2005.04790,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,evaluation,2,213,hatefulmemes
LLaVA-Instruct-158K,NeurIPS,10.48550/arXiv.2305.06500,10.48550/arXiv.2304.08485,https://arxiv.org/pdf/2304.08485,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,"training. paper mentions instruct-150k instead of 158, but this is the dataset found in the reference",2,213,llava-instruct-158k
MSVD-QA,NeurIPS,10.48550/arXiv.2305.06500,10.1145/3123266.3123427,http://staff.ustc.edu.cn/~hexn/papers/mm17-videoQA.pdf,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,evaluation,2,213,msvd-qa
MSRVTT-QA,NeurIPS,10.48550/arXiv.2305.06500,10.1145/3123266.3123428,http://staff.ustc.edu.cn/~hexn/papers/mm17-videoQA.pdf,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,evaluation,2,213,msrvtt-qa
iVQA,NeurIPS,10.48550/arXiv.2305.06500,10.48550/arXiv.2012.00451,https://arxiv.org/pdf/2012.00451,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,evaluation,2,213,ivqa
SuperNI,NeurIPS,10.48550/arXiv.2305.11206,10.18653/v1/2022.emnlp-main.340,https://aclanthology.org/2022.emnlp-main.340.pdf,LIMA: Less Is More for Alignment,subset manually curated,2,175,superni
StackExchange STEM,NeurIPS,10.48550/arXiv.2305.11206,10.48550/arXiv.2305.11206,https://arxiv.org/pdf/2305.11206,LIMA: Less Is More for Alignment,self-made,2,175,stackexchange stem
StackExchange Other,NeurIPS,10.48550/arXiv.2305.11206,10.48550/arXiv.2305.11207,https://arxiv.org/pdf/2305.11206,LIMA: Less Is More for Alignment,self-made,2,175,stackexchange other
wikihow,NeurIPS,10.48550/arXiv.2305.11206,10.48550/arXiv.2305.11208,https://arxiv.org/pdf/2305.11206,LIMA: Less Is More for Alignment,self-made,2,175,wikihow
Paper Authors A,NeurIPS,10.48550/arXiv.2305.11206,10.48550/arXiv.2305.11209,https://arxiv.org/pdf/2305.11206,LIMA: Less Is More for Alignment,self-made,2,175,paper authors a
Paper Authors B,NeurIPS,10.48550/arXiv.2305.11206,10.48550/arXiv.2305.11210,https://arxiv.org/pdf/2305.11206,LIMA: Less Is More for Alignment,self-made,2,175,paper authors b
PushShift Reddit,NeurIPS,10.48550/arXiv.2305.11206,10.1609/icwsm.v14i1.7347 ,https://ojs.aaai.org/index.php/ICWSM/article/view/7347/7201,LIMA: Less Is More for Alignment,they extract some samples from r/WritingPrompts and r/AskReddit,2,175,pushshift reddit
BoolQ,NeurIPS,10.48550/arXiv.2305.11627,10.18653/v1/N19-1300,https://aclanthology.org/N19-1300.pdf,LLM-Pruner: On the Structural Pruning of Large Language Models,,2,134,boolq
PIQA,NeurIPS,10.48550/arXiv.2305.11627,10.1609/aaai.v34i05.6239,https://ojs.aaai.org/index.php/AAAI/article/view/6239,LLM-Pruner: On the Structural Pruning of Large Language Models,,2,134,piqa
HellaSwag,NeurIPS,10.48550/arXiv.2305.11627,10.18653/v1/P19-1472,https://aclanthology.org/P19-1472.pdf,LLM-Pruner: On the Structural Pruning of Large Language Models,,2,134,hellaswag
WinoGrande,NeurIPS,10.48550/arXiv.2305.11627,10.1145/3474381,https://arxiv.org/pdf/1907.10641,LLM-Pruner: On the Structural Pruning of Large Language Models,,2,134,winogrande
ARC-easy,NeurIPS,10.48550/arXiv.2305.11627,10.48550/arXiv.1803.05457,https://arxiv.org/pdf/1803.05457,LLM-Pruner: On the Structural Pruning of Large Language Models,,2,134,arc-easy
ARC-challenge,NeurIPS,10.48550/arXiv.2305.11627,10.48550/arXiv.1803.05457,https://arxiv.org/pdf/1803.05457,LLM-Pruner: On the Structural Pruning of Large Language Models,,2,134,arc-challenge
OpenBookQA,NeurIPS,10.48550/arXiv.2305.11627,10.18653/v1/D18-1260,https://aclanthology.org/D18-1260.pdf,LLM-Pruner: On the Structural Pruning of Large Language Models,,2,134,openbookqa
WikiText-2,NeurIPS,10.48550/arXiv.2305.11627,10.48550/arXiv.1609.07843,https://arxiv.org/pdf/1609.07843,LLM-Pruner: On the Structural Pruning of Large Language Models,,2,134,wikitext-2
PTB,NeurIPS,10.48550/arXiv.2305.11627,N/A,https://aclanthology.org/J93-2004/,LLM-Pruner: On the Structural Pruning of Large Language Models,,2,134,ptb
BookCorpus,NeurIPS,10.48550/arXiv.2305.11627,10.1109/ICCV.2015.11,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=7410368,LLM-Pruner: On the Structural Pruning of Large Language Models,10 rand. selected samples,2,134,bookcorpus
DailyDialog,NeurIPS,10.48550/arXiv.2305.11627,10.48550/arXiv.1710.03957,https://arxiv.org/pdf/1710.03957,LLM-Pruner: On the Structural Pruning of Large Language Models,10 rand. samples,2,134,dailydialog
Alpaca,NeurIPS,10.48550/arXiv.2305.11627,N/A,https://crfm.stanford.edu/2023/03/13/alpaca.html,LLM-Pruner: On the Structural Pruning of Large Language Models,,2,134,alpaca
LaMini,NeurIPS,10.48550/arXiv.2305.11627,10.48550/arXiv.2304.14402,https://arxiv.org/pdf/2304.14402,LLM-Pruner: On the Structural Pruning of Large Language Models,more training for recovery,2,134,lamini
Alpaca,NeurIPS,10.48550/arXiv.2305.14314,N/A,https://crfm.stanford.edu/2023/03/13/alpaca.html,QLORA: Efficient Finetuning of Quantized LLMs,,2,504,alpaca
Self-instruct,NeurIPS,10.48550/arXiv.2305.14314,10.18653/v1/2023.acl-long.754,https://arxiv.org/pdf/2212.10560,QLORA: Efficient Finetuning of Quantized LLMs,,2,504,self-instruct
Unnatural Instructions,NeurIPS,10.48550/arXiv.2305.14314,10.48550/arXiv.2212.09689,https://arxiv.org/pdf/2212.09689,QLORA: Efficient Finetuning of Quantized LLMs,,2,504,unnatural instructions
FLAN v2,NeurIPS,10.48550/arXiv.2305.14314,10.48550/arXiv.2301.13688,https://arxiv.org/pdf/2301.13688,QLORA: Efficient Finetuning of Quantized LLMs,,2,504,flan v2
OpenAssistant Conversations,NeurIPS,10.48550/arXiv.2305.14314,10.48550/arXiv.2304.07327,https://papers.nips.cc/paper_files/paper/2023/file/949f0f8f32267d297c2d4e3ee10a2e7e-Paper-Datasets_and_Benchmarks.pdf,QLORA: Efficient Finetuning of Quantized LLMs,,2,504,openassistant conversations
HH-RLHF,NeurIPS,10.48550/arXiv.2305.14314,10.48550/arXiv.2204.05862,https://arxiv.org/pdf/2204.05862,QLORA: Efficient Finetuning of Quantized LLMs,,2,504,hh-rlhf
LongForm,NeurIPS,10.48550/arXiv.2305.14314,10.48550/arXiv.2304.08460,https://arxiv.org/pdf/2304.08460,QLORA: Efficient Finetuning of Quantized LLMs,,2,504,longform
GLUE,NeurIPS,10.48550/arXiv.2305.14314,10.18653/v1/W18-5446,https://aclanthology.org/W18-5446.pdf,QLORA: Efficient Finetuning of Quantized LLMs,used to train another model for comparison,2,504,glue
SuperNI,NeurIPS,10.48550/arXiv.2305.14314,10.18653/v1/2022.emnlp-main.340,https://aclanthology.org/2022.emnlp-main.340.pdf,QLORA: Efficient Finetuning of Quantized LLMs,used to train another model for comparison,2,504,superni
MMLU,NeurIPS,10.48550/arXiv.2305.14314,10.48550/arXiv.2009.03300,https://arxiv.org/pdf/2009.03300,QLORA: Efficient Finetuning of Quantized LLMs,,2,504,mmlu
VIcuna prompts,NeurIPS,10.48550/arXiv.2305.14314,N/A,https://lmsys.org/blog/2023-03-30-vicuna/,QLORA: Efficient Finetuning of Quantized LLMs,????,2,504,vicuna prompts
CrowS-Pairs,NeurIPS,10.48550/arXiv.2305.14314,10.18653/v1/2020.emnlp-main.154,https://aclanthology.org/2020.emnlp-main.154.pdf,QLORA: Efficient Finetuning of Quantized LLMs,"they just mention it in a table description, no other mention or reference whtasoever, i just assumed it is this one",2,504,crows-pairs
Chip2,NeurIPS,10.48550/arXiv.2305.14314,N/A,https://github.com/LAION-AI/Open-Instruction-Generalist,QLORA: Efficient Finetuning of Quantized LLMs,,2,504,chip2
IMDB-,NeurIPS,10.48550/arXiv.2305.18290,N/A,https://aclanthology.org/P11-1015.pdf,Direct Preference Optimization: Your Language Model is Secretly a Reward Model,,2,531,imdb-
Webis-TLDR-17,NeurIPS,10.48550/arXiv.2305.18290,10.18653/v1/W17-4508,https://aclanthology.org/W17-4508.pdf,Direct Preference Optimization: Your Language Model is Secretly a Reward Model,,2,531,webis-tldr-17
N/A,NeurIPS,10.48550/arXiv.2305.18290,10.48550/arXiv.2009.01325,https://arxiv.org/pdf/2009.01325,Direct Preference Optimization: Your Language Model is Secretly a Reward Model,,2,531,n/a
HH-RLHF,NeurIPS,10.48550/arXiv.2305.18290,10.48550/arXiv.2204.05862,https://arxiv.org/pdf/2204.05862,Direct Preference Optimization: Your Language Model is Secretly a Reward Model,,2,531,hh-rlhf
CNN/DM modified,NeurIPS,10.48550/arXiv.2305.18290,10.18653/v1/K16-1028,https://aclanthology.org/K16-1028.pdf,Direct Preference Optimization: Your Language Model is Secretly a Reward Model,,2,531,cnn/dm modified
PMC-15M,NeurIPS,10.48550/arXiv.2306.00890,10.48550/arXiv.2303.00915,https://arxiv.org/pdf/2303.00915,LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day,"used to generate a few gpt-annotated datasets by the authors. (60K-IM, 60K, 10K, and an unnamed one)",2,142,pmc-15m
VQA-RAD,NeurIPS,10.48550/arXiv.2306.00890,10.1038/sdata.2018.251,https://www.nature.com/articles/sdata2018251,LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day,,2,142,vqa-rad
Slake,NeurIPS,10.48550/arXiv.2306.00890,10.1109/ISBI48211.2021.9434010,https://arxiv.org/pdf/2102.09542,LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day,,2,142,slake
PathVQA,NeurIPS,10.48550/arXiv.2306.00890,10.48550/arXiv.2003.10286,https://arxiv.org/pdf/2003.10286,LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day,,2,142,pathvqa
DIS,NeurIPS,10.48550/arXiv.2306.01567,10.1007/978-3-031-19797-0_3,https://arxiv.org/pdf/2203.03041,Segment Anything in High Quality,maybe worth mentioning that the authors merged these into one big dataset which they called HQSeg-44K,2,114,dis
ThinObject-5K,NeurIPS,10.48550/arXiv.2306.01567,N/A,https://openaccess.thecvf.com/content/WACV2021/papers/Liew_Deep_Interactive_Thin_Object_Selection_WACV_2021_paper.pdf,Segment Anything in High Quality,maybe worth mentioning that the authors merged these into one big dataset which they called HQSeg-44K,2,114,thinobject-5k
FSS-1000,NeurIPS,10.48550/arXiv.2306.01567,10.1109/CVPR42600.2020.00294,https://arxiv.org/pdf/1907.12347,Segment Anything in High Quality,maybe worth mentioning that the authors merged these into one big dataset which they called HQSeg-44K,2,114,fss-1000
ECSSD,NeurIPS,10.48550/arXiv.2306.01567,10.1109/TPAMI.2015.2465960,https://arxiv.org/pdf/1408.5418,Segment Anything in High Quality,maybe worth mentioning that the authors merged these into one big dataset which they called HQSeg-44K,2,114,ecssd
MSRA-10K,NeurIPS,10.48550/arXiv.2306.01567,10.1109/TPAMI.2014.2345401,https://mmcheng.net/mftp/Papers/SaliencyTPAMI.pdf,Segment Anything in High Quality,maybe worth mentioning that the authors merged these into one big dataset which they called HQSeg-44K,2,114,msra-10k
DUT-OMRON,NeurIPS,10.48550/arXiv.2306.01567,10.1109/CVPR.2013.407,https://openaccess.thecvf.com/content_cvpr_2013/papers/Yang_Saliency_Detection_via_2013_CVPR_paper.pdf,Segment Anything in High Quality,maybe worth mentioning that the authors merged these into one big dataset which they called HQSeg-44K,2,114,dut-omron
COIFT,NeurIPS,10.48550/arXiv.2306.01567,N/A,https://openaccess.thecvf.com/content/WACV2021/papers/Liew_Deep_Interactive_Thin_Object_Selection_WACV_2021_paper.pdf,Segment Anything in High Quality,,2,114,coift
HR-SOD,NeurIPS,10.48550/arXiv.2306.01567,10.48550/arXiv.1908.07274,https://openaccess.thecvf.com/content_ICCV_2019/papers/Zeng_Towards_High-Resolution_Salient_Object_Detection_ICCV_2019_paper.pdf,Segment Anything in High Quality,,2,114,hr-sod
COCO,NeurIPS,10.48550/arXiv.2306.01567,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Segment Anything in High Quality,,2,114,coco
SGInW,NeurIPS,10.48550/arXiv.2306.01567,10.48550/arXiv.2212.11270,https://openaccess.thecvf.com/content/CVPR2023/papers/Zou_Generalized_Decoding_for_Pixel_Image_and_Language_CVPR_2023_paper.pdf,Segment Anything in High Quality,,2,114,sginw
UVO,NeurIPS,10.48550/arXiv.2306.01567,10.1109/ICCV48922.2021.01060,https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Unidentified_Video_Objects_A_Benchmark_for_Dense_Open-World_Segmentation_ICCV_2021_paper.pdf,Segment Anything in High Quality,,2,114,uvo
LVIS,NeurIPS,10.48550/arXiv.2306.01567,10.1109/CVPR.2019.00550,https://ieeexplore.ieee.org/document/8954457,Segment Anything in High Quality,,2,114,lvis
HQ-YTVIS,NeurIPS,10.48550/arXiv.2306.01567,10.1007/978-3-031-19815-1_42,https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136880721.pdf,Segment Anything in High Quality,,2,114,hq-ytvis
BIG,NeurIPS,10.48550/arXiv.2306.01567,10.48550/arXiv.2005.02551,https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_CascadePSP_Toward_Class-Agnostic_and_Very_High-Resolution_Segmentation_via_Global_and_CVPR_2020_paper.pdf,Segment Anything in High Quality,,2,114,big
ShutterStock Music,NeurIPS,10.48550/arXiv.2306.05284,N/A,https://www.shutterstock.com/music,Simple and Controllable Music Generation,training,2,88,shutterstock music
Pond5,NeurIPS,10.48550/arXiv.2306.05284,?,https://www.pond5.com/,Simple and Controllable Music Generation,training,2,88,pond5
MusicCaps,NeurIPS,10.48550/arXiv.2306.05284,10.48550/arXiv.2301.11325,https://arxiv.org/pdf/2301.11325,Simple and Controllable Music Generation,evaluation,2,88,musiccaps
N/A music,NeurIPS,10.48550/arXiv.2306.05284,10.48550/arXiv.2306.05284,https://arxiv.org/pdf/2306.05284,Simple and Controllable Music Generation,"this paper uses 2 datasets (1 for training, 1 for validation) which are only briefly mentioned and claimed to be internal and not accessible",2,88,n/a music
MT-Bench,NeurIPS,10.48550/arXiv.2306.05685,10.48550/arXiv.2306.05685,https://papers.nips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf,Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,dataset released by this paper,2,516,mt-bench
Chatbot Arena,NeurIPS,10.48550/arXiv.2306.05685,10.48550/arXiv.2306.05686,https://papers.nips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf,Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,dataset released by this paper,2,516,chatbot arena
N/A 1,NeurIPS,10.48550/arXiv.2307.02483,10.48550/arXiv.2307.02483,https://papers.nips.cc/paper_files/paper/2023/file/fd6613131889a4b656206c50a8bd7790-Paper-Conference.pdf,Jailbroken: How Does LLM Safety Training Fail?,"made by authors, not named",2,115,n/a 1
N/A 2,NeurIPS,10.48550/arXiv.2307.02483,10.48550/arXiv.2307.02483,https://papers.nips.cc/paper_files/paper/2023/file/fd6613131889a4b656206c50a8bd7790-Paper-Conference.pdf,Jailbroken: How Does LLM Safety Training Fail?,"made by authors, not named",2,115,n/a 2
COCO,NeurIPS,10.48550/arXiv.2309.11331,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism,mentions COCO 2017 but i'm not yet sure if it's the same or not,2,149,coco
ImageNet-1K,NeurIPS,10.48550/arXiv.2309.11331,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism,,2,149,imagenet-1k
Monash,NeurIPS,10.48550/arXiv.2310.07820,10.48550/arXiv.2105.06643,https://arxiv.org/pdf/2105.06643,Large Language Models Are Zero-Shot Time Series Forecasters,"evaluation / benchmarking. out of the 30 datasets, the paper used 19 (see appendix C.2)",2,91,monash
AirPassengers,NeurIPS,10.48550/arXiv.2310.07820,10.48550/arXiv.2110.03225,https://arxiv.org/pdf/2110.03224,Large Language Models Are Zero-Shot Time Series Forecasters,evaluation / benchmarking,2,91,airpassengers
AusBeer,NeurIPS,10.48550/arXiv.2310.07820,10.48550/arXiv.2110.03226,https://arxiv.org/pdf/2110.03224,Large Language Models Are Zero-Shot Time Series Forecasters,evaluation / benchmarking,2,91,ausbeer
GasRateCO2,NeurIPS,10.48550/arXiv.2310.07820,10.48550/arXiv.2110.03227,https://arxiv.org/pdf/2110.03224,Large Language Models Are Zero-Shot Time Series Forecasters,evaluation / benchmarking,2,91,gasrateco2
MonthlyMilk,NeurIPS,10.48550/arXiv.2310.07820,10.48550/arXiv.2110.03228,https://arxiv.org/pdf/2110.03224,Large Language Models Are Zero-Shot Time Series Forecasters,evaluation / benchmarking,2,91,monthlymilk
SunSpots,NeurIPS,10.48550/arXiv.2310.07820,10.48550/arXiv.2110.03229,https://arxiv.org/pdf/2110.03224,Large Language Models Are Zero-Shot Time Series Forecasters,evaluation / benchmarking,2,91,sunspots
Wine,NeurIPS,10.48550/arXiv.2310.07820,10.48550/arXiv.2110.03230,https://arxiv.org/pdf/2110.03224,Large Language Models Are Zero-Shot Time Series Forecasters,evaluation / benchmarking,2,91,wine
Wooly,NeurIPS,10.48550/arXiv.2310.07820,10.48550/arXiv.2110.03231,https://arxiv.org/pdf/2110.03224,Large Language Models Are Zero-Shot Time Series Forecasters,evaluation / benchmarking,2,91,wooly
HeartRate,NeurIPS,10.48550/arXiv.2310.07820,10.48550/arXiv.2110.03232,https://arxiv.org/pdf/2110.03224,Large Language Models Are Zero-Shot Time Series Forecasters,evaluation / benchmarking,2,91,heartrate
ETT,NeurIPS,10.48550/arXiv.2310.07820,10.1609/aaai.v35i12.17325,https://ojs.aaai.org/index.php/AAAI/article/view/17325,Large Language Models Are Zero-Shot Time Series Forecasters,only ETTm2. evaluation / benchmarking,2,91,ett
Exchange-rate,NeurIPS,10.48550/arXiv.2310.07820,10.48550/arXiv.1703.07015,https://arxiv.org/pdf/1703.07015,Large Language Models Are Zero-Shot Time Series Forecasters,"they say it is a dataset from the Informer paper, but it's not there?? i believe it is this one though",2,91,exchange-rate
ECL / Electricity,NeurIPS,10.48550/arXiv.2310.07820,10.24432/C58C86,https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014,Large Language Models Are Zero-Shot Time Series Forecasters,evaluation / benchmarking,2,91,ecl / electricity
Traffic,NeurIPS,10.48550/arXiv.2310.07820,N/A,N/A,Large Language Models Are Zero-Shot Time Series Forecasters,"they say it is a dataset from the Informer paper, but it's not there?? i believe it is this one though",2,91,traffic
US Weather,NeurIPS,10.48550/arXiv.2310.07820,N/A,https://www.ncei.noaa.gov/data/local-climatological-data/,Large Language Models Are Zero-Shot Time Series Forecasters,evaluation / benchmarking,2,91,us weather
Istanbul Traffic,NeurIPS,10.48550/arXiv.2310.07820,?,https://www.kaggle.com/datasets/leonardo00/istanbul-traffic-index,Large Language Models Are Zero-Shot Time Series Forecasters,evaluation / benchmarking,2,91,istanbul traffic
TSMC Stock,NeurIPS,10.48550/arXiv.2310.07820,N/A,https://www.kaggle.com/datasets/yeemeitsang/tsmc-stock-exchange-2022,Large Language Models Are Zero-Shot Time Series Forecasters,evaluation / benchmarking,2,91,tsmc stock
Turkey Power,NeurIPS,10.48550/arXiv.2310.07820,N/A,https://www.kaggle.com/datasets/dharanikra/electrical-power-demand-in-turkey,Large Language Models Are Zero-Shot Time Series Forecasters,evaluation / benchmarking,2,91,turkey power
N/A Synthetic,NeurIPS,10.48550/arXiv.2310.07820,10.48550/arXiv.2310.07820,https://arxiv.org/pdf/2310.07820,Large Language Models Are Zero-Shot Time Series Forecasters,,2,91,n/a synthetic
Solar,NeurIPS,10.48550/arXiv.2311.06184,N/A,https://www2.nrel.gov/grid/solar-power-data,Frequency-domain MLPs are More Effective Learners in Time Series Forecasting,short term,2,82,solar
Wiki,NeurIPS,10.48550/arXiv.2311.06184,N/A,https://www.kaggle.com/c/web-traffic-time-series-forecasting/data,Frequency-domain MLPs are More Effective Learners in Time Series Forecasting,short term,2,82,wiki
Traffic-Old,NeurIPS,10.48550/arXiv.2311.06184,https://dl-acm-org.tudelft.idm.oclc.org/doi/10.5555/3104482.3104599,http://www.icml-2011.org/papers/489_icmlpaper.pdf,Frequency-domain MLPs are More Effective Learners in Time Series Forecasting,short term. subset,2,82,traffic-old
ECL / Electricity,NeurIPS,10.48550/arXiv.2311.06184,10.24432/C58C86,https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014,Frequency-domain MLPs are More Effective Learners in Time Series Forecasting,both short-term and long-term,2,82,ecl / electricity
ECG,NeurIPS,10.48550/arXiv.2311.06184,N/A,https://www.cs.ucr.edu/%7Eeamonn/time_series_data/,Frequency-domain MLPs are More Effective Learners in Time Series Forecasting,short term,2,82,ecg
METR-LA,NeurIPS,10.48550/arXiv.2311.06184,https://openreview.net/forum?id=SJiHXGWAZ,https://openreview.net/pdf?id=SJiHXGWAZ,Frequency-domain MLPs are More Effective Learners in Time Series Forecasting,short term,2,82,metr-la
COVID-19,NeurIPS,10.48550/arXiv.2311.06184,N/A,https://openreview.net/pdf?id=wv6g8fWLX2q,Frequency-domain MLPs are More Effective Learners in Time Series Forecasting,short term,2,82,covid-19
Jena Weather,NeurIPS,10.48550/arXiv.2311.06184,N/A,https://www.bgc-jena.mpg.de/wetter/,Frequency-domain MLPs are More Effective Learners in Time Series Forecasting,long term,2,82,jena weather
Exchange-Rate,NeurIPS,10.48550/arXiv.2311.06184,10.48550/arXiv.1703.07015,https://arxiv.org/pdf/1703.07015,Frequency-domain MLPs are More Effective Learners in Time Series Forecasting,"both short-term and long-term, but they referenced the same dataset differently for the 2 instances",2,82,exchange-rate
Traffic,NeurIPS,10.48550/arXiv.2311.06184,N/A,N/A,Frequency-domain MLPs are More Effective Learners in Time Series Forecasting,long term; period of data collection not mentioned,2,82,traffic
ETT,NeurIPS,10.48550/arXiv.2311.06184,10.1609/aaai.v35i12.17325,https://ojs.aaai.org/index.php/AAAI/article/view/17325,Frequency-domain MLPs are More Effective Learners in Time Series Forecasting,"long term,  ETTh1 and ETTm1 ",2,82,ett
SpatialEval,NeurIPS,10.48550/arXiv.2406.14852,10.48550/arXiv.2406.14852,10.48550/arXiv.2406.14852,Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models,Dataset introduced by the same paper,5,"1,256",spatialeval
Moving-MNIST,NeurIPS,10.48550/arXiv.1506.04214,10.48550/arXiv.1502.04681,https://arxiv.org/pdf/1502.04681v3,Convolutional LSTM network: A machine learning approach for precipitation nowcasting,broken link of the dataset,15,"6,900",moving-mnist
Radar Echo Hong Kong,NeurIPS,10.48550/arXiv.1506.04214,10.48550/arXiv.1506.04214,https://proceedings.neurips.cc/paper_files/paper/2015/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf,Convolutional LSTM network: A machine learning approach for precipitation nowcasting,introduced by the paper,15,"6,900",radar echo hong kong
WordNet,NeurIPS,10.5555/2999792.2999923,10.1145/219717.219748,https://dl.acm.org/doi/pdf/10.1145/219717.219748,Translating embeddings for modeling multi-relational data,,15,"7,106",wordnet
FB15k,NeurIPS,10.5555/2999792.2999923,N/A,https://papers.nips.cc/paper_files/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html,Translating embeddings for modeling multi-relational data,introduced by the paper,15,"7,106",fb15k
FB1M,NeurIPS,10.5555/2999792.2999923,N/A,https://papers.nips.cc/paper_files/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html,Translating embeddings for modeling multi-relational data,introduced by the paper,15,"7,106",fb1m
Imagenet 2012,NeurIPS,10.1145/3065386,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,ImageNet classification with deep convolutional neural networks,,15,"85,220",imagenet 2012
Imagenet 2010,NeurIPS,10.1145/3065386,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,ImageNet classification with deep convolutional neural networks,paper referenced the original imagenet,15,"85,220",imagenet 2010
CIFAR-10,NeurIPS,10.1145/3065386,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,ImageNet classification with deep convolutional neural networks,,15,"85,220",cifar-10
Imagenet fall 2009,NeurIPS,10.1145/3065386,N/A,N/A,ImageNet classification with deep convolutional neural networks,,15,"85,220",imagenet fall 2009
Allstate,NeurIPS,https://papers.nips.cc/paper_files/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html,N/A,https://www.kaggle.com/c/ClaimPredictionChallenge,LightGBM: A highly efficient gradient boosting decision tree,,15,"8,998",allstate
LETOR4,NeurIPS,https://papers.nips.cc/paper_files/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html,10.48550/arXiv.1306.2597,https://arxiv.org/ftp/arxiv/papers/1306/1306.2597.pdf,LightGBM: A highly efficient gradient boosting decision tree,,15,"8,998",letor4
Flight Delay,NeurIPS,https://papers.nips.cc/paper_files/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html,N/A,https://github.com/szilard/benchm-ml#data,LightGBM: A highly efficient gradient boosting decision tree,,15,"8,998",flight delay
KDD Cup 2010,NeurIPS,https://papers.nips.cc/paper_files/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html,?,https://pslcdatashop.web.cmu.edu/KDDCup/,LightGBM: A highly efficient gradient boosting decision tree,,15,"8,998",kdd cup 2010
KDD Cup 2012,NeurIPS,https://papers.nips.cc/paper_files/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html,?,https://kdd2012.sigkdd.org/,LightGBM: A highly efficient gradient boosting decision tree,,15,"8,998",kdd cup 2012
Pascal VOC 2007,TPAMI,10.1109/TPAMI.2009.167,10.1007/s11263-009-0275-4,https://link.springer.com/article/10.1007/s11263-009-0275-4#preview,Object detection with discriminatively trained part-based models,,15,8657,pascal voc 2007
INRIA Person,TPAMI,10.1109/TPAMI.2009.167,10.1109/CVPR.2005.177,https://paperswithcode.com/dataset/inria-person,Object detection with discriminatively trained part-based models,,15,8657,inria person
BSDS500,TPAMI,10.1109/TPAMI.2010.161,10.1109/TPAMI.2010.161,https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/papers/amfm_pami2010.pdf,Contour detection and hierarchical image segmentation,,15,4698,bsds500
BSDS300,TPAMI,10.1109/TPAMI.2010.161,10.1109/ICCV.2001.937655,https://ieeexplore.ieee.org/document/937655,Contour detection and hierarchical image segmentation,,15,4698,bsds300
Dark Channel,TPAMI,10.1109/TPAMI.2010.168,10.1109/TPAMI.2010.168,https://projectsweb.cs.washington.edu/research/insects/CVPR2009/award/hazeremv_drkchnl.pdf,Single image haze removal using dark channel prior,,15,6443,dark channel
INRIA Person,TPAMI,10.1109/TPAMI.2011.155,10.1109/CVPR.2005.177,https://paperswithcode.com/dataset/inria-person,Pedestrian detection: An evaluation of the state of the art,,15,2793,inria person
Caltech Pedestrian,TPAMI,10.1109/TPAMI.2011.155,10.1109/CVPR.2009.5206631 ,https://ieeexplore.ieee.org/document/5206631,Pedestrian detection: An evaluation of the state of the art,,15,2793,caltech pedestrian
TUD-Brussels Pedestrian,TPAMI,10.1109/TPAMI.2011.155,10.1109/CVPR.2008.4587581,https://ieeexplore.ieee.org/document/4657363,Pedestrian detection: An evaluation of the state of the art,,15,2793,tud-brussels pedestrian
ETH Pedestrian,TPAMI,10.1109/TPAMI.2011.155,10.1109/CVPR.2008.4587581,https://ieeexplore.ieee.org/document/4587581,Pedestrian detection: An evaluation of the state of the art,,15,2793,eth pedestrian
BSDS500,TPAMI,10.1109/TPAMI.2012.120,10.1109/TPAMI.2010.161,https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/papers/amfm_pami2010.pdf,SLIC superpixels compared to state-of-the-art superpixel methods,,15,8295,bsds500
PASCAL VOC 2010,TPAMI,10.1109/TPAMI.2012.120,10.1007/s11263-009-0275-4,https://ieeexplore.ieee.org/document/6909514,SLIC superpixels compared to state-of-the-art superpixel methods,,15,8295,pascal voc 2010
MSRC 21,TPAMI,10.1109/TPAMI.2012.120,10.1007/s11263-007-0109-1,https://link.springer.com/article/10.1007/s11263-007-0109-1,SLIC superpixels compared to state-of-the-art superpixel methods,,15,8295,msrc 21
TREC,TPAMI,10.1109/TPAMI.2012.59,N/A,https://aclanthology.org/C02-1150.pdf,3D Convolutional neural networks for human action recognition,,15,5290,trec
KTH,TPAMI,10.1109/TPAMI.2012.59,10.1109/ICPR.2004.1334462,https://ieeexplore.ieee.org/document/1334462,3D Convolutional neural networks for human action recognition,,15,5290,kth
HumanEva,TPAMI,10.1109/TPAMI.2013.248,10.1007/s11263-009-0273-6 ,https://link.springer.com/article/10.1007/s11263-009-0273-6,Human3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments,,15,2714,humaneva
SCAPE,TPAMI,10.1109/TPAMI.2013.248,10.1145/1186822.1073207,https://dl.acm.org/doi/10.1145/1186822.1073207,Human3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments,,15,2714,scape
Human3.6M,TPAMI,10.1109/TPAMI.2013.248,10.1109/TPAMI.2013.248,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=6682899&tag=1,Human3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments,,15,2714,human3.6m
CAESAR,TPAMI,10.1109/TPAMI.2013.248,N/A,https://khanhha.github.io/posts/3D-human-datasets/,Human3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments,,15,2714,caesar
MNIST,TPAMI,10.1109/TPAMI.2013.50,10.1109/5.726791,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=726791,Representation learning: A review and new perspectives,,15,9997,mnist
ImageNet,TPAMI,10.1109/TPAMI.2013.50,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Representation learning: A review and new perspectives,,15,9997,imagenet
Online Object Tracking,TPAMI,10.1109/TPAMI.2014.2345390,10.1109/CVPR.2013.312,https://ieeexplore.ieee.org/document/6619156,High-speed tracking with kernelized correlation filters,,15,5702,online object tracking
TB-100,TPAMI,10.1109/TPAMI.2014.2388226,10.1109/TPAMI.2014.2388226,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=7001050,Object tracking benchmark,,15,3295,tb-100
TB-50,TPAMI,10.1109/TPAMI.2014.2388226,10.1109/TPAMI.2014.2388226,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=7001050,Object tracking benchmark,,15,3295,tb-50
VIVID,TPAMI,10.1109/TPAMI.2014.2388226,10.48550/arXiv.2204.06183,https://arxiv.org/pdf/2204.06183,Object tracking benchmark,,15,3295,vivid
CAVIAR,TPAMI,10.1109/TPAMI.2014.2388226,N/A,https://www.kaggle.com/datasets/chiragtagadiya/caviar,Object tracking benchmark,,15,3295,caviar
PETS 2009,TPAMI,10.1109/TPAMI.2014.2388226,10.1109/PETS-WINTER.2009.5399556,https://ieeexplore.ieee.org/document/5399556,Object tracking benchmark,,15,3295,pets 2009
ImageNet 2012,TPAMI,10.1109/TPAMI.2015.2389824,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,,15,9854,imagenet 2012
Pascal VOC 2007,TPAMI,10.1109/TPAMI.2015.2389824,10.1007/s11263-009-0275-4,https://link.springer.com/article/10.1007/s11263-009-0275-4#preview,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,,15,9854,pascal voc 2007
Caltech101,TPAMI,10.1109/TPAMI.2015.2389824,10.22002/D1.20086,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/1384978,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,,15,9854,caltech101
PASCAL VOC 2012,TPAMI,10.1109/TPAMI.2015.2439281,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,Image Super-Resolution Using Deep Convolutional Networks,,15,8040,pascal voc 2012
Pascal Context,TPAMI,10.1109/TPAMI.2015.2439281,10.1109/CVPR.2014.119,https://cs.stanford.edu/~roozbeh/pascal-context/mottaghi_et_al_cvpr14.pdf,Image Super-Resolution Using Deep Convolutional Networks,,15,8040,pascal context
COCO,TPAMI,10.1109/TPAMI.2015.2439281,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Image Super-Resolution Using Deep Convolutional Networks,,15,8040,coco
PASCAL VOC 2012,TPAMI,10.1109/TPAMI.2016.2572683,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,Fully Convolutional Networks for Semantic Segmentation,,15,8132,pascal voc 2012
NYUDv2,TPAMI,10.1109/TPAMI.2016.2572683,10.1007/978-3-642-33715-4_54,https://link.springer.com/chapter/10.1007/978-3-642-33715-4_54,Fully Convolutional Networks for Semantic Segmentation,,15,8132,nyudv2
Sift Flow,TPAMI,10.1109/TPAMI.2016.2572683,10.1109/TPAMI.2010.147,https://people.csail.mit.edu/celiu/SIFTflow/SIFTflow.pdf,Fully Convolutional Networks for Semantic Segmentation,,15,8132,sift flow
PASCAL Context,TPAMI,10.1109/TPAMI.2016.2572683,10.1109/CVPR.2014.119,https://cs.stanford.edu/~roozbeh/pascal-context/mottaghi_et_al_cvpr14.pdf,Fully Convolutional Networks for Semantic Segmentation,,15,8132,pascal context
PASCAL VOC 2007,TPAMI,10.1109/TPAMI.2016.2577031,10.1007/s11263-009-0275-4,https://link.springer.com/article/10.1007/s11263-009-0275-4#preview,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,,15,26841,pascal voc 2007
PASCAL VOC 2012,TPAMI,10.1109/TPAMI.2016.2577031,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,,15,26841,pascal voc 2012
COCO,TPAMI,10.1109/TPAMI.2016.2577031,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,,15,26841,coco
CamVid,TPAMI,10.1109/TPAMI.2016.2644615,10.1016/j.patrec.2008.04.005,https://www.sciencedirect.com/science/article/abs/pii/S0167865508001220,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,,15,15060,camvid
SUN RGB-D,TPAMI,10.1109/TPAMI.2016.2644615,10.1109/CVPR.2015.7298655,https://ieeexplore.ieee.org/document/7298655,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,,15,15060,sun rgb-d
PASCAL VOC 2012,TPAMI,10.1109/TPAMI.2017.2699184,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",,15,16385,pascal voc 2012
PASCAL Context,TPAMI,10.1109/TPAMI.2017.2699184,10.1109/CVPR.2014.119,https://cs.stanford.edu/~roozbeh/pascal-context/mottaghi_et_al_cvpr14.pdf,"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",,15,16385,pascal context
Cityscapes,TPAMI,10.1109/TPAMI.2017.2699184,10.1109/CVPR.2016.350,https://ieeexplore.ieee.org/document/7780719,"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",,15,16385,cityscapes
,TPAMI,10.1109/TPAMI.2017.2723009,,,Places: A 10 Million Image Database for Scene Recognition,,15,2656,
ImageNet 2012,TPAMI,10.1109/TPAMI.2017.2773081,10.48550/arXiv.1409.0575,https://arxiv.org/pdf/1409.0575,Learning without Forgetting,,15,2803,imagenet 2012
Places365,TPAMI,10.1109/TPAMI.2017.2773081,10.48550/arXiv.1909.02410,https://arxiv.org/pdf/1909.02410v3,Learning without Forgetting,,15,2803,places365
PASCAL VOC 2012,TPAMI,10.1109/TPAMI.2017.2773081,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,Learning without Forgetting,,15,2803,pascal voc 2012
MNIST,TPAMI,10.1109/TPAMI.2017.2773081,10.1109/5.726791,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=726791,Learning without Forgetting,,15,2803,mnist
MIT Indoor Scenes,TPAMI,10.1109/TPAMI.2017.2773081,10.1109/CVPRW.2009.5206537 ,https://ieeexplore.ieee.org/document/5206537,Learning without Forgetting,,15,2803,mit indoor scenes
CUB-200-2011 Birds,TPAMI,10.1109/TPAMI.2017.2773081,N/A,https://authors.library.caltech.edu/records/cvm3y-5hh21,Learning without Forgetting,,15,2803,cub-200-2011 birds
COCO,TPAMI,10.1109/TPAMI.2018.2844175,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Mask R-CNN,,5,2628,coco
Coco,TPAMI,10.1109/TPAMI.2018.2858826,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Focal Loss for Dense Object Detection,,5,4877,coco
COCO,TPAMI,10.1109/TPAMI.2018.2858826,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Focal Loss for Dense Object Detection,,5,4877,coco
I2R,TPAMI,10.1109/TPAMI.2019.2891760,10.1109/TIP.2004.836169,https://ieeexplore.ieee.org/document/1344037,Tensor Robust Principal Component Analysis with a New Tensor Nuclear Norm,,5,750,i2r
BSD100,TPAMI,10.1109/TPAMI.2019.2891760,10.1109/ICCV.2001.937655,https://ieeexplore.ieee.org/document/937655,Tensor Robust Principal Component Analysis with a New Tensor Nuclear Norm,,5,750,bsd100
ImageNet,TPAMI,10.1109/TPAMI.2019.2913372,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Squeeze-and-Excitation Networks,,5,4886,imagenet
CIFAR-10,TPAMI,10.1109/TPAMI.2019.2913372,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Squeeze-and-Excitation Networks,,5,4886,cifar-10
CIFAR-100,TPAMI,10.1109/TPAMI.2019.2913372,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Squeeze-and-Excitation Networks,,5,4886,cifar-100
Places,TPAMI,10.1109/TPAMI.2019.2913372,10.1109/TPAMI.2017.2723009,http://places2.csail.mit.edu/PAMI_places.pdf,Squeeze-and-Excitation Networks,,5,4886,places
COCO,TPAMI,10.1109/TPAMI.2019.2913372,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Squeeze-and-Excitation Networks,,5,4886,coco
ImageNet-1k,TPAMI,10.1109/TPAMI.2019.2916873,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding,,5,1109,imagenet-1k
CIFAR-100,TPAMI,10.1109/TPAMI.2019.2916873,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding,,5,1109,cifar-100
PASCAL VOC 2007,TPAMI,10.1109/TPAMI.2019.2916873,10.1007/s11263-009-0275-4,https://link.springer.com/article/10.1007/s11263-009-0275-4#preview,NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding,,5,1109,pascal voc 2007
COCO,TPAMI,10.1109/TPAMI.2019.2916873,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding,,5,1109,coco
PASCAL VOC 2012,TPAMI,10.1109/TPAMI.2019.2916873,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding,,5,1109,pascal voc 2012
MPII,TPAMI,10.1109/TPAMI.2019.2929257,10.1109/CVPR.2014.471,https://paperswithcode.com/dataset/mpii,OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields,,5,2666,mpii
COCO,TPAMI,10.1109/TPAMI.2019.2929257,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields,,5,2666,coco
Foot Keypoint,TPAMI,10.1109/TPAMI.2019.2929257,10.1109/TPAMI.2019.2929257,https://arxiv.org/pdf/1812.08008,OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields,,5,2666,foot keypoint
ImageNet-1k,TPAMI,10.1109/TPAMI.2019.2938758,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,Res2Net: A New Multi-Scale Backbone Architecture,,5,2319,imagenet-1k
CIFAR-100,TPAMI,10.1109/TPAMI.2019.2938758,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Res2Net: A New Multi-Scale Backbone Architecture,,5,2319,cifar-100
COCO,TPAMI,10.1109/TPAMI.2019.2956516,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Cascade R-CNN: High quality object detection and instance segmentation,,5,1100,coco
Pascal VOC 2012,TPAMI,10.1109/TPAMI.2019.2956516,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,Cascade R-CNN: High quality object detection and instance segmentation,,5,1100,pascal voc 2012
Kitti,TPAMI,10.1109/TPAMI.2019.2956516,10.1109/CVPR.2012.6248074,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6248074,Cascade R-CNN: High quality object detection and instance segmentation,,5,1100,kitti
CityPersons,TPAMI,10.1109/TPAMI.2019.2956516,10.48550/arXiv.1702.05693,https://arxiv.org/abs/2003.12729,Cascade R-CNN: High quality object detection and instance segmentation,,5,1100,citypersons
WiderFace,TPAMI,10.1109/TPAMI.2019.2956516,10.1109/CVPR.2016.596,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/7780965,Cascade R-CNN: High quality object detection and instance segmentation,,5,1100,widerface
Got-10k,TPAMI,10.1109/TPAMI.2019.2957464,10.1109/TPAMI.2019.2957464,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=8922619,Got-10k: A large high-diversity benchmark for generic object tracking in the wild,,5,1080,got-10k
Coco,TPAMI,10.1109/TPAMI.2020.2983686,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Deep High-Resolution Representation Learning for Visual Recognition,,5,2899,coco
Pascal VOC 2010,TPAMI,10.1109/TPAMI.2020.2983686,10.1007/s11263-009-0275-4,https://ieeexplore.ieee.org/document/6909514,Deep High-Resolution Representation Learning for Visual Recognition,,5,2899,pascal voc 2010
Cityscapes,TPAMI,10.1109/TPAMI.2020.2983686,10.1109/CVPR.2016.350,https://ieeexplore.ieee.org/document/7780719,Deep High-Resolution Representation Learning for Visual Recognition,,5,2899,cityscapes
LIP,TPAMI,10.1109/TPAMI.2020.2983686,10.48550/arXiv.1703.05446,https://paperswithcode.com/dataset/lip,Deep High-Resolution Representation Learning for Visual Recognition,,5,2899,lip
COCO,TPAMI,10.1109/TPAMI.2020.3007032,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,CCNet: Criss-Cross Attention for Semantic Segmentation,,2,257,coco
Cityscapes,TPAMI,10.1109/TPAMI.2020.3007032,10.1109/CVPR.2016.350,https://ieeexplore.ieee.org/document/7780719,CCNet: Criss-Cross Attention for Semantic Segmentation,,2,257,cityscapes
ADE20K,TPAMI,10.1109/TPAMI.2020.3007032,10.1007/s11263-018-1140-0,https://www.researchgate.net/publication/306357649_Semantic_Understanding_of_Scenes_Through_the_ADE20K_Dataset,CCNet: Criss-Cross Attention for Semantic Segmentation,,2,257,ade20k
LIP,TPAMI,10.1109/TPAMI.2020.3007032,10.48550/arXiv.1703.05446,https://paperswithcode.com/dataset/lip,CCNet: Criss-Cross Attention for Semantic Segmentation,,2,257,lip
CamVid,TPAMI,10.1109/TPAMI.2020.3007032,10.1016/j.patrec.2008.04.005,https://www.sciencedirect.com/science/article/abs/pii/S0167865508001220,CCNet: Criss-Cross Attention for Semantic Segmentation,,2,257,camvid
RoadScene,TPAMI,10.1109/TPAMI.2020.3012548,10.48550/arXiv.2401.07322,https://arxiv.org/abs/2401.07322,U2Fusion: A Unified Unsupervised Image Fusion Network,,5,1233,roadscene
DIML Indoor,TPAMI,10.1109/TPAMI.2020.3019967,10.1109/TIP.2018.2836318,https://dimlrgbd.github.io/,Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer,,5,825,diml indoor
Mega Depth,TPAMI,10.1109/TPAMI.2020.3019967,10.1109/CVPR.2018.00218,https://www.cs.cornell.edu/projects/megadepth/,Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer,,5,825,mega depth
ReDWeb,TPAMI,10.1109/TPAMI.2020.3019967,10.1109/CVPR.2018.00040,https://paperswithcode.com/dataset/redweb,Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer,,5,825,redweb
WSVD,TPAMI,10.1109/TPAMI.2020.3019967,10.1109/3DV.2019.00046,https://paperswithcode.com/dataset/wsvd,Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer,,5,825,wsvd
3DMovies,TPAMI,10.1109/TPAMI.2020.3019967,10.1109/TPAMI.2020.3019967,https://arxiv.org/pdf/1907.01341,Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer,,5,825,3dmovies
iNaturalist,TPAMI,10.1109/TPAMI.2021.3057446,10.1109/CVPR.2018.00914,https://ieeexplore.ieee.org/document/8579012,A Continual Learning Survey: Defying Forgetting in Classification Tasks,,5,1039,inaturalist
Tiny ImageNet,TPAMI,10.1109/TPAMI.2021.3057446,N/A,https://paperswithcode.com/dataset/tiny-imagenet,A Continual Learning Survey: Defying Forgetting in Classification Tasks,,5,1039,tiny imagenet
UniRef,TPAMI,10.1109/TPAMI.2021.3095381,10.1093/bioinformatics/btv107,https://academic.oup.com/bioinformatics/article/31/12/2056/214922,ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning,,5,809,uniref
BFD,TPAMI,10.1109/TPAMI.2021.3095381,10.1038/s41586-021-03819-2 ,https://bfd.mmseqs.com/,ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning,,5,809,bfd
COCO,TPAMI,10.1109/TPAMI.2022.3152247,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,A Survey on Vision Transformer,,2,2135,coco
ImageNet,TPAMI,10.1109/TPAMI.2022.3152247,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,A Survey on Vision Transformer,,2,2135,imagenet
SynthText,TPAMI,10.1109/TPAMI.2022.3155612,10.1109/CVPR.2016.254,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7780623,Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion,,2,268,synthtext
MLT-2017,TPAMI,10.1109/TPAMI.2022.3155612,10.1109/ICDAR.2017.237,https://ieeexplore.ieee.org/document/8270168,Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion,,2,268,mlt-2017
MLT-2019 ,TPAMI,10.1109/TPAMI.2022.3155612,10.1109/ICDAR.2019.00254 ,https://ieeexplore.ieee.org/document/8978096,Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion,,2,268,mlt-2019 
ICDAR2015-Challenge-4,TPAMI,10.1109/TPAMI.2022.3155612,10.1109/ICDAR.2015.7333942,https://ieeexplore.ieee.org/document/7333942,Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion,,2,268,icdar2015-challenge-4
CTW1500,TPAMI,10.1109/TPAMI.2022.3155612,10.1016/j.patcog.2019.02.002,https://www.sciencedirect.com/science/article/pii/S0031320319300664,Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion,,2,268,ctw1500
Total-Text,TPAMI,10.1109/TPAMI.2022.3155612,10.1109/ICDAR.2017.157,https://ieeexplore.ieee.org/document/8270088,Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion,,2,268,total-text
MSRA-TD500,TPAMI,10.1109/TPAMI.2022.3155612,10.1109/CVPR.2012.6247787,https://ieeexplore.ieee.org/document/6247787,Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion,,2,268,msra-td500
NTU RGB+D,TPAMI,10.1109/TPAMI.2022.3157033,10.48550/arXiv.1604.02808,https://arxiv.org/abs/1604.02808,Constructing Stronger and Faster Baselines for Skeleton-Based Action Recognition,,2,267,ntu rgb+d
ImageNet,TPAMI,10.1109/TPAMI.2022.3164083,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Contextual Transformer Networks for Visual Recognition,,2,406,imagenet
Moving MNIST,TPAMI,10.1109/TPAMI.2022.3165153,N/A,https://paperswithcode.com/dataset/moving-mnist,PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive Learning,,2,287,moving mnist
KTH,TPAMI,10.1109/TPAMI.2022.3165153,10.1109/ICPR.2004.1334462,https://ieeexplore.ieee.org/document/1334462,PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive Learning,,2,287,kth
Traffic4Cast,TPAMI,10.1109/TPAMI.2022.3165153,N/A,https://github.com/iarai/NeurIPS2022-traffic4cast,PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive Learning,,2,287,traffic4cast
BAIR,TPAMI,10.1109/TPAMI.2022.3165153,10.48550/arXiv.1710.05268,https://arxiv.org/abs/1710.05268,PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive Learning,,2,287,bair
Radar Echo,TPAMI,10.1109/TPAMI.2022.3165153,N/A,https://zenodo.org/records/7059117,PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive Learning,,2,287,radar echo
COCO,TPAMI,10.1109/TPAMI.2022.3166956,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,"SCRDet++: Detecting Small, Cluttered and Rotated Objects via Instance-Level Feature Denoising and Rotation Loss Smoothing",,2,216,coco
DOTA,TPAMI,10.1109/TPAMI.2022.3166956,10.1109/CVPR.2018.00418,https://arxiv.org/pdf/1711.10398,"SCRDet++: Detecting Small, Cluttered and Rotated Objects via Instance-Level Feature Denoising and Rotation Loss Smoothing",,2,216,dota
DIOR,TPAMI,10.1109/TPAMI.2022.3166956,10.21227/tq1e-nx82,https://arxiv.org/pdf/1909.00133,"SCRDet++: Detecting Small, Cluttered and Rotated Objects via Instance-Level Feature Denoising and Rotation Loss Smoothing",,2,216,dior
UCAS-AOD,TPAMI,10.1109/TPAMI.2022.3166956,10.1109/ICIP.2015.7351502,https://ieeexplore.ieee.org/document/7351502,"SCRDet++: Detecting Small, Cluttered and Rotated Objects via Instance-Level Feature Denoising and Rotation Loss Smoothing",,2,216,ucas-aod
ICDAR2015-Challenge-4,TPAMI,10.1109/TPAMI.2022.3166956,10.1109/ICDAR.2015.7333942,https://ieeexplore.ieee.org/document/7333942,"SCRDet++: Detecting Small, Cluttered and Rotated Objects via Instance-Level Feature Denoising and Rotation Loss Smoothing",,2,216,icdar2015-challenge-4
BSTLD,TPAMI,10.1109/TPAMI.2022.3166956,10.5281/zenodo.12706046,https://zenodo.org/records/12706046,"SCRDet++: Detecting Small, Cluttered and Rotated Objects via Instance-Level Feature Denoising and Rotation Loss Smoothing",,2,216,bstld
S2TLD,TPAMI,10.1109/TPAMI.2022.3166956,10.1109/TPAMI.2022.3166956,https://ieeexplore.ieee.org/document/9756223,"SCRDet++: Detecting Small, Cluttered and Rotated Objects via Instance-Level Feature Denoising and Rotation Loss Smoothing",,2,216,s2tld
SIDD,TPAMI,10.1109/TPAMI.2022.3167175,10.1109/CVPR.2018.00182,https://openaccess.thecvf.com/content_cvpr_2018/papers/Abdelhamed_A_High-Quality_Denoising_CVPR_2018_paper.pdf,Learning Enriched Features for Fast Image Restoration and Enhancement,,2,254,sidd
DND,TPAMI,10.1109/TPAMI.2022.3167175,10.1109/CVPR.2017.294,https://noise.visinf.tu-darmstadt.de/,Learning Enriched Features for Fast Image Restoration and Enhancement,,2,254,dnd
DPDD,TPAMI,10.1109/TPAMI.2022.3167175,10.1007/978-3-030-58607-2_7,https://link.springer.com/chapter/10.1007/978-3-030-58607-2_7,Learning Enriched Features for Fast Image Restoration and Enhancement,,2,254,dpdd
MIT-Adobe FiveK,TPAMI,10.1109/TPAMI.2022.3167175,10.1109/CVPR.2011.5995332,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=5995332,Learning Enriched Features for Fast Image Restoration and Enhancement,,2,254,mit-adobe fivek
LOL,TPAMI,10.1109/TPAMI.2022.3167175,10.48550/arXiv.1808.04560,https://arxiv.org/abs/1808.04560,Learning Enriched Features for Fast Image Restoration and Enhancement,,2,254,lol
KITTI-360,TPAMI,10.1109/TPAMI.2022.3179507,10.1109/TPAMI.2022.3179507,https://arxiv.org/pdf/2109.13410,KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D,,2,218,kitti-360
DUTS,TPAMI,10.1109/TPAMI.2022.3179526,10.1109/CVPR.2017.404,https://saliencydetection.net/duts/,Salient Object Detection via Integrity Learning,,2,242,duts
UCF101,TPAMI,10.1109/TPAMI.2022.3183112,10.48550/arXiv.1212.0402,https://arxiv.org/pdf/1212.0402,Human Action Recognition From Various Data Modalities: A Review,,2,355,ucf101
HMDB51,TPAMI,10.1109/TPAMI.2022.3183112,10.1109/ICCV.2011.6126543,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=6126543,Human Action Recognition From Various Data Modalities: A Review,,2,355,hmdb51
NTU RGB+D,TPAMI,10.1109/TPAMI.2022.3183112,10.48550/arXiv.1604.02808,https://arxiv.org/abs/1604.02808,Human Action Recognition From Various Data Modalities: A Review,,2,355,ntu rgb+d
CelebAHQ,TPAMI,10.1109/TPAMI.2022.3204461,10.48550/arXiv.1710.10196,https://openreview.net/pdf?id=Hk99zCeAb,Image Super-Resolution via Iterative Refinement,,2,831,celebahq
ImageNet,TPAMI,10.1109/TPAMI.2022.3204461,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Image Super-Resolution via Iterative Refinement,,2,831,imagenet
ImageNet-1K,TPAMI,10.1109/TPAMI.2022.3206148,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,ResMLP: Feedforward Networks for Image Classification with Data-Efficient Training,,2,302,imagenet-1k
ImageNet-v2,TPAMI,10.1109/TPAMI.2022.3206148,10.48550/arXiv.1902.10811,https://arxiv.org/pdf/1902.10811,ResMLP: Feedforward Networks for Image Classification with Data-Efficient Training,,2,302,imagenet-v2
ImageNet-real,TPAMI,10.1109/TPAMI.2022.3206148,10.48550/arXiv.2006.07159,https://arxiv.org/abs/2006.07159,ResMLP: Feedforward Networks for Image Classification with Data-Efficient Training,,2,302,imagenet-real
COCO,TPAMI,10.1109/TPAMI.2022.3211006,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Beyond Self-Attention: External Attention Using Two Linear Layers for Visual Tasks,,2,314,coco
COCO,TPAMI,10.1109/TPAMI.2022.3211006,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Beyond Self-Attention: External Attention Using Two Linear Layers for Visual Tasks,,2,314,coco
ImageNet-1k,TPAMI,10.1109/TPAMI.2022.3211006,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,Beyond Self-Attention: External Attention Using Two Linear Layers for Visual Tasks,,2,314,imagenet-1k
Cityscapes,TPAMI,10.1109/TPAMI.2022.3211006,10.1109/CVPR.2016.350,https://ieeexplore.ieee.org/document/7780719,Beyond Self-Attention: External Attention Using Two Linear Layers for Visual Tasks,,2,314,cityscapes
CIFAR-100,TPAMI,10.1109/TPAMI.2022.3213473,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Class-Incremental Learning: Survey and Performance Evaluation on Image Classification,,2,319,cifar-100
ImageNet-1k,TPAMI,10.1109/TPAMI.2022.3213473,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,Class-Incremental Learning: Survey and Performance Evaluation on Image Classification,,2,319,imagenet-1k
Oxford Flowers 102,TPAMI,10.1109/TPAMI.2022.3213473,10.1109/ICVGIP.2008.47,https://ieeexplore.ieee.org/document/4756141,Class-Incremental Learning: Survey and Performance Evaluation on Image Classification,,2,319,oxford flowers 102
MIT Indoor Scenes,TPAMI,10.1109/TPAMI.2022.3213473,10.1109/ICVGIP.2008.47,https://ieeexplore.ieee.org/document/4756141,Class-Incremental Learning: Survey and Performance Evaluation on Image Classification,,2,319,mit indoor scenes
CUB-200-2011 Birds,TPAMI,10.1109/TPAMI.2022.3213473,10.1109/ICVGIP.2008.47,https://ieeexplore.ieee.org/document/4756141,Class-Incremental Learning: Survey and Performance Evaluation on Image Classification,,2,319,cub-200-2011 birds
StanfordCars,TPAMI,10.1109/TPAMI.2022.3213473,10.1109/ICVGIP.2008.47,https://ieeexplore.ieee.org/document/4756141,Class-Incremental Learning: Survey and Performance Evaluation on Image Classification,,2,319,stanfordcars
FGVC Aircraft,TPAMI,10.1109/TPAMI.2022.3213473,10.48550/arXiv.1306.5151,https://arxiv.org/abs/1306.5151,Class-Incremental Learning: Survey and Performance Evaluation on Image Classification,,2,319,fgvc aircraft
Stanford 40 Actions,TPAMI,10.1109/TPAMI.2022.3213473,10.1109/ICCV.2011.6126386,https://ieeexplore.ieee.org/document/6126386,Class-Incremental Learning: Survey and Performance Evaluation on Image Classification,,2,319,stanford 40 actions
ImageNet,TPAMI,10.1109/TPAMI.2023.3268118,10.1109/CVPR.2009.5206848,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/5206848,Deep Long-Tailed Learning: A Survey,,2,275,imagenet
CIFAR-100,TPAMI,10.1109/TPAMI.2023.3268118,N/A,https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf,Deep Long-Tailed Learning: A Survey,,2,275,cifar-100
Places,TPAMI,10.1109/TPAMI.2023.3268118,10.1109/TPAMI.2017.2723009,http://places2.csail.mit.edu/PAMI_places.pdf,Deep Long-Tailed Learning: A Survey,,2,275,places
PASCAL VOC 2012,TPAMI,10.1109/TPAMI.2023.3268118,10.48550/arXiv.1902.06162,http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#testdata,Deep Long-Tailed Learning: A Survey,,2,275,pascal voc 2012
iNaturalist,TPAMI,10.1109/TPAMI.2023.3268118,10.1109/CVPR.2018.00914,https://ieeexplore.ieee.org/document/8579012,Deep Long-Tailed Learning: A Survey,,2,275,inaturalist
LVIS,TPAMI,10.1109/TPAMI.2023.3268118,10.1109/CVPR.2019.00550,https://ieeexplore.ieee.org/document/8954457,Deep Long-Tailed Learning: A Survey,,2,275,lvis
COCO,TPAMI,10.1109/TPAMI.2023.3268118,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,Deep Long-Tailed Learning: A Survey,,2,275,coco
VideoLT,TPAMI,10.1109/TPAMI.2023.3268118,10.1109/ICCV48922.2021.00786,https://ieeexplore.ieee.org/document/9711442,Deep Long-Tailed Learning: A Survey,,2,275,videolt
ImageNet-1K,TPAMI,10.1109/TPAMI.2023.3282631,10.48550/arXiv.1409.0575,https://arxiv.org/abs/1409.0575,UniFormer: Unifying Convolution and Self-Attention for Visual Recognition,,2,238,imagenet-1k
Kinetics-400,TPAMI,10.1109/TPAMI.2023.3282631,10.48550/arXiv.1705.06950,https://arxiv.org/pdf/1705.06950v1,UniFormer: Unifying Convolution and Self-Attention for Visual Recognition,,2,238,kinetics-400
Kinetics-600,TPAMI,10.1109/TPAMI.2023.3282631,10.57702/4hj86f6e,https://arxiv.org/pdf/1808.01340v1,UniFormer: Unifying Convolution and Self-Attention for Visual Recognition,,2,238,kinetics-600
Something-Something V1,TPAMI,10.1109/TPAMI.2023.3282631,10.48550/arXiv.1706.04261,https://arxiv.org/pdf/1706.04261,UniFormer: Unifying Convolution and Self-Attention for Visual Recognition,,2,238,something-something v1
Something-Something V2,TPAMI,10.1109/TPAMI.2023.3282631,10.48550/arXiv.2307.06527,https://arxiv.org/pdf/2307.06527,UniFormer: Unifying Convolution and Self-Attention for Visual Recognition,,2,238,something-something v2
UCF101,TPAMI,10.1109/TPAMI.2023.3282631,10.48550/arXiv.1212.0402,https://arxiv.org/pdf/1212.0402,UniFormer: Unifying Convolution and Self-Attention for Visual Recognition,,2,238,ucf101
HMDB51,TPAMI,10.1109/TPAMI.2023.3282631,10.1109/ICCV.2011.6126543,https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=6126543,UniFormer: Unifying Convolution and Self-Attention for Visual Recognition,,2,238,hmdb51
COCO,TPAMI,10.1109/TPAMI.2023.3282631,10.1007/978-3-319-10602-1_48,https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48,UniFormer: Unifying Convolution and Self-Attention for Visual Recognition,,2,238,coco
ADE20K,TPAMI,10.1109/TPAMI.2023.3282631,10.1007/s11263-018-1140-0,https://www.researchgate.net/publication/306357649_Semantic_Understanding_of_Scenes_Through_the_ADE20K_Dataset,UniFormer: Unifying Convolution and Self-Attention for Visual Recognition,,2,238,ade20k
ADE20K,TPAMI,10.1109/TPAMI.2023.3282631,10.1007/s11263-018-1140-0,https://www.researchgate.net/publication/306357649_Semantic_Understanding_of_Scenes_Through_the_ADE20K_Dataset,UniFormer: Unifying Convolution and Self-Attention for Visual Recognition,,2,238,ade20k
SODA-A,TPAMI,10.1109/TPAMI.2023.3290594,10.1109/TPAMI.2023.3290594,https://arxiv.org/pdf/2207.14096,Towards Large-Scale Small Object Detection: Survey and Benchmarks,,2,293,soda-a
SODA-D,TPAMI,10.1109/TPAMI.2023.3290594,10.1109/TPAMI.2023.3290594,https://arxiv.org/pdf/2207.14096,Towards Large-Scale Small Object Detection: Survey and Benchmarks,,2,293,soda-d
fMoW-S2,TPAMI,10.1109/TPAMI.2024.3362475,10.48550/arXiv.1711.07846,https://arxiv.org/pdf/1711.07846,SpectralGPT: Spectral Remote Sensing Foundation Model,,2,353,fmow-s2
BigEarthNet-S2,TPAMI,10.1109/TPAMI.2024.3362475,10.1109/IGARSS.2019.8900532,https://ieeexplore.ieee.org/document/8900532,SpectralGPT: Spectral Remote Sensing Foundation Model,,2,353,bigearthnet-s2